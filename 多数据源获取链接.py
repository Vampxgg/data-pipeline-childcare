# Dify ä¾èµ–ç®¡ç†: è¯·ç¡®ä¿å·²æ·»åŠ  httpx, json-repair, tavily-python
import sys
import io

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸º utf-8ï¼Œé˜²æ­¢åœ¨ Windows æ§åˆ¶å°ä¸‹å‡ºç° UnicodeEncodeError
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import asyncio
import random
from itertools import cycle

import httpx
import re
import os
import json
import traceback
from typing import Any, Dict, List, Coroutine, Literal, Optional, Union
from abc import ABC, abstractmethod

# å¼•å…¥æ–°ä¾èµ–çš„å®¢æˆ·ç«¯
from tavily import AsyncTavilyClient
import json_repair
from datetime import datetime, timedelta


# ==============================================================================
# ====================== æ—¶é—´è§£æè¾…åŠ©æ¨¡å— =========================
# ==============================================================================
def _normalize_date(date_input: str) -> Optional[str]:
    """
    å°è¯•å°†å„ç§æ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸º YYYY-MM-DD æ ¼å¼ã€‚
    æ”¯æŒ: YYYY-MM-DD, YYYY/MM/DD, YYYY.MM.DD, YYYYå¹´MMæœˆDDæ—¥
    """
    if not date_input:
        return None

    date_input = date_input.strip()

    # å¸¸è§æ ¼å¼æ­£åˆ™
    patterns = [
        (r'(\d{4})-(\d{1,2})-(\d{1,2})', '%Y-%m-%d'),
        (r'(\d{4})/(\d{1,2})/(\d{1,2})', '%Y/%m/%d'),
        (r'(\d{4})\.(\d{1,2})\.(\d{1,2})', '%Y.%m.%d'),
        (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Yå¹´%mæœˆ%dæ—¥'),
        (r'(\d{4})(\d{2})(\d{2})', '%Y%m%d'),
    ]

    for pat, fmt in patterns:
        match = re.search(pat, date_input)
        if match:
            try:
                # æå–åŒ¹é…çš„éƒ¨åˆ†è¿›è¡Œè§£æ
                date_str = match.group(0)
                dt = datetime.strptime(date_str, fmt)
                return dt.strftime('%Y-%m-%d')
            except ValueError:
                continue
    return None


def _parse_relative_time(text: str) -> Optional[str]:
    """è§£æç›¸å¯¹æ—¶é—´ï¼Œè¿”å› YYYY-MM-DD"""
    if not text: return None
    today = datetime.now()
    text = text.lower()

    # å‡ å¤©å‰ / è¿‘å‡ å¤©
    match = re.search(r'(?:è¿‘|æœ€è¿‘|)(\d+)\s*(?:å¤©|æ—¥)(?:å‰|å†…)?', text)
    if not match: match = re.search(r'(\d+)\s*days?\s*ago', text)
    if match:
        days = int(match.group(1))
        return (today - timedelta(days=days)).strftime('%Y-%m-%d')

    # å‡ å‘¨å‰
    match = re.search(r'(?:è¿‘|æœ€è¿‘|)(\d+)\s*å‘¨(?:å‰|å†…)?', text)
    if not match: match = re.search(r'(\d+)\s*weeks?\s*ago', text)
    if match:
        weeks = int(match.group(1))
        return (today - timedelta(weeks=weeks)).strftime('%Y-%m-%d')

    # å‡ æœˆå‰ (ç®€å•æŒ‰30å¤©ç®—)
    match = re.search(r'(?:è¿‘|æœ€è¿‘|)(\d+)\s*æœˆ(?:å‰|å†…)?', text)
    if not match: match = re.search(r'(\d+)\s*months?\s*ago', text)
    if match:
        months = int(match.group(1))
        return (today - timedelta(days=months * 30)).strftime('%Y-%m-%d')

    # å‡ å¹´å‰
    match = re.search(r'(?:è¿‘|æœ€è¿‘|)(\d+)\s*å¹´(?:å‰|å†…)?', text)
    if not match: match = re.search(r'(\d+)\s*years?\s*ago', text)
    if match:
        years = int(match.group(1))
        return (today - timedelta(days=years * 365)).strftime('%Y-%m-%d')

    return None


def _parse_time_filter(time_input: Any) -> Dict[str, str]:
    """
    è§£ææ—¶é—´è¾“å…¥ï¼Œè¿”å› {'after': 'YYYY-MM-DD', 'before': 'YYYY-MM-DD'} å­—å…¸ã€‚
    """
    result = {}
    if not time_input:
        return result

    # 1. å¦‚æœæ˜¯å­—å…¸
    if isinstance(time_input, dict):
        start = time_input.get('start') or time_input.get('after') or time_input.get('begin') or time_input.get(
            'start_date')
        end = time_input.get('end') or time_input.get('before') or time_input.get('end_date')

        # å°è¯•è§£æç»å¯¹æ—¶é—´
        norm_start = _normalize_date(str(start)) if start else None
        if not norm_start and start: norm_start = _parse_relative_time(str(start))

        norm_end = _normalize_date(str(end)) if end else None
        if not norm_end and end: norm_end = _parse_relative_time(str(end))

        if norm_start: result['after'] = norm_start
        if norm_end: result['before'] = norm_end
        return result

    # 2. å¦‚æœæ˜¯å­—ç¬¦ä¸²
    if isinstance(time_input, str):
        # 2.1 å°è¯•è§£æç›¸å¯¹æ—¶é—´ (e.g., "è¿‘3å¤©", "last week") -> é»˜è®¤ä¸º after
        rel_time = _parse_relative_time(time_input)
        if rel_time:
            # å¦‚æœæ˜¯ç›¸å¯¹æ—¶é—´ï¼Œé€šå¸¸æ„å‘³ç€ "ä»é‚£æ—¶åˆ°ç°åœ¨"ï¼Œå³ after
            result['after'] = rel_time
            return result

        # 2.2 å°è¯•æå–ç»å¯¹æ—¥æœŸ
        matches = re.findall(r'(\d{4}[-/å¹´\.]\d{1,2}[-/æœˆ\.]\d{1,2}æ—¥?)', time_input)
        normalized_dates = []
        for m in matches:
            norm = _normalize_date(m)
            if norm: normalized_dates.append(norm)

        if len(normalized_dates) >= 2:
            # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯ startï¼Œç¬¬äºŒä¸ªæ˜¯ end
            result['after'] = normalized_dates[0]
            result['before'] = normalized_dates[1]
        elif len(normalized_dates) == 1:
            # åªæœ‰ä¸€ä¸ªæ—¥æœŸï¼Œéœ€è¦åˆ¤æ–­æ˜¯ after è¿˜æ˜¯ before
            lower_input = time_input.lower()
            if any(kw in lower_input for kw in ['before', 'until', 'end', 'ä¹‹å‰', 'æˆªæ­¢']):
                result['before'] = normalized_dates[0]
            else:
                result['after'] = normalized_dates[0]

    return result


# ==============================================================================
# ====================== DIFY æœ¬åœ°è°ƒè¯•è¾…åŠ©æ¨¡å— =========================
# ==============================================================================
import pprint

# --- æœ¬åœ°è°ƒè¯•å¼€å…³ ---
# åœ¨ä½ çš„ IDE ä¸­è¿›è¡Œæµ‹è¯•æ—¶ï¼Œå°†æ­¤å€¼è®¾ä¸º Trueã€‚
# å½“ä½ å‡†å¤‡å°†ä»£ç å¤åˆ¶åˆ° Dify å¹³å°æ—¶ï¼Œè¯·å°†å…¶æ”¹å› Falseï¼Œæˆ–ç›´æ¥åˆ é™¤æ­¤è°ƒè¯•æ¨¡å—ã€‚
IS_LOCAL_DEBUG = True


def _dify_debug_return(data: Dict[str, Any], label: str = "Final Return") -> Dict[str, Any]:
    """
    ä¸€ä¸ªç”¨äºåœ¨ Dify ä»£ç èŠ‚ç‚¹ä¸­è¿›è¡Œæœ¬åœ°è°ƒè¯•çš„åŒ…è£…å‡½æ•°ã€‚

    å½“ IS_LOCAL_DEBUG ä¸º True æ—¶ï¼Œå®ƒä¼šæ¼‚äº®åœ°æ‰“å°å‡ºæœ€ç»ˆè¦è¿”å›çš„æ•°æ®ï¼Œ
    ç„¶ååŸå°ä¸åŠ¨åœ°è¿”å›è¯¥æ•°æ®ï¼Œä»¥ä¾¿ Dify å¹³å°èƒ½æ­£ç¡®æ¥æ”¶ã€‚

    Args:
        data (Dict[str, Any]): å‡†å¤‡ä» Dify èŠ‚ç‚¹è¿”å›çš„æ•°æ®ã€‚
        label (str, optional): ä¸€ä¸ªæ ‡ç­¾ï¼Œç”¨äºåœ¨æ§åˆ¶å°è¾“å‡ºä¸­æ ‡è¯†æ¥æºã€‚é»˜è®¤ä¸º "Final Return"ã€‚

    Returns:
        Dict[str, Any]: ä¼ å…¥çš„åŸå§‹æ•°æ®ã€‚
    """
    if IS_LOCAL_DEBUG:
        # æ‰“å°ä¸€ä¸ªæ¸…æ™°çš„åˆ†éš”ç¬¦å’Œæ ‡ç­¾ï¼Œæ–¹ä¾¿åœ¨ç»ˆç«¯ä¸­è¯†åˆ«
        print("\n" + "=" * 40 + f" DIFY DEBUG OUTPUT [{label}] " + "=" * 40)

        # ä½¿ç”¨ pprint æ¨¡å—è¿›è¡Œç¾åŒ–è¾“å‡ºï¼Œå¯¹å¤æ‚çš„åµŒå¥—å­—å…¸ç‰¹åˆ«å‹å¥½
        pprint.pprint(data, indent=2, width=120)

        # æ‰“å°ç»“æŸåˆ†éš”ç¬¦
        print("=" * 105 + "\n")

    # æ— è®ºæ˜¯å¦æ‰“å°ï¼Œéƒ½å¿…é¡»åŸå°ä¸åŠ¨åœ°è¿”å›åŸå§‹æ•°æ®
    return data


def _intelligent_input_parser(raw_input: Any) -> Dict[str, Any]:
    """
    å¥å£®åœ°è§£æå¤æ‚çš„è¾“å…¥ç»“æ„ï¼Œæå–webæœç´¢æŸ¥è¯¢ã€èŒä¸šæŸ¥è¯¢æ•°æ®å’Œä¼ä¸šæŸ¥è¯¢åç§°ã€‚
    """
    if isinstance(raw_input, list) and len(raw_input) > 0:
        actual_input = raw_input[0]
    else:
        actual_input = raw_input

    if isinstance(actual_input, str):
        if not actual_input.strip():
            # ã€è°ƒæ•´ã€‘å¦‚æœè¾“å…¥ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›æ‰€æœ‰å­—æ®µéƒ½ä¸ºç©ºçš„ç»“æ„ï¼Œé¿å…ä¸‹æ¸¸æŠ¥é”™ã€‚
            return {"comprehensive_queries": [], "career_query_data": {}, "tianyan_enterprise_names": []}
        try:
            data = json_repair.loads(actual_input)
        except Exception as e:
            raise ValueError(f"Failed to parse input string as JSON: {e}\nOriginal input: {str(actual_input)[:500]}...")
    elif isinstance(actual_input, dict):
        data = actual_input
    else:
        raise TypeError(f"Expected input type is str, list, or dict, but received {type(actual_input).__name__}")
    if not isinstance(data, dict):
        raise ValueError("Parsed data is not a valid object (dictionary).")

    # å®‰å…¨åœ°æå– web_queries å¯¹è±¡
    web_queries_obj = data.get("web_queries", {})
    if not isinstance(web_queries_obj, dict): web_queries_obj = {}
    # 1. æå– comprehensive_query
    comprehensive_queries = []
    query_list = web_queries_obj.get("comprehensive_query", [])
    if isinstance(query_list, list) and all(isinstance(item, str) for item in query_list):
        comprehensive_queries = query_list

    # 2. æå– career_query
    career_query_data = web_queries_obj.get("career_query", {})
    if not isinstance(career_query_data, dict): career_query_data = {}
    # 3. æå– tianyan_check_enterprise
    tianyan_input = web_queries_obj.get("tianyan_check_enterprise", [])  # é»˜è®¤å€¼æ”¹ä¸ºç©ºåˆ—è¡¨
    tianyan_enterprise_names: List[str] = []

    if isinstance(tianyan_input, str):
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ¸…ç†ååŒ…è£…æˆå•å…ƒç´ åˆ—è¡¨
        cleaned_name = tianyan_input.strip()
        if cleaned_name:
            tianyan_enterprise_names.append(cleaned_name)
    elif isinstance(tianyan_input, list):
        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ¸…ç†å¹¶è¿‡æ»¤æ‰æ— æ•ˆå…ƒç´ 
        tianyan_enterprise_names = [
            str(item).strip() for item in tianyan_input
            if isinstance(item, str) and str(item).strip()
        ]

    return {
        "comprehensive_queries": comprehensive_queries,
        "career_query_data": career_query_data,
        "tianyan_enterprise_names": tianyan_enterprise_names
    }


# ==============================================================================
# ====================== å…¨å±€æœç´¢ç­–ç•¥é…ç½® ======================
# ==============================================================================
SEARCH_STRATEGY_CONFIG = {
    # 1. æ³›ç”¨ç½‘é¡µæœç´¢ (General Web Search)
    "web": {
        "excludes": [
            "-filetype:pdf", "-filetype:docx", "-filetype:xlsx", "-filetype:pptx",
            "-inurl:login", "-inurl:register"  # æ’é™¤ç™»å½•æ³¨å†Œé¡µé¢
        ]
    },

    # 2. è§†é¢‘æœç´¢ (Video Search)
    "video": {
        "includes": [
            "site:douyin.com",  # æŠ–éŸ³
            "site:bilibili.com",  # Bilibili
            "site:ixigua.com",  # è¥¿ç“œè§†é¢‘
            "site:youtube.com"  # YouTube (å¦‚æœç½‘ç»œå¯è¾¾)
        ]
    },

    # 3. è¡Œä¸šæŠ¥å‘Šä¸åˆ†æ (Industry Reports & Analysis)
    "industry_reports": {
        "includes": [
            "site:36kr.com",  # 36æ°ª - é¢†å…ˆçš„ç§‘æŠ€åª’ä½“å’Œåˆ›æŠ•æœåŠ¡
            "site:iresearch.com.cn",  # è‰¾ç‘å’¨è¯¢ - çŸ¥åäº’è”ç½‘å’¨è¯¢æœºæ„
            "site:questmobile.com.cn",  # QuestMobile - ç§»åŠ¨äº’è”ç½‘æ•°æ®æ´å¯Ÿ
            "site:caixin.com",  # è´¢æ–°ç½‘ - é«˜è´¨é‡çš„è´¢ç»æ–°é—»ä¸æ·±åº¦åˆ†æ
            "site:iyiou.com",  # äº¿æ¬§ç½‘ - äº§ä¸šåˆ›æ–°æœåŠ¡å¹³å°
            "site:pedata.cn",  # æ¸…ç§‘ç ”ç©¶ - ä¸“æ³¨äºè‚¡æƒæŠ•èµ„å¸‚åœº
            "site:www.deloitte.com/cn/",  # å¾·å‹¤ä¸­å›½
            "site:www.pwccn.com",  # æ™®åæ°¸é“ä¸­å›½
            "site:www.ey.com/zh_cn",  # å®‰æ°¸ä¸­å›½
            "site:home.kpmg/cn/",  # æ¯•é©¬å¨ä¸­å›½
            "site:research.cicc.com"  # ä¸­é‡‘å…¬å¸ç ”ç©¶éƒ¨
        ]
    },

    # 4. æ”¿ç­–æ–‡ä»¶ä¸ä¸“åˆ© (Policy Documents & Patents)
    "policy_patents": {
        "includes": [
            "site:gov.cn",  # ä¸­å›½æ”¿åºœç½‘ (ä¸­å¤®)
            "site:ndrc.gov.cn",  # å‘æ”¹å§”
            "site:miit.gov.cn",  # å·¥ä¿¡éƒ¨
            "site:most.gov.cn",  # ç§‘æŠ€éƒ¨
            "site:cnipa.gov.cn",  # å›½å®¶çŸ¥è¯†äº§æƒå±€
            "site:patents.google.com",  # è°·æ­Œä¸“åˆ© (è¦†ç›–å…¨çƒï¼Œå«ä¸­å›½)
            "site:soopat.com"  # Soopat ä¸“åˆ©æœç´¢
        ]
    },

    # 5. ä¸“å®¶è®¿è°ˆä¸ä¸“ä¸šæ´è§ (Expert Interviews & Insights)
    "expert_insights": {
        "includes": [
            "site:infoq.cn",  # InfoQä¸­å›½ - æŠ€æœ¯ä¸“å®¶ç¤¾åŒº
            "site:csdn.net",  # CSDN - ç¨‹åºå‘˜ç¤¾åŒº
            "site:xueqiu.com"  # é›ªçƒ - æŠ•èµ„è€…ç¤¾åŒº
        ]
    },

    # 6. ä¼ä¸šæ‹›è˜ä¸å²—ä½æè¿° (Enterprise Recruitment)
    # æ³¨æ„: è¿™äº›ç½‘ç«™æ›´é€‚åˆå†…éƒ¨æœç´¢ï¼Œä½†site:è¯­æ³•æœ‰æ—¶èƒ½å‘ç°è¢«ç´¢å¼•çš„å…¬å¼€é¡µé¢
    "recruitment": {
        "includes": [
            "site:zhipin.com",  # BOSSç›´è˜
            "site:liepin.com",  # çŒè˜
            "site:zhaopin.com",  # æ™ºè”æ‹›è˜
            "site:lagou.com",  # æ‹‰å‹¾ç½‘ (åæŠ€æœ¯å²—)
            "site:maimai.cn"  # è„‰è„‰ (èŒåœºç¤¾äº¤ä¸æ‹›è˜)
        ]
    },

    # 7. æ‰˜è‚²æ”¿ç­–ä¸æ–°é—» (Childcare Policy & News)
    "childcare_policy_and_news": {
        "includes": [
            "site:tuoyu.cpdrc.org.cn",  # å…¨å›½æ‰˜è‚²æœºæ„ä¿¡æ¯å…¬ç¤ºå¹³å°
            "site:zs.kaipuyun.cn",  # å«å¥å§”ç›¸å…³æ”¿ç­–æœç´¢ (å¼€æ™®äº‘)
            "site:www.tuoyufuwu.org.cn",  # ä¸­å›½æ‰˜è‚²æœåŠ¡ç½‘ (æ”¿ç­–/æ–°é—»/ä¸“åŒº)
            "site:www.cpaw.org.cn"  # ä¸­å›½äººå£å­¦ä¼š (æ”¿ç­–æ³•è§„)
        ]
    },

    # ========================== æ‰˜è‚²äº”å¤§æ ¸å¿ƒåˆ†æç»´åº¦ ==========================
    # ç»´åº¦ä¸€ï¼šæ”¿ç­–å¯¼å‘ä¸åŒºåŸŸè§„åˆ’ (Policy & Regional Planning)
    # å¯¹åº”æ•°æ®æºï¼šæ•™è‚²éƒ¨ã€å«å¥å§”ã€äººç¤¾éƒ¨ã€ä¸­å›½æ”¿åºœç½‘ã€åœ°æ–¹å‘æ”¹å§”
    "policy_regional": {
        "includes": [
            "site:moe.gov.cn",  # æ•™è‚²éƒ¨ (èŒä¸šæ•™è‚²/å¹¼æ•™æ”¿ç­–)
            "site:nhc.gov.cn",  # å›½å®¶å«å¥å§” (æ‰˜è‚²æœºæ„å¤‡æ¡ˆ/å«ç”Ÿæ ‡å‡†)
            "site:mohrss.gov.cn",  # äººç¤¾éƒ¨ (èŒä¸šèµ„æ ¼/æŠ€èƒ½æ ‡å‡†)
            "site:ndrc.gov.cn",  # å‘æ”¹å§” (äº§ä¸šè§„åˆ’/èµ„é‡‘æŠ•å…¥)
            "site:people.com.cn",  # äººæ°‘ç½‘ (æƒå¨è§£è¯»)
            "site:tuoyu.cpdrc.org.cn"  # å…¨å›½æ‰˜è‚²æœºæ„ä¿¡æ¯å…¬ç¤ºå¹³å°
        ],
        "regional_patterns": [
            "site:wjw.{scope}.gov.cn",  # åœ°æ–¹å«å¥å§”
            "site:{scope}.edu.gov.cn",  # åœ°æ–¹æ•™è‚²å±€ (æ³¨æ„ï¼šå¾ˆå¤šåœ°æ–¹æ•™è‚²å±€åŸŸåä¸ç»Ÿä¸€ï¼Œè¿™æ˜¯é€šç”¨è§„åˆ™)
            "site:edu.{scope}.gov.cn",  # å¦ä¸€ç§å¸¸è§çš„æ•™è‚²å±€åŸŸåæ ¼å¼
            "site:{scope}.drc.gov.cn",  # åœ°æ–¹å‘æ”¹å§”
            "site:{scope}.tjj.gov.cn",  # åœ°æ–¹ç»Ÿè®¡å±€
            "site:www.{scope}.gov.cn"  # åœ°æ–¹æ”¿åºœé—¨æˆ·
        ],
        "_source_ownership": "TuoYu"
    },
    # ç»´åº¦äºŒï¼šå¸‚åœºä¾›éœ€ä¸äº§ä¸šè§„æ¨¡ (Market Supply/Demand & Scale)
    # å¯¹åº”æ•°æ®æºï¼šè‰¾åª’ã€å¤´è±¹ã€ç»Ÿè®¡å±€ã€å¤©çœ¼æŸ¥(å…¬å¼€é¡µ)
    "market_supply": {
        "includes": [
            "site:stats.gov.cn",  # å›½å®¶ç»Ÿè®¡å±€ (äººå£/ä¸‰äº§æ•°æ®)
            "site:iresearch.com.cn",  # è‰¾ç‘å’¨è¯¢ (è¡Œä¸šç ”æŠ¥)
            "site:leadleo.com",  # å¤´è±¹ç ”ç©¶é™¢ (æ·±åº¦ç ”æŠ¥)
            "site:iimedia.cn",  # è‰¾åª’å’¨è¯¢ (å¸‚åœºæ•°æ®)
            "site:drcnet.com.cn"  # å›½ç ”ç½‘ (å®è§‚ç»æµ)
        ],
        "regional_patterns": [
            "site:{scope}.tjj.gov.cn"  # åœ°æ–¹ç»Ÿè®¡å±€æŸ¥çœ‹äººå£æ•°æ®
        ],
        "_source_ownership": "TuoYu"
    },
    # ç»´åº¦ä¸‰ï¼šä»ä¸šäººå‘˜ä¸äººæ‰éœ€æ±‚ (Personnel & Talent Demand)
    # å¯¹åº”æ•°æ®æºï¼šæ‹›è˜å¹³å°ã€é™¢æ ¡æ‹›ç”Ÿã€äººç¤¾éƒ¨
    # æ³¨æ„ï¼šæ‹›è˜ç½‘ç«™é€šå¸¸æœ‰åçˆ¬ï¼Œsite:è¯­æ³•ä¸»è¦ç”¨äºæœç´¢å…¬å¼€çš„å²—ä½åˆ†ææ–‡ç« æˆ–éƒ¨åˆ†ç´¢å¼•é¡µé¢
    "personnel_talent": {
        "includes": [
            "site:chsi.com.cn",  # å­¦ä¿¡ç½‘ (ä¸“ä¸š/é™¢æ ¡å¼€è®¾æƒ…å†µ)
        ],
        "_source_ownership": "TuoYu"
    },
    # ç»´åº¦å››ï¼šäº§ä¸šå‘å±•è¶‹åŠ¿ä¸ä¸šæ€åˆ›æ–° (Trends & Innovation)
    # å¯¹åº”æ•°æ®æºï¼š36æ°ªã€äº¿æ¬§ã€è¡Œä¸šå±•ä¼šã€æ™ºæ…§åŒ»ç–—å‚å•†
    "trends_innovation": {
        "includes": [
            "site:36kr.com",  # 36æ°ª (æŠ•èèµ„/æ–°é¡¹ç›®)
            "site:iyiou.com",  # äº¿æ¬§ç½‘ (äº§ä¸šåˆ›æ–°)
            "site:vcbeat.top",  # åŠ¨è„‰ç½‘ (åŒ»è‚²ç»“åˆ/åŒ»ç–—å¥åº·)
            "site:cyzone.cn",  # åˆ›ä¸šé‚¦
            "site:woshipm.com"  # äººäººéƒ½æ˜¯äº§å“ç»ç† (äº§å“åˆ†æ/æ¨¡å¼æ‹†è§£)
        ],
        "_source_ownership": "TuoYu"
    },
    # ç»´åº¦äº”ï¼šè¡Œä¸šè§„èŒƒä¸èŒä¸šæ ‡å‡† (Standards & Qualifications)
    # å¯¹åº”æ•°æ®æºï¼šäººç¤¾éƒ¨æŠ€èƒ½é‰´å®šä¸­å¿ƒã€èŒä¸šèµ„æ ¼ç½‘ã€æ ‡å‡†åŒ–å§”å‘˜ä¼š
    "standards_norms": {
        "includes": [
            "site:osta.mohrss.gov.cn",  # èŒä¸šæŠ€èƒ½é‰´å®šä¸­å¿ƒ (è¯ä¹¦æŸ¥è¯¢/æ ‡å‡†)
            "site:sac.gov.cn",  # å›½å®¶æ ‡å‡†åŒ–ç®¡ç†å§”å‘˜ä¼š (å›½æ ‡æ–‡ä»¶)
            "site:chinanews.com"  # ä¸­å›½æ–°é—»ç½‘ (è¡Œä¸šè§„èŒƒæ–°é—»)
        ],
        "_source_ownership": "TuoYu"
    },

    # 8. ä¸“å±åŒºåŸŸ/é¢†åŸŸè§„åˆ™
    "exclusive_rules": {
        # å®šä¹‰äº†å¤šä¸ªæŸ¥è¯¢æ¨¡æ¿
        "templates": [
            '"{school}" AND "{major}" site:edu.cn',
            '"{major}" AND "{scope}" site:gov.cn'
        ],
        # å¯¹åº”æ¯ä¸ªæ¨¡æ¿æ‰€å¿…éœ€çš„ regional_rules é”®
        "requires": [
            ["school", "major"],
            ["major", "scope"]
        ]
    }

}


def _generate_exclusive_queries(
        regional_data: Optional[Dict[str, str]] = None
) -> List[str]:
    """
    æ ¹æ®åŒºåŸŸè§„åˆ™æ•°æ®å’Œå…¨å±€é…ç½®ï¼Œç”Ÿæˆç‹¬ç«‹çš„ã€å¯ç›´æ¥æœç´¢çš„æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

    Args:
        regional_data (Optional[Dict[str, str]]): åŒ…å«åŒºåŸŸè§„åˆ™æ•°æ®çš„å­—å…¸ã€‚

    Returns:
        List[str]: ä¸€ä¸ªç”±ä¸“å±è§„åˆ™ç”Ÿæˆçš„æŸ¥è¯¢å­—ç¬¦ä¸²ç»„æˆçš„åˆ—è¡¨ã€‚
    """
    if not regional_data:
        return []

    strategy = SEARCH_STRATEGY_CONFIG.get("exclusive_rules", {})
    templates = strategy.get("templates", [])
    requirements = strategy.get("requires", [])

    if not templates:
        return []

    generated_queries = []
    for i, template in enumerate(templates):
        # æ£€æŸ¥å½“å‰æ¨¡æ¿çš„æ‰€æœ‰å¿…è¦å­—æ®µæ˜¯å¦éƒ½åœ¨ regional_data ä¸­ä¸”ä¸ä¸ºç©º
        if i < len(requirements) and all(regional_data.get(key) for key in requirements[i]):
            try:
                # æ ¼å¼åŒ–æ¨¡æ¿ï¼Œå¡«å……æ•°æ®ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æŸ¥è¯¢
                formatted_query = template.format(**regional_data)
                generated_queries.append(formatted_query)
                print(f"  -> [Exclusive Query Generated] \"{formatted_query}\"")
            except KeyError as e:
                # å¦‚æœæ¨¡æ¿ä¸­çš„å ä½ç¬¦åœ¨ regional_data ä¸­æ‰¾ä¸åˆ°ï¼Œåˆ™è·³è¿‡
                print(f"  -> [Warning] Skipping rule template due to missing key: {e}")
                pass

    return generated_queries


def _build_filtered_query(original_query: str, search_type: str,
                          regional_data: Optional[Dict[str, str]] = None,
                          use_regional_patterns: bool = False,
                          time_filter: Optional[Dict[str, str]] = None
                          ) -> str:
    """
    æ ¹æ®å…¨å±€é…ç½®ï¼Œä¸ºåŸå§‹æŸ¥è¯¢æ„å»ºå¸¦æœ‰è¿‡æ»¤æ¡ä»¶çš„æœ€ç»ˆæŸ¥è¯¢å­—ç¬¦ä¸²ã€‚

    Args:
        original_query (str): ç”¨æˆ·çš„åŸå§‹æœç´¢è¯ã€‚
        search_type (str): æœç´¢ç±»å‹ï¼Œå¦‚ 'web', 'video', 'industry_reports' ç­‰ã€‚
        time_filter (Optional[Dict[str, str]]): æ—¶é—´è¿‡æ»¤æ¡ä»¶ï¼Œå¦‚ {'after': '2023-01-01'}ã€‚


    Returns:
        str: é™„åŠ äº†è¿‡æ»¤è§„åˆ™çš„æœ€ç»ˆæŸ¥è¯¢å­—ç¬¦ä¸²ã€‚
    """
    strategy = SEARCH_STRATEGY_CONFIG.get(search_type, {})

    # æ„å»ºåŸºç¡€æŸ¥è¯¢
    final_query = original_query

    # === æ¨¡å¼ A: åŒºåŸŸæ€§æ£€ç´¢ (ä»…å½“å¼€å…³æ‰“å¼€ä¸”æœ‰åŒºåŸŸæ•°æ®æ—¶) ===
    if use_regional_patterns and regional_data:
        regional_patterns = strategy.get("regional_patterns", [])
        if regional_patterns:
            valid_sites = []
            for pattern in regional_patterns:
                try:
                    formatted_site = pattern.format(**regional_data)
                    valid_sites.append(formatted_site)
                except KeyError:
                    pass
            if valid_sites:
                sites_query_part = " OR ".join(valid_sites)
                final_query = f"{original_query} ({sites_query_part})".strip()

    # === æ¨¡å¼ B: æ ‡å‡†æ’é™¤/åŒ…å«è§„åˆ™ ===
    # æ³¨æ„ï¼šå¦‚æœå·²ç»åº”ç”¨äº†åŒºåŸŸæ¨¡å¼ï¼Œé€šå¸¸ä¸å†åº”ç”¨ includesï¼Œé™¤éé€»è¾‘éœ€è¦å åŠ ã€‚
    # è¿™é‡Œä¿æŒåŸæœ‰é€»è¾‘ï¼šå¦‚æœæ²¡è¿›åŒºåŸŸæ¨¡å¼ï¼Œæˆ–è€…åŒºåŸŸæ¨¡å¼åªæ˜¯ä¿®æ”¹äº† final_queryï¼Œä¸‹é¢ç»§ç»­è¿½åŠ  excludes/includes

    # ä¸ºäº†é¿å…é€»è¾‘å†²çªï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼š
    # å¦‚æœä¸Šé¢çš„åŒºåŸŸæ¨¡å¼ä¿®æ”¹äº† queryï¼Œè¿™é‡Œåªå¤„ç† excludesï¼Œä¸å†å¤„ç† includes (å‡è®¾åŒºåŸŸæ¨¡å¼å·²ç»æŒ‡å®šäº† site)
    # ä½†åŸä»£ç é€»è¾‘ä¼¼ä¹æ˜¯äº’æ–¥çš„ï¼ŸåŸä»£ç é€»è¾‘ï¼š
    # if use_regional... return ...
    # elif excludes ... return ...
    # elif includes ... return ...
    # æ‰€ä»¥åŸä»£ç æ˜¯äº’æ–¥çš„ã€‚æˆ‘ä¹Ÿä¿æŒäº’æ–¥ç»“æ„ã€‚

    if use_regional_patterns and regional_data and strategy.get("regional_patterns"):
        pass  # final_query å·²ç»åœ¨ä¸Šé¢æ„å»ºå¥½äº†
    elif "excludes" in strategy:
        exclusions = strategy["excludes"]
        if exclusions:
            final_query = f"{original_query} {' '.join(exclusions)}".strip()
    elif "includes" in strategy:
        inclusions = strategy["includes"]
        if inclusions:
            sites_query_part = " OR ".join(inclusions)
            final_query = f"{original_query} ({sites_query_part})".strip()

    # === æ¨¡å¼ C: æ—¶é—´è¿‡æ»¤ (æ–°å¢) ===
    # Google Search æ”¯æŒ after:YYYY-MM-DD å’Œ before:YYYY-MM-DD
    # ä»…å¯¹ web æœç´¢ç”Ÿæ•ˆï¼Œæˆ–è€…ä»»ä½•æ”¯æŒè¯¥è¯­æ³•çš„å¼•æ“
    if time_filter and search_type in ['web', 'industry_reports', 'policy_patents', 'expert_insights', 'recruitment',
                                       'childcare_policy_and_news', 'policy_regional', 'market_supply',
                                       'personnel_talent', 'trends_innovation', 'standards_norms']:
        # åŸºæœ¬ä¸Šå¤§éƒ¨åˆ†åŸºäº web çš„æœç´¢éƒ½æ”¯æŒ
        if time_filter.get('after'):
            final_query += f" after:{time_filter['after']}"
        if time_filter.get('before'):
            final_query += f" before:{time_filter['before']}"

    print("-" * 20)
    print(f"Original: {original_query} -> Final: {final_query}")
    return final_query


DEFAULT_VIDEO_THUMBNAIL = "https://server.x-pilot.cn/static/meta-doc/png/797ba9dff794925f01d59c47f1248d35.png"


def _parse_video_url(url: str) -> Dict[str, Optional[str]]:
    """
    ä¸€ä¸ªé€šç”¨çš„è§†é¢‘URLè§£æå™¨ï¼Œç”¨äºä»URLä¸­æå–å…ƒæ•°æ®ã€‚
    å¯è½»æ¾æ‰©å±•ä»¥æ”¯æŒæ›´å¤šå¹³å°ã€‚
    """
    # æŠ–éŸ³ (douyin.com)
    douyin_match = re.search(r'/video/(\d+)', url)
    if douyin_match:
        video_id = douyin_match.group(1)
        return {"video_id": video_id, "embed_url": url, "thumbnail_url": DEFAULT_VIDEO_THUMBNAIL}
    # Bilibili (bilibili.com)
    bilibili_match = re.search(r'bilibili\.com/video/(BV[a-zA-Z0-9]+)', url)
    if bilibili_match:
        video_id = bilibili_match.group(1)
        return {"video_id": video_id, "embed_url": f"//player.bilibili.com/player.html?bvid={video_id}",
                "thumbnail_url": DEFAULT_VIDEO_THUMBNAIL}  # Bç«™å°é¢å›¾éœ€è¦APIè·å–ï¼Œæš‚ä¸å¤„ç†
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å·²çŸ¥å¹³å°ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
    return {"video_id": None, "embed_url": url, "thumbnail_url": DEFAULT_VIDEO_THUMBNAIL}


# --- æ™ºèƒ½è§£ææ¨¡å— ---
# def _intelligent_input_parser(raw_input: Any) -> Dict[str, Any]:
#     """
#     å¥å£®åœ°è§£æå¤æ‚çš„è¾“å…¥ç»“æ„ï¼Œæå–webæœç´¢æŸ¥è¯¢ã€èŒä¸šæŸ¥è¯¢æ•°æ®å’Œä¼ä¸šæŸ¥è¯¢åç§°ã€‚
#     """
#     if isinstance(raw_input, list) and len(raw_input) > 0:
#         actual_input = raw_input[0]
#     else:
#         actual_input = raw_input
#     if isinstance(actual_input, str):
#         if not actual_input.strip(): raise ValueError("Input string is empty or contains only whitespace.")
#         try:
#             data = json_repair.loads(actual_input)
#         except Exception as e:
#             raise ValueError(f"Failed to parse input string as JSON: {e}\nOriginal input: {str(actual_input)[:500]}...")
#     elif isinstance(actual_input, dict):
#         data = actual_input
#     else:
#         raise TypeError(f"Expected input type is str, list, or dict, but received {type(actual_input).__name__}")
#
#     if not isinstance(data, dict): raise ValueError("Parsed data is not a valid object (dictionary).")
#     # å®‰å…¨åœ°æå– web_queries å¯¹è±¡
#     web_queries_obj = data.get("web_queries", {})
#     if not isinstance(web_queries_obj, dict): web_queries_obj = {}
#     # 1. æå– comprehensive_query
#     comprehensive_queries = []
#     query_list = web_queries_obj.get("comprehensive_query", [])
#     if isinstance(query_list, list) and all(isinstance(item, str) for item in query_list):
#         comprehensive_queries = query_list
#     # 2. æå– career_query
#     career_query_data = web_queries_obj.get("career_query", {})
#     if not isinstance(career_query_data, dict): career_query_data = {}
#
#     # 3. ã€è°ƒæ•´ã€‘æå– tianyan_check_enterprise
#     tianyan_enterprise_name = web_queries_obj.get("tianyan_check_enterprise", "")
#     if not isinstance(tianyan_enterprise_name, str): tianyan_enterprise_name = ""
#     return {
#         "comprehensive_queries": comprehensive_queries,
#         "career_query_data": career_query_data,
#         "tianyan_enterprise_name": tianyan_enterprise_name.strip()
#     }
#

# ã€æ–°å¢ã€‘ä¸€ä¸ªå¯é çš„æŠ–éŸ³å…ƒæ•°æ®æŠ“å–å‡½æ•°
# async def _fetch_douyin_metadata_reliably(
#         url: str,
#         client: httpx.AsyncClient
# ) -> Optional[Dict[str, str]]:
#     """
#     é€šè¿‡è®¿é—®æŠ–éŸ³é¡µé¢HTMLæ¥å¯é åœ°è·å–è§†é¢‘IDå’Œå°é¢å›¾ã€‚
#     - è‡ªåŠ¨å¤„ç† v.douyin.com çŸ­é“¾æ¥è·³è½¬ã€‚
#     - ä»é¡µé¢çš„ <meta property="og:image"> æ ‡ç­¾ä¸­è§£æå°é¢å›¾ã€‚
#     - ä»æœ€ç»ˆçš„URLä¸­è§£æè§†é¢‘IDã€‚
#     """
#     try:
#         print(f"ğŸ”— [Douyin Scraper] å¼€å§‹è§£æ URL: {url}")
#
#         # æ­¥éª¤ 1: å‘é€ HEAD è¯·æ±‚ä»¥å¤„ç†çŸ­é“¾æ¥è·³è½¬ï¼Œè·å–æœ€ç»ˆçš„URL
#         # ä½¿ç”¨ HEAD è¯·æ±‚æ¯” GET æ›´å¿«ï¼Œå› ä¸ºå®ƒåªè·å–å¤´éƒ¨ä¿¡æ¯
#         headers = {
#             'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1'
#         }
#         # ã€æ³¨æ„ã€‘éœ€è¦è®¾ç½® allow_redirects=True
#         head_resp = await client.head(url, headers=headers, timeout=15, follow_redirects=True)
#         final_url = str(head_resp.url)
#         print(f"  -> è·³è½¬åæœ€ç»ˆ URL: {final_url}")
#
#         # æ­¥éª¤ 2: ä»æœ€ç»ˆURLä¸­è§£æè§†é¢‘ID
#         video_id_match = re.search(r'/video/(\d+)', final_url)
#         if not video_id_match:
#             print(f"  -> âš ï¸ æ— æ³•ä»æœ€ç»ˆURLä¸­è§£æåˆ° video_idã€‚")
#             return None
#         video_id = video_id_match.group(1)
#
#         # æ­¥éª¤ 3: è®¿é—®æœ€ç»ˆé¡µé¢ï¼Œè·å–HTMLå†…å®¹
#         get_resp = await client.get(final_url, headers=headers, timeout=15)
#         get_resp.raise_for_status()
#         html_content = get_resp.text
#
#         # æ­¥éª¤ 4: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¿«é€Ÿä»HTMLä¸­è§£æ og:image æ ‡ç­¾å†…å®¹
#         # è¿™ç§æ–¹æ³•æ¯”å®Œæ•´çš„BeautifulSoupè§£ææ›´å¿«ï¼Œå¯¹äºç›®æ ‡æ˜ç¡®çš„åœºæ™¯éå¸¸é«˜æ•ˆ
#         og_image_match = re.search(r'<meta\s+property="og:image"\s+content="([^"]+)"', html_content)
#
#         if og_image_match:
#             thumbnail_url = og_image_match.group(1)
#             print(f"  -> âœ… æˆåŠŸæŠ“å–åˆ°å°é¢å›¾: {thumbnail_url}")
#             return {
#                 "video_id": video_id,
#                 "embed_url": final_url,
#                 "thumbnail_url": thumbnail_url
#             }
#         else:
#             print(f"  -> âš ï¸ é¡µé¢HTMLä¸­æœªæ‰¾åˆ° og:image æ ‡ç­¾ã€‚")
#             return {"video_id": video_id, "embed_url": final_url, "thumbnail_url": None}
#
#     except Exception as e:
#         print(f"  -> âŒ è§£ææŠ–éŸ³URLæ—¶å‘ç”Ÿé”™è¯¯: {e}")
#         return None


# --- æ£€ç´¢ç­–ç•¥æ¨¡å— (ä¿æŒä¸å˜) ---
class SearchProvider(ABC):
    @abstractmethod
    async def search(self, query: str, client: httpx.AsyncClient, num_results: int,
                     search_type: Literal['web', 'video']) -> List[Dict[str, Any]]: pass

    def _prefix_keys(self, result: Dict[str, Any], prefix: str) -> Dict[str, Any]: return {f"{prefix}_{key}": value for
                                                                                           key, value in result.items()}


class SearchApiIoProvider(SearchProvider):
    # çœç•¥å®ç°...
    def __init__(self):
        self.api_key = os.environ.get("SEARCHAPI_IO_API_KEY", "7MPC8616NewB263CoB1NMcgS")
        if not self.api_key: raise ValueError("SearchAPI.io API Key is not provided.")
        self.base_url = "https://www.searchapi.io/api/v1/search"
        self.prefix = "searchapi"

    # @staticmethod
    # def _extract_youtube_info(url: str) -> Dict[str, Optional[str]]:
    #     match = re.search(r"(?:v=|youtu\.be/|embed/)([a-zA-Z0-9_-]{11})", url)
    #     if not match: return {"video_id": None, "embed_url": None, "thumbnail_url": None}
    #     video_id = match.group(1)
    #     return {"video_id": video_id, "embed_url": f"https://www.youtube.com/embed/{video_id}",
    #             "thumbnail_url": f"https://img.youtube.com/vi/{video_id}/0.jpg"}
    #
    # def _extract_douyin_info(self, url: str) -> Dict[str, Optional[str]]:
    #     """
    #     ä»æŠ–éŸ³çš„åˆ†äº«é“¾æ¥ä¸­æå–è§†é¢‘IDï¼Œå¹¶è¿”å›å›ºå®šçš„å ä½ç¼©ç•¥å›¾ã€‚
    #     """
    #     video_id = None
    #     # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æŠ–éŸ³è§†é¢‘ID (é€šå¸¸æ˜¯ä¸€é•¿ä¸²æ•°å­—)
    #     match = re.search(r'/video/(\d+)', url)
    #     if match:
    #         video_id = match.group(1)
    #     # ã€è°ƒæ•´ã€‘æ— è®ºæ˜¯å¦æå–åˆ° video_idï¼Œéƒ½è¿”å›å›ºå®šçš„å ä½å›¾URL
    #     return {
    #         "video_id": video_id,
    #         "embed_url": url,  # ç›´æ¥ä½¿ç”¨åŸå§‹URLä½œä¸ºåµŒå…¥é“¾æ¥
    #         "thumbnail_url": self.DEFAULT_DOUYIN_THUMBNAIL
    #     }

    async def search(self, query: str, client: httpx.AsyncClient, num_results: int,
                     search_type: Literal['web', 'video'] = 'web') -> List[Dict[str, Any]]:
        try:
            params = {"q": query, "engine": "google", "gl": "cn", "hl": "zh-cn", "num": num_results,
                      "api_key": self.api_key}
            resp = await client.get(self.base_url, params=params, timeout=20)
            resp.raise_for_status()
            data = resp.json().get("organic_results", [])

            results = []
            for item in data:
                if not item.get("link"): continue
                base_info = {"type": search_type, "url": item.get("link"), "title": item.get("title"),
                             "source": item.get("source", ""), "snippet": item.get("snippet", "")}
                if search_type == 'video': base_info.update(_parse_video_url(item.get("link")))
                results.append(self._prefix_keys(base_info, self.prefix))
            return results

            # tasks = []
            # original_items = []
            # # ã€è°ƒæ•´ã€‘å¯¹äºè§†é¢‘æœç´¢ï¼Œæˆ‘ä»¬å…ˆæ”¶é›†æ‰€æœ‰é“¾æ¥ï¼Œç„¶åå¹¶å‘åœ°å»æŠ“å–å…ƒæ•°æ®
            # if search_type == 'video':
            #     for item in data:
            #         link = item.get("link")
            #         if link:
            #             # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            #             tasks.append(_fetch_douyin_metadata_reliably(link, client))
            #             original_items.append(item)
            #
            #     # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŠ–éŸ³å…ƒæ•°æ®æŠ“å–ä»»åŠ¡
            #     metadata_results = await asyncio.gather(*tasks)
            #     # å°†æŠ“å–ç»“æœä¸åŸå§‹æœç´¢ç»“æœåˆå¹¶
            #     results = []
            #     for i, item in enumerate(original_items):
            #         metadata = metadata_results[i]
            #         base_info = {
            #             "type": search_type,
            #             "url": item.get("link"),
            #             "title": item.get("title"),
            #             "source": item.get("source", "æŠ–éŸ³"),
            #             "snippet": item.get("snippet", "")
            #         }
            #         if metadata:
            #             base_info.update(metadata)
            #         else:
            #             # å¦‚æœæŠ“å–å¤±è´¥ï¼Œåˆ™å¡«å……ç©ºå€¼
            #             base_info.update({"video_id": None, "embed_url": item.get("link"), "thumbnail_url": None})
            #
            #         results.append(self._prefix_keys(base_info, self.prefix))
            #     return results
            # # å¯¹äºWebæœç´¢ï¼Œé€»è¾‘ä¿æŒä¸å˜
            # else:
            #     results = []
            #     for item in data:
            #         if not item.get("link"): continue
            #         base_info = {
            #             "type": search_type,
            #             "url": item.get("link"),
            #             "title": item.get("title"),
            #             "source": item.get("source", ""),
            #             "snippet": item.get("snippet", "")
            #         }
            #         results.append(self._prefix_keys(base_info, self.prefix))
            #     return results

        except Exception as e:
            return [self._prefix_keys({"type": search_type, "error": f"SearchAPI.io request failed for '{query}': {e}"},
                                      self.prefix)]


class JinaSearchProvider(SearchProvider):
    def __init__(self):
        self.api_key = os.environ.get("JINA_API_KEY",
                                      "jina_b4348ffc39ca47bfbe753b95f59428c7i6ifkOFXRPdF3dRa5Rwb6T8FvrLH")
        if not self.api_key: raise ValueError("Jina.ai API Key is not provided.")
        self.base_url = "https://s.jina.ai/"
        # **å…³é”®ä¿®æ­£ 1: æ›´æ–°è¯·æ±‚å¤´ï¼Œä¸å®˜æ–¹æ–‡æ¡£ä¿æŒä¸€è‡´**
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "X-Respond-With": "no-content"
        }
        self.prefix = "jina"

    async def search(self, query: str, client: httpx.AsyncClient, num_results: int,
                     search_type: Literal['web', 'video'] = 'web') -> List[Dict[str, Any]]:
        # if search_type == 'video': return []
        try:
            # **å…³é”®ä¿®æ­£ 2: æ›´æ–°è¯·æ±‚ä½“ï¼ŒåŠ å…¥æœ¬åœ°åŒ–å‚æ•°**
            data = {
                "q": query,
                "gl": "CN",
                "hl": "zh-cn"
            }
            resp = await client.post(self.base_url, headers=self.headers, json=data, timeout=30)  # å¢åŠ è¶…æ—¶æ—¶é—´
            resp.raise_for_status()

            # **å…³é”®ä¿®æ­£ 3: ç›´æ¥è§£æå“åº”æ–‡æœ¬ï¼Œé¿å…æ½œåœ¨çš„ç¼–ç é—®é¢˜**
            # Jina API è¿”å›çš„å¯èƒ½æ˜¯éæ ‡å‡†JSONï¼Œç›´æ¥ç”¨ .text
            # httpx çš„ .json() å¯èƒ½ä¼šä¸¥æ ¼æ£€æŸ¥ content-type
            api_response = json.loads(resp.text)

            api_results = api_response.get("data", [])
            results = []
            for item in api_results[:num_results]:
                if not item.get("url"): continue
                base_info = {"type": search_type, "url": item.get("url"), "title": item.get("title"),
                             "snippet": item.get("description"), "content": item.get("content", "")}
                if search_type == 'video': base_info.update(_parse_video_url(item.get("url")))
                results.append(self._prefix_keys(base_info, self.prefix))
            return results
        except Exception as e:
            return [
                self._prefix_keys({"type": search_type, "error": f"Jina.ai request failed for '{query}': {e}"},
                                  self.prefix)]


class FirecrawlSearchProvider(SearchProvider):
    # çœç•¥å®ç°...
    def __init__(self):
        self.api_key = os.environ.get("FIRECRAWL_API_KEY", "fc-a36b7d2fb273485680d0fe6abd686935")
        if not self.api_key: raise ValueError("Firecrawl API Key is not provided.")
        self.base_url = "https://api.firecrawl.dev/v2/search"
        self.headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        self.prefix = "firecrawl"

    async def search(self, query: str, client: httpx.AsyncClient, num_results: int,
                     search_type: Literal['web', 'video'] = 'web') -> List[Dict[str, Any]]:
        # if search_type == 'video': return []
        try:
            data = {"query": query, "limit": num_results}
            resp = await client.post(self.base_url, headers=self.headers, json=data, timeout=30)
            resp.raise_for_status()
            api_results = resp.json().get("data", {}).get("web", [])
            results = []
            for item in api_results:
                if not item.get("url"): continue
                base_info = {"type": search_type, "url": item.get("url"), "title": item.get("title"),
                             "snippet": item.get("description"), "markdown": item.get("markdown", "")}
                if search_type == 'video': base_info.update(_parse_video_url(item.get("url")))
                results.append(self._prefix_keys(base_info, self.prefix))
            return results
        except Exception as e:
            return [self._prefix_keys({"type": search_type, "error": f"Firecrawl request failed for '{query}': {e}"},
                                      self.prefix)]


class TavilySearchProvider(SearchProvider):
    # çœç•¥å®ç°...
    def __init__(self):
        self.api_key = os.environ.get("TAVILY_API_KEY", "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")
        if not self.api_key: raise ValueError("Tavily API Key is not provided.")
        self.async_client = AsyncTavilyClient(self.api_key)
        self.prefix = "tavily"

    async def search(self, query: str, client: httpx.AsyncClient, num_results: int,
                     search_type: Literal['web', 'video'] = 'web') -> List[Dict[str, Any]]:
        # if search_type == 'video': return []
        try:
            api_results = await self.async_client.search(query=query, search_depth="basic", max_results=num_results)
            results = []
            for item in api_results.get("results", []):
                if not item.get("url"): continue
                base_info = {"type": search_type, "url": item.get("url"), "title": item.get("title"),
                             "snippet": item.get("content"), "score": item.get("score")}
                if search_type == 'video': base_info.update(_parse_video_url(item.get("url")))
                results.append(self._prefix_keys(base_info, self.prefix))
            return results
        except Exception as e:
            return [
                self._prefix_keys({"type": search_type, "error": f"Tavily request failed for '{query}': {e}"},
                                  self.prefix)]


class ZhiLianJobProvider:
    """ä¸€ä¸ªâ€œè™šæ‹Ÿâ€æä¾›å•†ï¼Œä»…ç”¨äºæå–å’Œä¼ é€’èŒä¸šæ•°æ®ã€‚"""

    def get_data(self, career_query_input: Dict) -> Dict:
        print("ğŸ’¼ [ZhiLianJobProvider] æ­£åœ¨æå–èŒä¸šæ•°æ®...")
        return career_query_input


class TianyanCheckProvider:
    def get_data(self, tianyan_input: Any) -> Any:
        print("ğŸ”­ [TianyanCheckProvider] æ­£åœ¨æå–ä¼ä¸šæ•°æ®...")
        return tianyan_input


# --- æ ¸å¿ƒæ£€ç´¢æ§åˆ¶å™¨ (ä¿æŒä¸å˜) ---
# class MultiSourceSearcher:
#     # çœç•¥å®ç°...
#     def __init__(self):
#         self.providers: Dict[str, SearchProvider] = {"searchapi_io": SearchApiIoProvider(),
#                                                      "jina": JinaSearchProvider(),
#                                                      "firecrawl": FirecrawlSearchProvider(),
#                                                      "tavily": TavilySearchProvider()}
#
#     async def _search_single_provider(self, queries, provider_name, client, web_count, video_count):
#         provider = self.providers[provider_name]
#         tasks = []
#         for q in queries:
#             if web_count > 0: tasks.append(provider.search(q, client, web_count, 'web'))
#             if video_count > 0: tasks.append(provider.search(q, client, video_count, 'video'))
#         task_results = await asyncio.gather(*tasks, return_exceptions=True)
#         urls_info, task_idx = [], 0
#         for query in queries:
#             res = {"query": query, "web_results": [], "video_results": [], "errors": []}
#             for stype, count, key in [('web', web_count, 'web_results'), ('video', video_count, 'video_results')]:
#                 if count > 0:
#                     result = task_results[task_idx]
#                     task_idx += 1
#                     if isinstance(result, Exception):
#                         res["errors"].append(f"[{provider.prefix}] {stype.capitalize()} search task failed: {result}")
#                     elif result and result[0].get(f'{provider.prefix}_error'):
#                         res["errors"].append(result[0][f'{provider.prefix}_error'])
#                     else:
#                         res[key] = result
#             urls_info.append(res)
#         return urls_info
#
#     async def _search_all_providers(self, queries, client, web_count, video_count):
#         async def search_and_tag(p_name, provider, query, num, stype):
#             try:
#                 data = await provider.search(query, client, num, stype)
#                 return {"query": query, "provider": p_name, "type": stype, "data": data, "exception": None}
#             except Exception as e:
#                 return {"query": query, "provider": p_name, "type": stype, "data": [], "exception": e}
#
#         tasks = []
#         for q in queries:
#             for name, provider in self.providers.items():
#                 if web_count > 0: tasks.append(search_and_tag(name, provider, q, web_count, 'web'))
#                 if video_count > 0: tasks.append(search_and_tag(name, provider, q, video_count, 'video'))
#         task_results = await asyncio.gather(*tasks)
#         agg = {q: {"query": q, "web_results": [], "video_results": [], "errors": []} for q in queries}
#         for res in task_results:
#             query, p_name, stype, data, exc = res["query"], res["provider"], res["type"], res["data"], res["exception"]
#             provider_prefix = self.providers[p_name].prefix
#             if exc:
#                 agg[query]["errors"].append(
#                     f"[{provider_prefix}] {stype.capitalize()} search task threw an exception: {exc}")
#                 continue
#             if data and data[0].get(f'{provider_prefix}_error'):
#                 agg[query]["errors"].append(data[0][f'{provider_prefix}_error'])
#             elif stype == 'web':
#                 agg[query]["web_results"].extend(data)
#             elif stype == 'video':
#                 agg[query]["video_results"].extend(data)
#         return list(agg.values())
#
#     async def search(self, queries, provider_name, web_count, video_count):
#         async with httpx.AsyncClient() as client:
#             if provider_name.lower() == "all":
#                 return await self._search_all_providers(queries, client, web_count, video_count)
#             else:
#                 if provider_name.lower() not in self.providers: raise ValueError(
#                     f"Provider '{provider_name}' not found.")
#                 return await self._search_single_provider(queries, provider_name.lower(), client, web_count,
#                                                           video_count)
#
#
# # --- Dify å¼‚æ­¥ä¸»å‡½æ•° ---
# async def main_async(raw_input: Any, provider_name: str, web_results_count: int, video_results_count: int) -> Dict[
#     str, Any]:
#     # è¿™ä¸ªå‡½æ•°ç°åœ¨åªå¤„ç†æˆåŠŸè·¯å¾„ï¼Œæ‰€æœ‰å¼‚å¸¸ç”±è°ƒç”¨è€… main æ•è·
#     parsed_input = _intelligent_input_parser(raw_input)
#     queries = parsed_input["queries"]
#     searcher = MultiSourceSearcher()
#     urls_info = await searcher.search(
#         queries,
#         provider_name=provider_name,
#         web_count=web_results_count,
#         video_count=video_results_count
#     )
#     return {
#         "urls_info": urls_info,
#         "urls_info_str": json.dumps(urls_info, ensure_ascii=False, indent=2)
#     }
#
#
# # --- Dify åŒæ­¥å…¥å£ (å·²å½»åº•é‡æ„é”™è¯¯å¤„ç†å’Œè¾“å…¥éªŒè¯) ---
# def main(
#         raw_input: Any,
#         provider: str = "tavily",
#         web_results: Any = 3,
#         video_results: Any = 0
# ) -> Dict[str, Any]:
#     """
#     DifyåŒæ­¥å…¥å£ã€‚ä»»ä½•æƒ…å†µä¸‹éƒ½è¿”å› {"urls_info": [], "urls_info_str": ""} ç»“æ„ã€‚
#     """
#     try:
#         # **å…³é”®ä¿®æ­£ 1: å¥å£®çš„è¾“å…¥å¤„ç†**
#         # Dify å¯èƒ½ä¼ å…¥ None æˆ– ""ï¼Œåœ¨è¿™é‡Œå°†å…¶è½¬æ¢ä¸ºæœ‰æ•ˆçš„æ•´æ•°
#         try:
#             web_count = int(web_results) if web_results not in [None, ""] else 3
#         except (ValueError, TypeError):
#             web_count = 3  # å¦‚æœä¼ å…¥æ— æ³•è½¬æ¢çš„å­—ç¬¦ä¸²ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
#
#         try:
#             video_count = int(video_results) if video_results not in [None, ""] else 0
#         except (ValueError, TypeError):
#             video_count = 0  # å¦‚æœä¼ å…¥æ— æ³•è½¬æ¢çš„å­—ç¬¦ä¸²ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
#
#         # è¿è¡Œæ ¸å¿ƒå¼‚æ­¥é€»è¾‘
#         return asyncio.run(main_async(
#             raw_input=raw_input,
#             provider_name=provider,
#             web_results_count=web_count,
#             video_results_count=video_count
#         ))
#
#     except Exception as e:
#         # **å…³é”®ä¿®æ­£ 2: ç»Ÿä¸€çš„é”™è¯¯è¾“å‡ºç»“æ„**
#         # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œå¹¶å°†å…¶æ‰“åŒ…åˆ°ç¬¦åˆè¾“å‡ºå˜é‡å®šä¹‰çš„å­—å…¸ä¸­
#         error_message = f"An error occurred in the node: {str(e)}"
#         # åˆ›å»ºä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„ payloadï¼Œå…¶ç»“æ„ä¸æ­£å¸¸è¾“å‡ºçš„ urls_info ä¸€è‡´
#         error_payload = [
#             {
#                 "query": "NODE_EXECUTION_ERROR",
#                 "web_results": [],
#                 "video_results": [],
#                 "errors": [error_message, traceback.format_exc()]  # åŒ…å«ç®€çŸ­å’Œè¯¦ç»†çš„é”™è¯¯
#             }
#         ]
#
#         # è¿”å›ä¸æˆåŠŸæ—¶å®Œå…¨ç›¸åŒçš„ key
#         return {
#             "urls_info": error_payload,
#             "urls_info_str": json.dumps(error_payload, ensure_ascii=False, indent=2)
#         }
class MultiSourceSearcher:
    def __init__(self):
        # ã€è°ƒæ•´ã€‘ä»…ä»ç±»å¼•ç”¨æ˜ å°„å¼€å§‹ï¼Œåˆå§‹åŒ–æ—¶ä¸åˆ›å»ºä»»ä½•å®ä¾‹
        # 1. Web æœç´¢ç±»æ˜ å°„
        self.web_provider_classes: Dict[str, Any] = {
            "searchapi_io": SearchApiIoProvider,
            "jina": JinaSearchProvider,
            "firecrawl": FirecrawlSearchProvider,
            "tavily": TavilySearchProvider
        }

        # ã€è°ƒæ•´ã€‘2. å¢åŠ ï¼šéWeb (è¾…åŠ©) æœåŠ¡çš„æ‡’åŠ è½½æ˜ å°„
        # è¿™æ ·å¯ä»¥å°† ZhiLian å’Œ Tianyan ä¹Ÿçº³å…¥æ‡’åŠ è½½ç®¡ç†
        self.auxiliary_provider_classes: Dict[str, Any] = {
            "zhilian_job": ZhiLianJobProvider,
            "tianyan_check_enterprises": TianyanCheckProvider
        }
        # ã€è°ƒæ•´ã€‘3. ç»Ÿä¸€çš„å®ä¾‹ç¼“å­˜æ± 
        self.active_instances: Dict[str, Any] = {}

    def get_web_provider_names(self) -> List[str]:
        """è·å–æ”¯æŒçš„ web provider åç§°åˆ—è¡¨"""
        return list(self.web_provider_classes.keys())

    def _get_provider_instance(self, p_name: str) -> Any:
        # ã€è°ƒæ•´ã€‘é€šç”¨æ‡’åŠ è½½å·¥å‚æ–¹æ³•
        # å¦‚æœå®ä¾‹å·²å­˜åœ¨ç¼“å­˜ä¸­ï¼Œç›´æ¥è¿”å›
        if p_name in self.active_instances:
            return self.active_instances[p_name]
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Web Provider
        if p_name in self.web_provider_classes:
            print(f"ğŸ”Œ [System] Initializing WEB provider: {p_name}...")
            instance = self.web_provider_classes[p_name]()
            self.active_instances[p_name] = instance
            return instance

        # æ£€æŸ¥æ˜¯å¦æ˜¯ è¾…åŠ© Provider
        elif p_name in self.auxiliary_provider_classes:
            print(f"ğŸ”Œ [System] Initializing AUX provider: {p_name}...")
            instance = self.auxiliary_provider_classes[p_name]()
            self.active_instances[p_name] = instance
            return instance

        else:
            raise ValueError(f"Provider '{p_name}' not supported.")

    # æš´éœ²ç»™å¤–éƒ¨è°ƒç”¨çš„ç‰¹å®š getterï¼Œç¡®ä¿ç±»å‹å®‰å…¨å’Œæ‡’åŠ è½½è§¦å‘
    def get_zhilian_provider(self) -> ZhiLianJobProvider:
        return self._get_provider_instance("zhilian_job")

    def get_tianyan_provider(self) -> TianyanCheckProvider:
        return self._get_provider_instance("tianyan_check_enterprises")

    async def web_search(self, queries: List[str], providers_to_use: List[str], client: httpx.AsyncClient,
                         search_types: List[str], web_results_per_type: int, video_results_count: int,
                         regional_data: Optional[Dict[str, str]] = None,
                         time_filter: Optional[Dict[str, str]] = None
                         ) -> List[Dict[str, Any]]:
        # ... [è¿™é‡Œçš„è½®è¯¢è°ƒåº¦é€»è¾‘ä¿æŒä¸å˜] ...
        async def search_and_tag(p_name: str, original_query: str, num: int, stype: str, is_regional: bool):
            provider = self._get_provider_instance(p_name)
            filtered_query = _build_filtered_query(original_query, stype, regional_data=regional_data,
                                                   use_regional_patterns=is_regional,
                                                   time_filter=time_filter)
            # æ‰“ä¸åŒçš„æ—¥å¿— tag æ–¹ä¾¿åŒºåˆ†
            log_tag = "Regional" if is_regional else "Standard"
            # print(f"  ->  Query: \"{filtered_query}\"")
            print(
                f"  -> [Task Scheduled] Provider: {p_name}, [Task: {stype} | {log_tag}], Results: {num}, Query: \"{filtered_query}\"")
            try:
                data = await provider.search(filtered_query, client, num, stype)
                # åœ¨è¿”å›ç»“æœä¸­æ˜ç¡®æ ‡è®°åŸå§‹æŸ¥è¯¢å’Œæœç´¢ç±»å‹
                return {"original_query": original_query, "search_type": stype, "provider": p_name, "data": data}
            except Exception as e:
                error_data = [
                    provider._prefix_keys({"type": stype, "error": f"Task failed for '{original_query}': {e}"},
                                          provider.prefix)]
                return {"original_query": original_query, "search_type": stype, "provider": p_name, "data": error_data}

        provider_cycle = cycle(providers_to_use)
        tasks = []
        # ä¸ºæ¯ä¸ª query å’Œæ¯ä¸ª search_type åˆ›å»ºä»»åŠ¡
        for query in queries:
            for stype in search_types:
                assigned_provider = next(provider_cycle)

                # ã€è§£è€¦ã€‘&ã€ä¼˜åŒ–ã€‘é€»è¾‘
                num_results_for_task = 0
                if stype == 'video':
                    num_results_for_task = video_results_count
                else:  # 'web', 'industry_reports', etc.
                    num_results_for_task = web_results_per_type

                # ã€ä¼˜åŒ–ã€‘å¦‚æœè¯·æ±‚çš„ç»“æœæ•°ä¸º0ï¼Œåˆ™ç›´æ¥è·³è¿‡ï¼Œä¸åˆ›å»ºä»»åŠ¡
                if num_results_for_task <= 0:
                    print(f"  -> [Task Skipped] Type: {stype} requested 0 results for query: \"{query}\"")
                    continue

                strategy_config = SEARCH_STRATEGY_CONFIG.get(stype, {})

                # --- ä»»åŠ¡ A: æ ‡å‡†æ£€ç´¢ (æ°¸è¿œæ‰§è¡Œ) ---
                tasks.append(search_and_tag(
                    assigned_provider,
                    query,
                    num_results_for_task,
                    stype,
                    is_regional=False
                ))
                # --- ä»»åŠ¡ B: åŒºåŸŸæ€§æ£€ç´¢ (æ¡ä»¶è§¦å‘) ---
                # åªæœ‰å½“ï¼šä¸æ˜¯è§†é¢‘æœç´¢ + æä¾›äº†åŒºåŸŸæ•°æ® + è¯¥ç±»å‹é…ç½®äº† regional_patterns æ—¶æ‰æ‰§è¡Œ
                if (stype != 'video' and
                        regional_data and
                        "regional_patterns" in strategy_config):
                    # è¿™é‡Œä¹Ÿå¯ä»¥é€‰æ‹©è½®è¯¢ä¸‹ä¸€ä¸ª providerï¼Œæˆ–è€…å¤ç”¨å½“å‰çš„ï¼Œè¿™é‡Œå¤ç”¨ä»¥ä¿æŒç›¸å…³æ€§
                    # ä¹Ÿå¯ä»¥ next(provider_cycle) æ¥åˆ†æ•£å‹åŠ›

                    print(f"  -> [System] Detected regional data for {stype}, spawning extra regional task.")
                    tasks.append(search_and_tag(
                        assigned_provider,
                        query,
                        num_results_for_task,  # ä¹Ÿå¯ä»¥ç»™åŒºåŸŸæœç´¢è®¾ç½®ç‹¬ç«‹çš„ numï¼Œè¿™é‡Œæš‚æ—¶ç”¨ç›¸åŒçš„
                        stype,
                        is_regional=True
                    ))

                # tasks.append(search_and_tag(assigned_provider, query, num_results_for_task, stype))
        if not tasks: return []  # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œç›´æ¥è¿”å›
        task_results = await asyncio.gather(*tasks)

        # èšåˆç»“æœ
        # æ–°çš„èšåˆç»“æ„: { "query_string": { "type_A_results": [], "type_B_results": [], "errors": [] } }
        agg_by_query = {q: {"query": q, "errors": []} for q in queries}
        for res in task_results:
            original_query = res["original_query"]
            stype = res["search_type"]
            p_name = res["provider"]
            data = res["data"]

            provider_instance = self._get_provider_instance(p_name)
            provider_prefix = getattr(provider_instance, 'prefix', p_name)

            # åˆå§‹åŒ–è¯¥ç±»å‹çš„ç»“æœåˆ—è¡¨ (ä¾‹å¦‚: "industry_reports_results")
            results_key = f"{stype}_results"
            if results_key not in agg_by_query[original_query]:
                agg_by_query[original_query][results_key] = []
            for item in data:
                if item.get(f'{provider_prefix}_error'):
                    agg_by_query[original_query]["errors"].append(item[f'{provider_prefix}_error'])
                else:
                    agg_by_query[original_query][results_key].append(item)

        return list(agg_by_query.values())


# --- 5. Dify å¼‚æ­¥ä¸»å‡½æ•° (æ€»æŒ‡æŒ¥) ---
EXCLUSIVE_SEARCH_RESULTS_COUNT = 10


async def main_async(raw_input: Any, provider_selection: Union[str, List[str]], search_types: List[str],
                     web_results_per_type: int, video_results_count: int,
                     regional_data: Optional[Dict[str, str]] = None,
                     time_filter_input: Any = None) -> Dict[str, Any]:
    # 1. è§£ææ‰€æœ‰æ½œåœ¨è¾“å…¥
    parsed_data = _intelligent_input_parser(raw_input)
    comprehensive_queries = parsed_data["comprehensive_queries"]
    career_query_data = parsed_data["career_query_data"]
    tianyan_enterprise_names = parsed_data["tianyan_enterprise_names"]

    # 1.1 è§£ææ—¶é—´è¿‡æ»¤
    time_filter = _parse_time_filter(time_filter_input)
    if time_filter:
        print(f"[Time Filter] Applied: {time_filter}")

    # 2. åˆå§‹åŒ–ç»“æœå®¹å™¨
    comprehensive_results = []
    career_results = {}
    tianyan_results: List[str] = []

    # 3. å¤„ç†ä¸“å±è§„åˆ™ (Exclusive Rules)
    exclusive_queries = []
    # æ¸…ç† search_typesï¼Œç§»é™¤ 'exclusive_rules' æ ‡è®°
    is_exclusive_requested = "exclusive_rules" in search_types
    if is_exclusive_requested:
        # ç§»é™¤æ ‡è®°ï¼Œé¿å…ä¸‹æ¸¸æŠ¥é”™ï¼Œä½†è¿™æ—¶å€™ä¸ç”Ÿæˆ queryï¼Œé™¤éçœŸè¦æœ
        search_types = [t for t in search_types if t != "exclusive_rules"]
        # å¦‚æœç§»å‡ºåä¸ºç©ºï¼Œè¡¥å› web
        if not search_types: search_types = ["web"]
        # åªæœ‰åœ¨éœ€è¦æ—¶ç”Ÿæˆ
        exclusive_queries = _generate_exclusive_queries(regional_data)

    # å¦‚æœæ¸…ç†å search_types ä¸ºç©ºï¼Œåˆ™è®¾ç½®é»˜è®¤å€¼ï¼Œä»¥ç¡®ä¿ä¸“å±æŸ¥è¯¢å¯ä»¥æ‰§è¡Œ
    effective_search_types = search_types if search_types else ["web"]

    has_web_work = bool(comprehensive_queries) or bool(exclusive_queries)

    # 3.1 å¤„ç† Provider é€‰æ‹©é€»è¾‘ (ä»…å½“æœ‰å·¥ä½œæ—¶æ‰æ·±å…¥å¤„ç†)
    selected_providers = []
    web_search_providers_to_use = []
    is_zhilian_requested = False
    is_tianyan_requested = False

    # å³ä½¿æ²¡æœ‰ web å·¥ä½œï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦è§£æ provider ä»¥ç¡®å®šæ˜¯å¦è¿è¡Œ zhilian/tianyan
    # ä½†æˆ‘ä»¬ä¸éœ€è¦å®ä¾‹åŒ– Searcher
    if isinstance(provider_selection, str):
        selected_providers = [p.strip().lower() for p in provider_selection.split(',')]
    elif isinstance(provider_selection, list):
        selected_providers = [str(p).lower() for p in provider_selection]

    # æ£€æŸ¥é Web ä»»åŠ¡
    is_zhilian_requested = "zhilian_job" in selected_providers
    is_tianyan_requested = "tianyan_check_enterprises" in selected_providers
    # 4. æ‰§è¡Œ Web æœç´¢ (å¦‚æœéœ€è¦) - è¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†ï¼ŒåŠ äº†ä¸¥é‡çš„é˜²å®ˆé€»è¾‘
    if has_web_work:

        # 3. è§£æç”¨æˆ·é€‰æ‹©çš„ provider
        searcher = MultiSourceSearcher()
        all_web_provider_names = searcher.get_web_provider_names()

        # å¤„ç† "all" å…³é”®å­—
        if "all" in selected_providers:
            # å°† "all" æ›¿æ¢ä¸ºæ‰€æœ‰ web providerï¼Œå¹¶ä¸å…¶ä»–ç‰¹æ®Šä»»åŠ¡å»é‡åˆå¹¶
            web_search_providers_to_use = all_web_provider_names
        else:
            web_search_providers_to_use = [p for p in selected_providers if p in all_web_provider_names]
            # selected_providers = list(set(selected_providers + all_web_provider_names))

    # # 4. ä»»åŠ¡åˆ†æ´¾ä¸æ‰§è¡Œ
    # web_search_providers_to_use = [p for p in selected_providers if p in all_web_provider_names]
    # is_zhilian_requested = "zhilian_job" in selected_providers
    # is_tianyan_requested = "tianyan_check_enterprises" in selected_providers

    # # 4.1 æ‰§è¡ŒWebæœç´¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
    # if web_search_providers_to_use and comprehensive_queries:
    #     print(f"ğŸŒ [Web Search] ä½¿ç”¨ {web_search_providers_to_use} æœç´¢ {len(comprehensive_queries)} ä¸ªæŸ¥è¯¢...")
    #     async with httpx.AsyncClient(http2=True, verify=False) as client:
    #         comprehensive_results = await searcher.web_search(
    #             queries=comprehensive_queries,
    #             providers_to_use=web_search_providers_to_use,
    #             client=client,
    #             search_types=search_types,
    #             web_results_per_type=web_results_per_type,
    #             video_results_count=video_results_count
    #         )
    #
    # else:
    #     print("ğŸŸ¡ [Web Search] æ— éœ€æ‰§è¡ŒWebæœç´¢ã€‚(æŸ¥è¯¢ä¸ºç©ºæˆ–æœªé€‰æ‹©ä»»ä½•æœ‰æ•ˆçš„Webæä¾›å•†)")

    if web_search_providers_to_use:
        # http2=True éœ€è¦å®‰è£… h2 åº“ï¼Œå¦‚æœæ²¡è£…ä¼šæŠ¥é”™ã€‚ä¸ºä¿é™©èµ·è§ï¼Œè¿™é‡Œå…ˆå…³æ‰ http2ï¼Œæˆ–è€…æ”¹ä¸º try-except è‡ªåŠ¨é™çº§
        # async with httpx.AsyncClient(http2=True, verify=False) as client:
        async with httpx.AsyncClient(http2=False, verify=False) as client:
            async_tasks = []
            # 5.1 åˆ›å»ºæ™®é€šæŸ¥è¯¢ä»»åŠ¡ (å¦‚æœå­˜åœ¨)
            if comprehensive_queries:
                print(
                    f"  -> [Task Group 1: Normal] Scheduling {len(comprehensive_queries)} queries, requesting {web_results_per_type} results each.")
                normal_task = searcher.web_search(
                    queries=comprehensive_queries,
                    providers_to_use=web_search_providers_to_use,
                    client=client,
                    search_types=effective_search_types,
                    web_results_per_type=web_results_per_type,
                    video_results_count=video_results_count,
                    regional_data=regional_data,
                    time_filter=time_filter
                )
                async_tasks.append(normal_task)
            # 5.2 åˆ›å»ºä¸“å±æŸ¥è¯¢ä»»åŠ¡ (å¦‚æœå­˜åœ¨)
            if is_exclusive_requested and exclusive_queries:
                print(
                    f"  -> [Task Group 2: Exclusive] Scheduling {len(exclusive_queries)} queries, requesting {EXCLUSIVE_SEARCH_RESULTS_COUNT} results each.")
                # æ³¨æ„ï¼šä¸“å±æŸ¥è¯¢é€šå¸¸æ˜¯ç½‘é¡µæœç´¢ï¼Œæ‰€ä»¥ video_count=0
                exclusive_task = searcher.web_search(
                    queries=exclusive_queries,
                    providers_to_use=web_search_providers_to_use,
                    client=client,
                    search_types=["exclusive_rules"],
                    web_results_per_type=EXCLUSIVE_SEARCH_RESULTS_COUNT,  # <-- ä½¿ç”¨ä¸“å±çš„å›ºå®šæ•°é‡
                    video_results_count=0,
                    time_filter=time_filter  # ä¸“å±æŸ¥è¯¢ä¹Ÿåº”ç”¨æ—¶é—´è¿‡æ»¤
                )
                async_tasks.append(exclusive_task)
            # 5.3 å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ç»„
            if async_tasks:
                print(f"ğŸŒ [Web Search] Executing {len(async_tasks)} task group(s) concurrently...")
                # gather ä¼šè¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ web_search è°ƒç”¨çš„ç»“æœ (ä¹Ÿæ˜¯ä¸€ä¸ªåˆ—è¡¨)
                # ä¾‹å¦‚: [ [normal_results], [exclusive_results] ]
                all_results_groups = await asyncio.gather(*async_tasks)

                # 5.4 åˆå¹¶ç»“æœ
                for result_group in all_results_groups:
                    comprehensive_results.extend(result_group)
            else:
                print("ğŸŸ¡ [Web Search] æ— éœ€æ‰§è¡ŒWebæœç´¢ã€‚(æŸ¥è¯¢ä¸ºç©ºæˆ–æœªé€‰æ‹©ä»»ä½•æœ‰æ•ˆçš„Webæä¾›å•†)")

    # 4.2 æ‰§è¡ŒZhiLianæ•°æ®æå–ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if is_zhilian_requested:
        career_results = searcher.get_zhilian_provider().get_data(career_query_data)
    else:
        print("ğŸŸ¡ [ZhiLian] æœªè¯·æ±‚æ‹›è˜æ•°æ®æå–ã€‚")

    # 4.3 æ‰§è¡ŒTianyanæ•°æ®æå–ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if is_tianyan_requested:
        tianyan_results = searcher.get_tianyan_provider().get_data(tianyan_enterprise_names)
    else:
        print("ğŸŸ¡ [Tianyan] æœªè¯·æ±‚ä¼ä¸šæ•°æ®æå–ã€‚")

    # 5. ç»„è£…æœ€ç»ˆè¾“å‡º
    final_output = {
        "datas": {
            "comprehensive_data": comprehensive_results,
            "career_data": career_results,
            "tianyan_check_data": tianyan_results
        }
    }

    return {
        "datas": final_output["datas"],
        "datas_str": json.dumps(final_output, ensure_ascii=False, indent=2)
    }


# --- Dify åŒæ­¥å…¥å£ ---
# ã€è°ƒæ•´ã€‘é‡æ„ main å‡½æ•°ä»¥é€‚åº”æ–°çš„å¼‚æ­¥é€»è¾‘å’Œæ›´å¤æ‚çš„ provider è¾“å…¥
def main(
        raw_input: Any,
        provider: Union[str, List[str]] = "tavily",
        search_types: Union[str, List[str]] = "web",
        web_results_per_type: Any = 3,
        video_results_count: Any = 2,
        regional_rules: Any = None,
        time_filter: Any = None
) -> Dict[str, Any]:
    # å®šä¹‰ä¸€ä¸ªæ ‡å‡†çš„ç©º/é”™è¯¯è¿”å›ç»“æ„
    error_datas_structure = {"comprehensive_data": [], "career_data": {}, "tianyan_check_data": []}

    def construct_error_payload(e, trace):
        error_message = f"An error occurred in the node: {str(e)}"
        error_datas_structure["comprehensive_data"] = [
            {"query": "NODE_EXECUTION_ERROR", "web_results": [], "video_results": [], "errors": [error_message, trace]}
        ]
        return {
            "datas": error_datas_structure,
            "datas_str": json.dumps({"datas": error_datas_structure}, ensure_ascii=False, indent=2)
        }

    try:
        # 1. å¥å£®åœ°å¤„ç† provider è¾“å…¥
        provider_selection = provider
        # if isinstance(provider, str) and provider.strip().startswith('[') and provider.strip().endswith(']'):
        #     try:
        #         # å°è¯•å°†å­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨è§£æä¸ºçœŸå®çš„ Python åˆ—è¡¨
        #         provider_selection = json.loads(provider)
        #     except json.JSONDecodeError:
        #         raise ValueError(f"Provider input '{provider}' looks like a list but is not valid JSON.")
        if isinstance(provider, str):
            cleaned_provider = provider.strip()

            # --- æ£€æµ‹æ˜¯å¦æ˜¯åˆ—è¡¨æ ¼å¼ "[...]" ---
            if cleaned_provider.startswith('[') and cleaned_provider.endswith(']'):
                try:
                    # å°è¯•1: æ ‡å‡† JSON è§£æ (è¦æ±‚åŒå¼•å·)
                    provider_selection = json.loads(cleaned_provider)
                except json.JSONDecodeError:
                    try:
                        # å°è¯•2: ä½¿ç”¨ json_repair (å¯ä»¥è‡ªåŠ¨æŠŠå•å¼•å·ä¿®æˆåŒå¼•å·ï¼Œå®Œç¾è§£å†³æ‚¨çš„é—®é¢˜)
                        provider_selection = json_repair.loads(cleaned_provider)
                    except Exception:
                        # å°è¯•3: æœ€åçš„æš´åŠ›å…œåº• (æ‰‹åŠ¨å»é™¤æ‹¬å·å’Œå¼•å·)
                        # é€»è¾‘ï¼šå»æ‰é¦–å°¾æ‹¬å· -> æŒ‰é€—å·åˆ†å‰² -> å»æ‰æ¯ä¸€é¡¹å‘¨å›´çš„ç©ºæ ¼å’Œå•/åŒå¼•å·
                        inner_content = cleaned_provider[1:-1]
                        provider_selection = [
                            item.strip().strip("'").strip('"')
                            for item in inner_content.split(',')
                            if item.strip()
                        ]

            # --- æ£€æµ‹æ˜¯å¦æ˜¯é€—å·åˆ†éš”å­—ç¬¦ä¸² "a, b" (éåˆ—è¡¨æ ¼å¼) ---
            elif ',' in cleaned_provider:
                provider_selection = [p.strip() for p in cleaned_provider.split(',') if p.strip()]

        # 2. ã€æ–°å¢ã€‘å¥å£®åœ°å¤„ç† search_types è¾“å…¥
        search_types_list = []
        if isinstance(search_types, str):
            try:
                # å°è¯•è§£æ JSON å­—ç¬¦ä¸² (e.g., '["web", "industry_reports"]')
                parsed_list = json.loads(search_types)
                if isinstance(parsed_list, list):
                    search_types_list = parsed_list
                else:
                    raise ValueError("Input is not a list.")
            except (json.JSONDecodeError, ValueError):
                # å¦‚æœå¤±è´¥ï¼Œåˆ™æŒ‰é€—å·åˆ†å‰² (e.g., "web,video")
                search_types_list = [t.strip() for t in search_types.split(',') if t.strip()]
        elif isinstance(search_types, list):
            search_types_list = search_types

        # 3. å¥å£®åœ°å¤„ç† regional_rules è¾“å…¥
        regional_data_dict = {}
        if isinstance(regional_rules, dict):
            regional_data_dict = regional_rules
        elif isinstance(regional_rules, str) and regional_rules.strip():
            try:
                # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                parsed_data = json_repair.loads(regional_rules)
                if isinstance(parsed_data, dict):
                    regional_data_dict = parsed_data
            except Exception as e:
                print(f"  -> [Warning] Failed to parse regional_rules as JSON: {e}. It will be ignored.")

        # 4. å¥å£®åœ°å¤„ç†æ•°å­—è¾“å…¥
        try:
            web_count = int(web_results_per_type) if web_results_per_type not in [None, ""] else 3
        except (ValueError, TypeError):
            web_count = 3
        try:
            video_count = int(video_results_count) if video_results_count not in [None, ""] else 2
        except (ValueError, TypeError):
            video_count = 2

        # 4. è¿è¡Œæ ¸å¿ƒå¼‚æ­¥é€»è¾‘
        res = asyncio.run(main_async(
            raw_input=raw_input,
            provider_selection=provider_selection,
            search_types=search_types_list,
            web_results_per_type=web_count,
            video_results_count=video_count,
            regional_data=regional_data_dict,
            time_filter_input=time_filter
        ))

        return _dify_debug_return(res, label='Success')
    except Exception as e:
        trace = traceback.format_exc()
        print(f"!! èŠ‚ç‚¹æ‰§è¡Œæ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}\n{trace}")
        error_payload = construct_error_payload(e, trace)
        return _dify_debug_return(error_payload, label='Exception')


main({
    "doc_id": "comprehensive_query_set_001",
    "web_queries": {
        "career_query": {
        },
        "comprehensive_query": [
            "ä¿è‚²å‘˜",
        ],
        "tianyan_check_enterprise": [
        ]
    }
}, ["searchapi_io"], [
    "web"
], web_results_per_type="3", regional_rules={"school": "ä¸­åŒ»åŒ»ç–—", "major": "åŒ»å­¦å‚ç›´ç±»ä¸“ä¸š", "scope": "æ¹–åŒ—"},
    time_filter="2026-01-22"
)

# async def main_async(raw_input: Any, provider_selection: Union[str, List[str]], web_results_count: int,
#                      video_results_count: int) -> Dict[str, Any]:
#     # 1. è§£æè¾“å…¥
#     parsed_data = _intelligent_input_parser(raw_input)
#     comprehensive_queries = parsed_data["comprehensive_queries"]
#     career_query_data = parsed_data["career_query_data"]
#     tianyan_enterprise_name = parsed_data["tianyan_enterprise_name"]  # ã€è°ƒæ•´ã€‘è·å–æ–°æ•°æ®
#
#     # 2. åˆå§‹åŒ–ç»“æœå®¹å™¨
#     comprehensive_results = []
#     career_results = {}
#     tianyan_results = ""  # ã€è°ƒæ•´ã€‘åˆå§‹åŒ–æ–°ç»“æœå®¹å™¨
#
#     # 3. è§£æå’Œåˆ†æ´¾ä»»åŠ¡
#     searcher = MultiSourceSearcher()
#     all_web_provider_names = list(searcher.web_providers.keys())
#
#     selected_providers = []
#     if isinstance(provider_selection, str):
#         if provider_selection.lower() == "all":
#             selected_providers = all_web_provider_names
#         else:
#             selected_providers = [provider_selection.lower()]
#     elif isinstance(provider_selection, list):
#         selected_providers = [str(p).lower() for p in provider_selection]
#     # ã€è°ƒæ•´ã€‘ä»»åŠ¡åˆ†æ´¾é€»è¾‘
#     web_search_providers_to_use = [p for p in selected_providers if p in all_web_provider_names]
#     is_zhilian_requested = "zhilian_job" in selected_providers
#     is_tianyan_requested = "tianyan_check_enterprises" in selected_providers  # ã€æ–°å¢ã€‘
#     # 3.2 æ‰§è¡ŒWebæœç´¢
#     if web_search_providers_to_use and comprehensive_queries:
#         print(f"ğŸŒ [Web Search] ä½¿ç”¨ {web_search_providers_to_use} æœç´¢ {len(comprehensive_queries)} ä¸ªæŸ¥è¯¢...")
#         async with httpx.AsyncClient() as client:
#             comprehensive_results = await searcher.web_search(queries=comprehensive_queries,
#                                                               providers_to_use=web_search_providers_to_use,
#                                                               client=client, web_count=web_results_count,
#                                                               video_count=video_results_count)
#     else:
#         print("ğŸŸ¡ [Web Search] æ— éœ€æ‰§è¡ŒWebæœç´¢ã€‚")
#     # 3.3 æ‰§è¡ŒZhiLianæ•°æ®æå–
#     if is_zhilian_requested:
#         career_results = searcher.zhilian_provider.get_data(career_query_data)
#
#     # ã€æ–°å¢ã€‘3.4 æ‰§è¡ŒTianyanæ•°æ®æå–
#     if is_tianyan_requested:
#         tianyan_results = searcher.tianyan_provider.get_data(tianyan_enterprise_name)
#     # 4. ã€è°ƒæ•´ã€‘ç»„è£…æœ€ç»ˆè¾“å‡º
#     final_output = {
#         "datas": {
#             "comprehensive_data": comprehensive_results,
#             "career_data": career_results,
#             "tianyan_check_data": tianyan_results  # ã€æ–°å¢ã€‘
#         }
#     }
#     return {
#         "datas": final_output["datas"],  # ã€è°ƒæ•´ã€‘ç›´æ¥è¿”å›dataså¯¹è±¡
#         "datas_str": json.dumps(final_output, ensure_ascii=False, indent=2)
#     }
#
#
# # --- Dify åŒæ­¥å…¥å£ ---
# def main(raw_input: Any, provider: Union[str, List[str]] = "tavily", web_results: Any = 3, video_results: Any = 0) -> \
# Dict[str, Any]:
#     # ã€è°ƒæ•´ã€‘ç»Ÿä¸€çš„é”™è¯¯è¾“å‡ºç»“æ„
#     error_datas_structure = {"comprehensive_data": [], "career_data": {}, "tianyan_check_data": ""}
#     error_payload = {
#         "datas": error_datas_structure,
#         "datas_str": json.dumps({"datas": error_datas_structure}, ensure_ascii=False, indent=2)
#     }
#     try:
#         provider_selection = provider
#         if isinstance(provider, str) and provider.strip().startswith('[') and provider.strip().endswith(']'):
#             try:
#                 provider_selection = json.loads(provider)
#             except json.JSONDecodeError:
#                 raise ValueError(f"Provider input '{provider}' looks like a list but is not valid JSON.")
#
#         try:
#             web_count = int(web_results) if web_results not in [None, ""] else 3
#         except (ValueError, TypeError):
#             web_count = 3
#         try:
#             video_count = int(video_results) if video_results not in [None, ""] else 0
#         except (ValueError, TypeError):
#             video_count = 0
#
#         res = asyncio.run(
#             main_async(raw_input=raw_input, provider_selection=provider_selection, web_results_count=web_count,
#                        video_results_count=video_count))
#         return _dify_debug_return(res, label='Success')
#
#     except Exception as e:
#         error_message = f"An error occurred in the node: {str(e)}"
#         trace = traceback.format_exc()
#
#         # ã€è°ƒæ•´ã€‘å°†é”™è¯¯ä¿¡æ¯æ”¾å…¥ comprehensive_data ä¸­
#         error_datas_structure["comprehensive_data"] = [
#             {"query": "NODE_EXECUTION_ERROR", "web_results": [], "video_results": [], "errors": [error_message, trace]}
#         ]
#         error_payload["datas"] = error_datas_structure
#         error_payload["datas_str"] = json.dumps({"datas": error_datas_structure}, ensure_ascii=False, indent=2)
#         print(f"â€¼ï¸ èŠ‚ç‚¹æ‰§è¡Œæ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}\n{trace}")
#         return _dify_debug_return(error_payload, label='Exception')

# class MultiSourceSearcher:
#     def __init__(self):
#         self.providers: Dict[str, SearchProvider] = {"searchapi_io": SearchApiIoProvider(),
#                                                      "jina": JinaSearchProvider(),
#                                                      "firecrawl": FirecrawlSearchProvider(),
#                                                      "tavily": TavilySearchProvider()}
#
#     async def _search_with_providers(self, queries: List[str], providers_to_use: List[str], client: httpx.AsyncClient,
#                                      web_count: int, video_count: int) -> List[Dict[str, Any]]:
#         async def search_and_tag(p_name: str, query: str, num: int, stype: str):
#             provider = self.providers[p_name]
#             try:
#                 data = await provider.search(query, client, num, stype)
#                 return {"query": query, "provider": p_name, "type": stype, "data": data}
#             except Exception as e:
#                 error_data = [
#                     provider._prefix_keys({"type": stype, "error": f"Task execution failed for '{query}': {e}"},
#                                           provider.prefix)]
#                 return {"query": query, "provider": p_name, "type": stype, "data": error_data}
#
#         provider_cycle = cycle(providers_to_use)
#         tasks_by_query = {q: [] for q in queries}
#         for query in queries:
#             assigned_provider = next(provider_cycle)
#             if web_count > 0:
#                 tasks_by_query[query].append(search_and_tag(assigned_provider, query, web_count, 'web'))
#             if video_count > 0 and assigned_provider == "searchapi_io":  # Only searchapi supports video
#                 tasks_by_query[query].append(search_and_tag(assigned_provider, query, video_count, 'video'))
#         all_tasks = [task for task_list in tasks_by_query.values() for task in task_list]
#         task_results = await asyncio.gather(*all_tasks)
#         # èšåˆç»“æœ
#         agg = {q: {"query": q, "web_results": [], "video_results": [], "errors": []} for q in queries}
#         for res in task_results:
#             query, p_name, stype, data = res["query"], res["provider"], res["type"], res["data"]
#             provider_prefix = self.providers[p_name].prefix
#
#             clean_data = []
#             for item in data:
#                 if item.get(f'{provider_prefix}_error'):
#                     agg[query]["errors"].append(item[f'{provider_prefix}_error'])
#                 else:
#                     clean_data.append(item)
#
#             if stype == 'web':
#                 agg[query]["web_results"].extend(clean_data)
#             elif stype == 'video':
#                 agg[query]["video_results"].extend(clean_data)
#
#         return list(agg.values())
#
#     async def search(self, queries: List[str], provider_selection: Union[str, List[str]], web_count: int,
#                      video_count: int) -> List[Dict[str, Any]]:
#         providers_to_use = []
#         if isinstance(provider_selection, str):
#             if provider_selection.lower() == "all":
#                 providers_to_use = list(self.providers.keys())
#             else:
#                 if provider_selection.lower() not in self.providers: raise ValueError(
#                     f"Provider '{provider_selection}' not found.")
#                 providers_to_use = [provider_selection.lower()]
#         elif isinstance(provider_selection, list):
#             providers_to_use = [p.lower() for p in provider_selection if p.lower() in self.providers]
#             if not providers_to_use: raise ValueError("None of the specified providers are valid.")
#
#         if not providers_to_use: raise ValueError("No valid providers selected for search.")
#         async with httpx.AsyncClient() as client:
#             return await self._search_with_providers(queries, providers_to_use, client, web_count, video_count)
#
#
# # --- Dify å¼‚æ­¥ä¸»å‡½æ•° ---
# async def main_async(raw_input: Any, provider_selection: Union[str, List[str]], web_results_count: int,
#                      video_results_count: int) -> Dict[str, Any]:
#     parsed_input = _intelligent_input_parser(raw_input)
#     queries = parsed_input["queries"]
#     searcher = MultiSourceSearcher()
#     urls_info = await searcher.search(queries, provider_selection=provider_selection, web_count=web_results_count,
#                                       video_count=video_results_count)
#     return {"urls_info": urls_info, "urls_info_str": json.dumps(urls_info, ensure_ascii=False, indent=2)}
#
#
# # --- Dify åŒæ­¥å…¥å£ ---
# def main(raw_input: Any, provider: Union[str, List[str]] = "tavily", web_results: Any = 3, video_results: Any = 0) -> \
#         Dict[str, Any]:
#     try:
#         provider_selection = provider
#         if isinstance(provider, str) and provider.strip().startswith('[') and provider.strip().endswith(']'):
#             try:
#                 provider_selection = json.loads(provider)
#             except json.JSONDecodeError:
#                 raise ValueError(f"Provider input '{provider}' looks like a list but is not valid JSON.")
#         try:
#             web_count = int(web_results) if web_results not in [None, ""] else 3
#         except (ValueError, TypeError):
#             web_count = 3
#         try:
#             video_count = int(video_results) if video_results not in [None, ""] else 0
#         except (ValueError, TypeError):
#             video_count = 0
#         res = asyncio.run(
#             main_async(raw_input=raw_input, provider_selection=provider_selection, web_results_count=web_count,
#                        video_results_count=video_count))
#         return _dify_debug_return(res)
#     except Exception as e:
#         error_payload = [{"query": "NODE_EXECUTION_ERROR", "web_results": [], "video_results": [],
#                           "errors": [f"An error occurred in the node: {str(e)}", traceback.format_exc()]}]
#         err = {"urls_info": error_payload, "urls_info_str": json.dumps(error_payload, ensure_ascii=False, indent=2)}
#         return _dify_debug_return(err, label='exc')
