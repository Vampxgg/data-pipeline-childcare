# coding: utf-8
import asyncio
import json
import httpx
import os
import pprint
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Set, Tuple
from collections import defaultdict


# --- é…ç½®ç®¡ç† ---
class Config:
    # æ›¿æ¢ä¸ºæ‚¨çš„å®é™…é…ç½®
    BASE_IP = os.getenv("DIFY_API_BASE_IP", "119.45.167.133:5125")
    AUTH_TOKEN = os.getenv("DIFY_API_AUTH_TOKEN", "dataset-pJ11Qq6BAfhYR4AJfLGtulbv")
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY", "sk-zdxzbykdzqbmpjlnasfjpapuzdkupupghxsaopftaqnvyfrv")
    SILICONFLOW_RERANK_URL = "https://api.siliconflow.cn/v1/rerank"
    RERANK_MODEL = "BAAI/bge-reranker-v2-m3"
    IS_DEBUG = True
    TIMEOUT = 90


# --- è°ƒè¯•è¾…åŠ© ---
def debug_print(data: Any, label: str = "DEBUG"):
    if Config.IS_DEBUG:
        print(f"\n{'=' * 30} {label} {'=' * 30}")
        pprint.pprint(data, indent=2, width=120)
        print("=" * 70 + "\n")
    return data


# --- æ¨¡å—ä¸€ï¼šDify API å®¢æˆ·ç«¯ ---
class DifyApiClient:
    """è´Ÿè´£ä¸ Dify Dataset æœåŠ¡é€šä¿¡ï¼Œå¹¶æ¸…æ´—æ•°æ®"""

    def __init__(self):
        self.base_url = f"http://{Config.BASE_IP}/v1/datasets"
        self.headers = {
            'Authorization': f'Bearer {Config.AUTH_TOKEN}',
            'Content-Type': 'application/json'
        }
        self.client = httpx.AsyncClient(headers=self.headers, timeout=Config.TIMEOUT)

    async def close(self):
        if not self.client.is_closed:
            await self.client.aclose()

    async def fetch_document_detail(self, database_id: str, document_id: str) -> Dict[str, Any]:
        url = f"{self.base_url}/{database_id}/documents/{document_id}"
        try:
            resp = await self.client.get(url)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            print(f"âš ï¸ [Meta Error] DB: {database_id}, Doc: {document_id} - {e}")
            return {}

    async def fetch_all_segments(self, database_id: str, document_id: str) -> List[Dict]:
        """é€šè¿‡åˆ†é¡µå¾ªç¯ï¼Œè·å–ä¸€ä¸ªæ–‡æ¡£ä¸‹çš„æ‰€æœ‰åˆ‡ç‰‡"""
        all_segments = []
        page = 1
        # æ³¨æ„ï¼šè¿™é‡Œçš„ URL éœ€è¦æ ¹æ®æ‚¨çš„ Dify ç‰ˆæœ¬ç¡®è®¤ã€‚
        # åŸä»£ç é€»è¾‘æ˜¯ /datasets/{db}/documents/{doc}/segments
        url = f"{self.base_url}/{database_id}/documents/{document_id}/segments"

        while True:
            params = {'limit': 100, 'page': page}  # Dify æœ€å¤§ limit é€šå¸¸æ˜¯ 100
            success = False
            for attempt in range(3):
                try:
                    resp = await self.client.get(url, params=params)
                    resp.raise_for_status()
                    data = resp.json()

                    segments = data.get("data", [])
                    if not segments: 
                        success = True
                        break

                    all_segments.extend(segments)

                    if not data.get("has_more", False):
                        success = True
                        break
                    
                    success = True
                    break
                except Exception as e:
                    if attempt < 2:
                        await asyncio.sleep(1)
                        continue
                    print(f"âš ï¸ [Fetch Segments Error] DB:{database_id} Doc:{document_id} Page:{page} - {type(e).__name__}: {e}")
            
            if not success:
                break
            
            if not data.get("has_more", False):
                break
            page += 1

        return all_segments

    async def retrieve(self, query: str, payload: Dict[str, Any]) -> List[Dict[str, Any]]:
        # æå– Dify æ¥å£éœ€è¦çš„ Database ID
        db_id = payload.pop("database_id_for_url")
        url = f"{self.base_url}/{db_id}/retrieve"

        # æ„é€  Dify æ ‡å‡†è¯·æ±‚ä½“
        req_body = {
            "query": query,
            "retrieval_model": {
                "search_method": "hybrid_search",
                "reranking_enable": False,
                "top_k": 100,  # å°½å¯èƒ½å¤šå¬å›ï¼Œè®©åç»­ RRF å’Œå¤–éƒ¨ Rerank å†³å®šæ’å
                "score_threshold_enabled": False,
            }
        }
        if "metadata_filtering_conditions" in payload:
            req_body["retrieval_model"]["metadata_filtering_conditions"] = payload["metadata_filtering_conditions"]

        try:
            resp = await self.client.post(url, json=req_body)
            resp.raise_for_status()
            data = resp.json()

            # æ•°æ®æ¸…æ´—ï¼šå°† Dify å¤æ‚çš„åµŒå¥—ç»“æ„æ‰å¹³åŒ–
            clean_results = []
            for rec in data.get("records", []):
                seg = rec.get("segment", {})
                d = seg.get("document", {})
                if seg.get("content"):
                    clean_results.append({
                        "id": seg.get("id"),
                        "chunk_id": seg.get("id"),  # å…³é”® IDï¼Œç”¨äºå»é‡
                        "content": seg.get("content", ""),
                        "score": rec.get("score", 0.0),  # åŸå§‹åˆ†æ•°
                        "database_id": db_id,
                        "document_id": seg.get("document_id"),
                        "document_name": d.get("name"),
                        "position": seg.get("position", 0),
                        "doc_metadata": d.get("doc_metadata") or d.get("metadata") or {}
                    })
            return clean_results
        except Exception as e:
            print(f"âš ï¸ [Retrieve Error] Query: '{query}' - {e}")
            return []


# --- æ¨¡å—äºŒï¼šRAG æœåŠ¡ (ç®—æ³•æ ¸å¿ƒ) ---
class RagService:
    @staticmethod
    def reciprocal_rank_fusion(list_of_lists: List[List[Dict]], k: int = 60) -> List[Dict]:
        """
        ã€ä¿®æ­£ç‰ˆ RRFã€‘
        è¾“å…¥ï¼šList[List[Dict]] -> åŒ…å«å¤šä¸ª Query æ£€ç´¢ç»“æœçš„åˆ—è¡¨
        é€»è¾‘ï¼šå¹³è¡ŒæŠ•ç¥¨ã€‚æ¯ä¸ªåˆ—è¡¨çš„ç¬¬ n åäº«æœ‰åŒç­‰çš„æƒé‡ã€‚
        """
        scores = defaultdict(float)
        obj_map = {}

        # å¤–å±‚å¾ªç¯ï¼šéå†ä¸åŒçš„ Query æ¥æº (Q1, Q2, Q3...)
        for ranked_list in list_of_lists:
            if not ranked_list: continue

            # å†…å±‚å¾ªç¯ï¼šéå†è¯¥ Query ä¸‹çš„æ’å
            for rank, item in enumerate(ranked_list):
                cid = item.get('chunk_id')
                if not cid: continue

                # è®°å½•å¯¹è±¡ä»¥ä¾¿æœ€åè¿”å›
                if cid not in obj_map:
                    obj_map[cid] = item

                # ç´¯åŠ  RRF åˆ†æ•°
                scores[cid] += 1.0 / (k + rank)

        # æŒ‰èåˆåˆ†æ•°é™åºæ’åˆ—
        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
        return [obj_map[uid] for uid in sorted_ids]

    @staticmethod
    async def compute_rerank_scores(query: str, chunks: List[Dict]) -> List[Dict]:
        """
        ã€ä¼˜åŒ–ç‰ˆ Rerankã€‘
        1. ä½¿ç”¨ httpx å¼‚æ­¥è°ƒç”¨ï¼Œä¸é˜»å¡ã€‚
        2. å…¨é‡æ‰“åˆ† (top_n = len)ï¼Œä¸è¿›è¡Œæˆªæ–­ï¼Œå°†æˆªæ–­æƒäº¤ç»™ä¸‹æ¸¸ä¸šåŠ¡ã€‚
        """
        if not chunks: return []

        # é™åˆ¶å•æ¬¡æœ€å¤§æ‰“åˆ†æ•°é‡ï¼Œé˜²æ­¢ HTTP åŒ…è¿‡å¤§ (å¯æŒ‰éœ€è°ƒæ•´)
        candidates = chunks[:100]
        doc_contents = [c["content"] for c in candidates]

        payload = {
            "model": Config.RERANK_MODEL,
            "query": query,
            "documents": doc_contents,
            "return_documents": False,
            "top_n": len(doc_contents)  # å…³é”®ï¼šè¿”å›æ‰€æœ‰å€™é€‰çš„åˆ†æ•°
        }
        headers = {
            "Authorization": f"Bearer {Config.SILICONFLOW_API_KEY}",
            "Content-Type": "application/json"
        }

        try:
            # ä½¿ç”¨ä¸´æ—¶ Client æˆ–å•ä¾‹ Client å‡å¯ï¼Œæ­¤å¤„ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿è¿æ¥å…³é—­
            async with httpx.AsyncClient(timeout=30) as temp_client:
                resp = await temp_client.post(Config.SILICONFLOW_RERANK_URL, json=payload, headers=headers)
                resp.raise_for_status()
                data = resp.json()

            results = data.get("results", [])

            # å°†æ–°åˆ†æ•°å›å¡«å…¥ Chunk å¯¹è±¡
            scored_chunks = []
            for item in results:
                original_idx = item["index"]
                new_score = item["relevance_score"]

                chunk = candidates[original_idx].copy()
                chunk["score"] = round(new_score, 4)  # æ›´æ–°ä¸ºæ¨¡å‹é‡æ’åˆ†
                scored_chunks.append(chunk)

            return scored_chunks  # è¿”å›å·²æ‰“åˆ†çš„åˆ—è¡¨ï¼Œé¡ºåºé€šå¸¸å·²ç»æ˜¯é™åº

        except Exception as e:
            print(f"âš ï¸ [Rerank Failed] {e}. Falling back to RRF results.")
            # é™çº§ç­–ç•¥ï¼šå¦‚æœé‡æ’å¤±è´¥ï¼Œç›´æ¥è¿”å›åŸå§‹åˆ—è¡¨ï¼ˆä¿ç•™ RRF æ’åºï¼‰
            return chunks


# --- æ¨¡å—ä¸‰ï¼šæµç¨‹ç¼–æ’å™¨ (ä¸šåŠ¡é€»è¾‘) ---
class RetrievalOrchestrator:
    def __init__(self, api_client: DifyApiClient):
        self.api = api_client
        self.doc_meta_cache = {}  # ç¼“å­˜ (db_id, doc_id) -> Detail info

    async def prefetch_metadata(self, tasks: List[Dict]):
        """Stage 0: é¢„åŠ è½½æ–‡æ¡£å…ƒæ•°æ®ï¼Œå‡å°‘åç»­å¾ªç¯ä¸­çš„ API è°ƒç”¨"""
        print("ğŸš€ [Init] Prefetching document metadata...")
        needed = set()
        for t in tasks:
            if t.get("document_id") and t.get("database_id"):
                needed.add((t["database_id"], t["document_id"]))

        if not needed: return

        # å¹¶å‘è·å–
        coros = [self.api.fetch_document_detail(db, doc) for db, doc in needed]
        results = await asyncio.gather(*coros, return_exceptions=True)

        for (key, detail) in zip(needed, results):
            if detail:
                self.doc_meta_cache[key] = detail

            # 1. æå– Ugly List -> Clean Dict
            raw_meta_list = detail.get("doc_metadata", [])
            clean_meta = ContentFormatter.clean_metadata(raw_meta_list)
            # 2. è¡¥å…… Root Level çš„å…³é”®ä¿¡æ¯åˆ° metadata ä¸­ (æ¯”å¦‚ doc_form)
            # æœ‰æ—¶å€™ document info åœ¨å¤–é¢ï¼Œä¸åœ¨ doc_metadata list é‡Œ
            if "doc_form" in detail:
                clean_meta["doc_form"] = detail["doc_form"]

            # 3. æ„é€ æç®€ Cache å¯¹è±¡
            self.doc_meta_cache[key] = {
                "id": detail.get("id"),
                "name": detail.get("name"),
                "doc_metadata": clean_meta,  # åªæœ‰è¿™æ˜¯ä¸€ä¸ªå¹²å‡€çš„å­—å…¸
                "_source": "api_detail"  # æ ‡è®°æ¥æº
            }
        print(self.doc_meta_cache)

    def build_execution_plan(self, tasks: List[Dict]) -> List[Dict]:
        """Stage 1: æ„å»ºé«˜æ•ˆæ£€ç´¢è®¡åˆ’ï¼Œåˆå¹¶ç›¸åŒ DB çš„è¯·æ±‚"""
        # æ•°æ®ç»“æ„: db_id -> {doc_names: set, include_full: bool}
        plan_map = defaultdict(lambda: {"doc_names": set(), "include_full": False})

        for t in tasks:
            db = t.get("database_id")
            if not db: continue

            mode = t.get("retrieval_mode")
            if mode == "segment_retrieval":
                d_id = t.get("document_id")
                # ä»ç¼“å­˜ä¸­æ‹¿åå­— (Dify è¿‡æ»¤ä¾èµ–åå­—)
                meta = self.doc_meta_cache.get((db, d_id))
                if meta and meta.get("name"):
                    plan_map[db]["doc_names"].add(meta["name"])
            elif mode == "full_database_retrieval":
                plan_map[db]["include_full"] = True

        # ç”Ÿæˆå®é™…çš„ Job Payloads
        jobs = []
        for db_id, constraints in plan_map.items():
            # Job A: å¸¦ Metadata Filter çš„æ£€ç´¢ (é’ˆå¯¹æŒ‡å®šæ–‡æ¡£)
            if constraints["doc_names"]:
                names = list(constraints["doc_names"])
                filter_cond = {
                    "logical_operator": "or" if len(names) > 1 else "and",
                    "conditions": [{"name": "document_name", "comparison_operator": "is", "value": n} for n in names]
                }
                jobs.append({
                    "database_id_for_url": db_id,
                    "metadata_filtering_conditions": filter_cond
                })

            # Job B: å…¨åº“æ£€ç´¢ (å¦‚æœéœ€è¦)
            if constraints["include_full"]:
                jobs.append({"database_id_for_url": db_id})

        return jobs

    def _inject_metadata_from_search_results(self, chunks: List[Dict]):
        """ä»æœç´¢ç»“æœæ³¨å…¥æ—¶ï¼Œä¹Ÿä¿æŒç»“æ„ä¸€è‡´"""
        for c in chunks:
            key = (c["database_id"], c["document_id"])
            existing = self.doc_meta_cache.get(key)

            # å¦‚æœå·²æœ‰å…¨é‡è¯¦æƒ…ï¼Œè·³è¿‡
            if existing and existing.get("_source") == "api_detail":
                continue
            # æ³¨å…¥æœç´¢å¿«ç…§
            if c.get("doc_metadata") or c.get("document_name"):
                # æ³¨æ„ retrieve åº”è¯¥å·²ç»æŠŠ c["doc_metadata"] æ´—æˆå­—å…¸äº†
                self.doc_meta_cache[key] = {
                    "id": c["document_id"],
                    "name": c["document_name"],
                    "doc_metadata": c["doc_metadata"],  # ä¹Ÿæ˜¯å¹²å‡€çš„å­—å…¸
                    "_source": "retrieve_snapshot"
                }

    def distribute_chunks_to_tasks(self, scored_chunks: List[Dict], tasks: List[Dict]) -> List[Dict]:
        """
        Stage 4: åº“å­˜åˆ†å‘ (Inventory Slicing)
        å°†æ‰“å¥½åˆ†çš„å¤§åˆ—è¡¨ï¼ŒæŒ‰ç…§ Task çš„éœ€æ±‚ï¼ˆæ–‡æ¡£IDã€TopKï¼‰åˆ†å‘å‡ºå»ã€‚
        """
        # 1. å»ºç«‹åº“å­˜ç´¢å¼•ï¼šDB -> Doc -> Chunks
        inventory = defaultdict(lambda: defaultdict(list))
        for c in scored_chunks:
            inventory[c['database_id']][c['document_id']].append(c)

        final_results = []
        used_chunk_ids = set()  # ç”¨äºå…¨åº“æ£€ç´¢æ—¶é˜²æ­¢é‡å¤

        # 2. éå† Task è¿›è¡Œâ€œè¿›è´§â€
        for task in tasks:
            mode = task.get("retrieval_mode")
            db = task.get("database_id")
            top_k = task.get("top_k", 20)

            candidates = []

            if mode == "segment_retrieval":
                # åªå–ç‰¹å®šæ–‡æ¡£çš„åº“å­˜
                doc_id = task.get("document_id")
                candidates = inventory.get(db, {}).get(doc_id, [])

            elif mode == "full_database_retrieval":
                # å–è¯¥ DB ä¸‹æ‰€æœ‰æ–‡æ¡£çš„åº“å­˜
                docs_in_db = inventory.get(db, {})
                for doc_chunks in docs_in_db.values():
                    candidates.extend(doc_chunks)

            # 3. æ’åºå¹¶æˆªæ–­ (åŸºäº Rerank åˆ†æ•°)
            # æ³¨æ„ï¼šcandidates æ˜¯å¼•ç”¨ï¼Œè¿™é‡Œ sort ä¸ä¼šå½±å“åŸå§‹åº“å­˜åˆ—è¡¨çš„å®Œæ•´æ€§ï¼Œ
            # ä½†ä¸ºäº†å®‰å…¨ï¼ŒRerank æ­¥éª¤é€šå¸¸å·²ç»æ’å¥½åºäº†ï¼Œè¿™é‡Œåªæ˜¯å†æ¬¡ç¡®ä¿ã€‚
            candidates.sort(key=lambda x: x["score"], reverse=True)

            # 4. é€‰å– Top K (å¸¦å»é‡é€»è¾‘)
            picked_count = 0
            for c in candidates:
                if picked_count >= top_k:
                    break

                # å¦‚æœæ˜¯ segment ä»»åŠ¡ï¼Œç›´æ¥æ‹¿ï¼›å¦‚æœæ˜¯ full ä»»åŠ¡ï¼Œé¿å…æ‹¿é‡å¤çš„(å¦‚æœå‰é¢ segment ä»»åŠ¡æ‹¿è¿‡äº†)
                # æ³¨ï¼šè¿™é‡Œçš„é€»è¾‘å–å†³äºï¼šæ˜¯å¦å…è®¸åŒä¸€ä¸ª chunk å‡ºç°åœ¨ä¸åŒçš„ Task ç»“æœä¸­ï¼Ÿ
                # å¦‚æœ Dify è¦æ±‚æ¯ä¸ª chunk åªèƒ½å‡ºç°ä¸€æ¬¡ï¼Œä¿æŒè¿™ä¸ª ifã€‚
                # å¦‚æœå…è®¸ä¸åŒ Task åŒ…å«ç›¸åŒ chunkï¼Œå¯ç§»é™¤ checkã€‚
                if c['chunk_id'] not in used_chunk_ids:
                    final_results.append(c)
                    used_chunk_ids.add(c['chunk_id'])
                    picked_count += 1

        return final_results

    # åœ¨ RetrievalOrchestrator ç±»ä¸­æ›´æ–°/æ·»åŠ æ­¤æ–¹æ³•

    # async def process_full_document_task(self, task: Dict) -> Dict:
    #     """
    #     å¤„ç†å…¨æ–‡æ¡£ä»»åŠ¡ã€‚
    #     å…³é”®ç‚¹ï¼šè¿”å›çš„ç»“æ„å¿…é¡»ä¸ format_output ç”Ÿæˆçš„ç»“æ„å®Œå…¨ä¸€è‡´ï¼ˆSchema Alignmentï¼‰ã€‚
    #     """
    #     db_id = task.get("database_id")
    #     doc_id = task.get("document_id")
    #
    #     # 1. å°è¯•ä»ç¼“å­˜æ‹¿ Metaï¼Œæ‹¿ä¸åˆ°å°±å» Fetch
    #     # è¿™é‡Œçš„ fetch_document_detail å†…éƒ¨åº”å®ç°é˜²é‡å¤è¯·æ±‚ï¼Œæˆ–è€…ä¾èµ–å¤–éƒ¨ prefetch
    #     doc_meta = self.doc_meta_cache.get((db_id, doc_id))
    #
    #     # 2. è·å–å…¨éƒ¨åˆ†æ®µ (å¦‚æœ Meta æ²¡æ‹¿åˆ°ï¼Œè¿™é‡Œå¹¶å‘å»æ‹¿ Meta å’Œ Segments)
    #     tasks_list = [self.api.fetch_all_segments(db_id, doc_id)]
    #     if not doc_meta:
    #         tasks_list.append(self.api.fetch_document_detail(db_id, doc_id))
    #
    #     results = await asyncio.gather(*tasks_list)
    #     all_segments = results[0]
    #
    #     # å¦‚æœåˆšæ‰å¹¶å‘å–äº† Metaï¼Œæ›´æ–°ä¸€ä¸‹
    #     if not doc_meta and len(results) > 1:
    #         doc_meta = results[1]
    #         if doc_meta:
    #             self.doc_meta_cache[(db_id, doc_id)] = doc_meta
    #
    #     # å®‰å…¨å–å€¼
    #     doc_name = doc_meta.get("name", "Unknown") if doc_meta else "Unknown"
    #     doc_metadata_dict = doc_meta.get("doc_metadata") or doc_meta.get("metadata") or {}
    #
    #     # 3. æ’åºåˆ†æ®µ (ä¿è¯é˜…è¯»é¡ºåº)
    #     all_segments.sort(key=lambda x: x.get('position', float('inf')))
    #
    #     # 4. æ„é€  Content Blocks
    #     content_blocks = [
    #         {
    #             "position": s.get("position"),
    #             "content": s.get("content", ""),
    #             "score": None  # å…¨æ–‡æ¡£é»˜è®¤æ»¡åˆ†ï¼Œè¡¨ç¤ºæ˜¯ç¡¬æ€§æŒ‡å®šçš„
    #         }
    #         for s in all_segments
    #     ]
    #
    #     # 5. ã€å…³é”®ã€‘è¿”å›æ ‡å‡†åŒ–ç»“æ„ (Standardized Schema)
    #     # è¿™å°±æ˜¯å¿…é¡»æ”¾å…¥ retrieve_data é‡Œçš„é‚£ä¸ªå¯¹è±¡
    #     return {
    #         "database_id": db_id,
    #         "document_infos": [
    #             {
    #                 "doc_metadata": doc_metadata_dict,
    #                 "document_id": doc_id,
    #                 "document_name": doc_name,
    #                 "source_type": "document",  # åŒºåˆ«äº 'excerpt'
    #                 "content_blocks": content_blocks
    #             }
    #         ]
    #     }

    async def process_full_document_task(self, task: Dict) -> Dict:
        """
        å¤„ç†å…¨æ–‡æ¡£ä»»åŠ¡ (é€‚é…å¤šæ¨¡æ€)ã€‚
        """
        db_id = task.get("database_id")
        doc_id = task.get("document_id")
        print(db_id, doc_id)
        # 1. è·å–å…ƒæ•°æ®å’Œå…¨éƒ¨åˆ†æ®µ
        doc_meta_obj = self.doc_meta_cache.get((db_id, doc_id))
        if not doc_meta_obj or doc_meta_obj.get("_source") != "api_detail":
            raw_detail = await self.api.fetch_document_detail(db_id, doc_id)
            if raw_detail:

                raw_list = raw_detail.get("doc_metadata", [])
                clean_meta = ContentFormatter.clean_metadata(raw_list)
                # è¡¥å…… root level ä¿¡æ¯
                if "doc_form" in raw_detail: clean_meta["doc_form"] = raw_detail["doc_form"]
                doc_meta_obj = {
                    "id": raw_detail.get("id"),
                    "name": raw_detail.get("name"),
                    "doc_metadata": clean_meta,
                    "_source": "full_detail"
                }
                self.doc_meta_cache[(db_id, doc_id)] = doc_meta_obj

        all_segments = await self.api.fetch_all_segments(db_id, doc_id)

        # å®‰å…¨æ£€æŸ¥
        if not doc_meta_obj or not all_segments:
            return {
                "database_id": db_id,
                "document_infos": [{
                    "document_id": doc_id,
                    "document_name": "Error: Not Found",
                    "source_type": "error",
                    "content_blocks": []
                }]
            }

        # 2. è½¬æ¢ä¸ºå†…éƒ¨ chunk ç»“æ„
        temp_chunks = [
            {"content": s.get("content"), "position": s.get("position"), "score": None,
             "document_id": doc_id, "database_id": db_id, "document_name": doc_meta_obj.get("name")}
            for s in all_segments
        ]

        # 3. è°ƒç”¨ç»Ÿä¸€æ ¼å¼åŒ–å™¨ï¼Œå¹¶æ˜ç¡®å‘ŠçŸ¥ä¸Šä¸‹æ–‡æ˜¯ 'full_doc'
        document_info = ContentFormatter.format_document(temp_chunks, doc_meta_obj, context='full_doc')
        # 4. è¿”å›æ ‡å‡†ç»“æ„
        return {"database_id": db_id, "document_infos": [document_info] if document_info else []}

    def format_output(self, chunks: List[Dict]) -> List[Dict]:
        """Stage 5: æ ¼å¼åŒ–ä¸º Dify è¦æ±‚çš„ JSON ç»“æ„"""
        grouped = defaultdict(lambda: defaultdict(list))
        for c in chunks:
            grouped[c['database_id']][c['document_id']].append(c)

        formatted_sources = []
        for db_id, docs_map in grouped.items():
            doc_infos = []
            for doc_id, chunk_list in docs_map.items():
                if not chunk_list: continue
                # # æŒ‰åŸæ–‡ä½ç½®æ’åºï¼Œæ–¹ä¾¿é˜…è¯»
                # chunk_list.sort(key=lambda x: x.get('score', 0), reverse=True)

                # ä»ç¼“å­˜è¯»å–å…ƒæ•°æ®
                meta = self.doc_meta_cache.get((db_id, doc_id), {})

                formatted_doc = ContentFormatter.format_document(chunk_list, meta, context='rag')
                if formatted_doc:
                    doc_infos.append(formatted_doc)

                # doc_meta_dict = meta.get("doc_metadata") or meta.get("metadata") or {}
                # doc_infos.append({
                #     "doc_metadata": doc_meta_dict,
                #     "document_id": doc_id,
                #     "document_name": chunk_list[0]['document_name'] or "Unknown",
                #     "source_type": "excerpt",
                #     "content_blocks": [
                #         {
                #             "content": c["content"],
                #             "position": c["position"],
                #             "score": c["score"]
                #         } for c in chunk_list
                #     ]
                # })

            if doc_infos:
                formatted_sources.append({
                    "database_id": db_id,
                    "document_infos": doc_infos
                })
        return formatted_sources

    async def process_slide(self, query_group: Dict, execution_plan: List[Dict], tasks: List[Dict]) -> Dict:
        """å¤„ç†å•é¡µ PPT é€»è¾‘ï¼šRetrieve -> RRF -> Rerank -> Slice -> Format"""
        # slide_id = slide_group.get("slide_id", "unknown")
        queries = query_group.get("local_queries", [])

        result_object = query_group.copy()
        if not queries:
            result_object["retrieve_data"] = []
            return result_object

        print(f"ğŸ‘‰ Processing Slide:^_^ ({len(queries)} queries)")

        # 1. å¹¶å‘ Execute Retrieval Jobs
        # ç”Ÿæˆ (Queryæ•°é‡ x Plan Jobæ•°é‡) ä¸ªå¼‚æ­¥è¯·æ±‚
        fetch_tasks = []
        for q in queries:
            for job in execution_plan:
                # å¿…é¡» copy jobï¼Œå› ä¸º retrieve å†…éƒ¨ä¼š pop å­—æ®µ
                fetch_tasks.append(self.api.retrieve(q, job.copy()))
        # print(fetch_tasks)

        # ç­‰å¾…æ‰€æœ‰ API è¿”å›
        # raw_results_list çš„ç»“æ„æ˜¯: [ [ChunkA1, ChunkA2], [ChunkB1], [] ... ]
        raw_results_list = await asyncio.gather(*fetch_tasks)
        # print(raw_results_list)

        # 2. RRF èåˆ (ä¿®å¤ç‚¹ï¼šç›´æ¥ä¼ å…¥ List[List])
        # è¿‡æ»¤æ‰ç©ºç»“æœï¼Œä¼ å…¥ RRF
        valid_results = [res for res in raw_results_list if res]
        fused_chunks = RagService.reciprocal_rank_fusion(valid_results)

        # 3. å…¨å±€ Rerank æ‰“åˆ† (ä¿®å¤ç‚¹ï¼šå¼‚æ­¥è°ƒç”¨ï¼Œå…¨é‡ä¸æˆªæ–­)
        rerank_context = " ".join(queries)  # ç®€å•æ‹¼æ¥åš context
        scored_chunks = await RagService.compute_rerank_scores(rerank_context, fused_chunks)

        # 4. åº“å­˜åˆ†å‘ (Slicing)
        final_chunks = self.distribute_chunks_to_tasks(scored_chunks, tasks)
        # print(final_chunks)
        # å­˜å‚¨cacheæ–‡æ¡£ä¿¡æ¯
        for res_list in raw_results_list:
            if res_list:
                self._inject_metadata_from_search_results(final_chunks)
        print(self.doc_meta_cache)

        # 5. æŒ‚è½½æ•°æ®åˆ°å‰¯æœ¬ä¸­
        result_object["retrieve_data"] = self.format_output(final_chunks)
        # 6. æ ¼å¼åŒ–è¾“å‡º
        return result_object


# --- æ¨¡å—å››ï¼šå¤šæ¨¡æ€å†…å®¹æ ¼å¼åŒ–å™¨ ---
class ContentFormatter:
    """
    è´Ÿè´£å°†ä¸åŒæºç±»å‹çš„ chunk å†…å®¹æ ¼å¼åŒ–ä¸ºæ ‡å‡†è¾“å‡ºç»“æ„ã€‚
    è¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¨¡å—ï¼Œæœªæ¥å¯æ·»åŠ  Image, Audio ç­‰æ ¼å¼åŒ–å™¨ã€‚
    """

    @staticmethod
    def clean_metadata(raw_data: Any) -> Dict[str, Any]:
        """
        ã€æ ¸å¿ƒæ¸…æ´—å™¨ã€‘
        è¾“å…¥ï¼šå¯èƒ½æ˜¯ Dify è¯¦æƒ…æ¥å£è¿”å›çš„ List[Dict]ï¼Œä¹Ÿå¯èƒ½æ˜¯æ£€ç´¢æ¥å£è¿”å›çš„ Dictã€‚
        è¾“å‡ºï¼šç»Ÿä¸€çš„æ‰å¹³ Dict {key: value}ã€‚
        """
        if not raw_data:
            return {}
        # åœºæ™¯ A: Retrieve æ¥å£è¿”å›çš„å·²ç»æ˜¯æ¼‚äº®çš„ Dict
        # ä¾‹å¦‚: {"source": "file_upload", "description": "xxx", "source_type": "document"}
        if isinstance(raw_data, dict):
            return raw_data
        # åœºæ™¯ B: Detail æ¥å£è¿”å›çš„ Ugly List
        # ä¾‹å¦‚: [{"name": "vehicle_model", "value": "ç†æƒ³L8"}, {"name": "doc_type", "value": "ç»´ä¿®æ‰‹å†Œ"}]
        result = {}
        if isinstance(raw_data, list):
            for item in raw_data:
                # å®¹é”™ï¼šå¿…é¡»æ˜¯å­—å…¸ä¸”åŒ…å« name å’Œ value
                if isinstance(item, dict) and 'name' in item:
                    # ä¼˜å…ˆå– valueï¼Œå¦‚æœæ²¡æœ‰ value å¯èƒ½æ˜¯ç©ºå­—æ®µ
                    val = item.get('value')
                    # æœ‰äº›ç‰¹æ®Šå­—æ®µ value æ˜¯ "NULL" å­—ç¬¦ä¸²ï¼Œè½¬ä¸º None æˆ–ç©º
                    if val == "NULL":
                        val = None
                    result[item['name']] = val
            return result
        return {}

    @staticmethod
    def _transform_metadata(meta_list_or_dict: Any) -> Dict[str, Any]:
        """
        ã€å…³é”®ä¿®å¤ã€‘
        å°† Dify è¿”å›çš„å…ƒæ•°æ®åˆ—è¡¨ `[{'name': k, 'value': v}, ...]`
        è½¬æ¢ä¸ºæ ‡å‡†çš„å­—å…¸ `{'k': 'v', ...}`ã€‚
        åŒæ—¶å…¼å®¹å·²ç»æ˜¯å­—å…¸çš„æƒ…å†µã€‚
        """
        if isinstance(meta_list_or_dict, dict):
            return meta_list_or_dict  # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥è¿”å›
        if not isinstance(meta_list_or_dict, list):
            return {}  # å¦‚æœæ˜¯å…¶ä»–æœªçŸ¥ç±»å‹ï¼Œè¿”å›ç©ºå­—å…¸
        # æ ¸å¿ƒè½¬æ¢é€»è¾‘
        transformed_meta = {}
        for item in meta_list_or_dict:
            if isinstance(item, dict) and 'name' in item and 'value' in item:
                transformed_meta[item['name']] = item['value']
        return transformed_meta

    @staticmethod
    def _parse_key_value_string(content: str) -> Dict[str, Any]:
        """
        è§£æ "key1":"value1";"key2":"value2" æ ¼å¼çš„å­—ç¬¦ä¸²ã€‚
        """
        data = {}
        if not isinstance(content, str):
            return data

        pairs = content.strip().split(';')
        for pair in pairs:
            if ':' in pair:
                key, val = pair.split(':', 1)
                # å»é™¤é”®å’Œå€¼çš„å¼•å·
                key = key.strip().strip('"')
                val = val.strip().strip('"')
                data[key] = val
        return data

    @classmethod
    def _format_video_document(cls, chunks: List[Dict], meta: Dict) -> Dict:
        """
        å°†è§†é¢‘ç±»å‹çš„ Chunks åˆ—è¡¨æ ¼å¼åŒ–ä¸ºè§†é¢‘æ–‡æ¡£ç»“æ„ã€‚
        """
        if not chunks:
            return {}

        chunks.sort(key=lambda x: x.get('position', 0))
        content_blocks = []
        video_info = {}

        # 1. è§£ææ‰€æœ‰ video chunksï¼Œæ„å»º frame åˆ—è¡¨
        for chunk in chunks:
            raw_data = cls._parse_key_value_string(chunk.get("content", ""))

            # æå–å…¨å±€è§†é¢‘ä¿¡æ¯ (åªéœ€ä¸€æ¬¡)
            if not video_info:
                video_info = {
                    "duration": float(raw_data.get("è§†é¢‘æ—¶é•¿", 0.0)),
                    "videoUrl": raw_data.get("è§†é¢‘é“¾æ¥", ""),
                    "videoName": raw_data.get("è§†é¢‘åç§°", ""),
                }

            # 2. æ„å»ºå•ä¸ª frame å¯¹è±¡
            try:
                frame = {
                    "frameId": raw_data.get("è§†é¢‘ç‰‡æ®µID"),
                    "frameName": raw_data.get("è§†é¢‘ç‰‡æ®µåç§°"),
                    "frameUrl": raw_data.get("è§†é¢‘ç‰‡æ®µåˆ†æ®µURL"),
                    "frameImageUrl": raw_data.get("è§†é¢‘ç‰‡æ®µå¸§å›¾ç‰‡URL"),
                    "startTime": float(raw_data.get("å¼€å§‹æ—¶é—´", 0.0)),
                    "endTime": float(raw_data.get("ç»“æŸæ—¶é—´", 0.0)),
                    "frameDuration": float(raw_data.get("è§†é¢‘ç‰‡æ®µæ—¶é•¿", 0.0)),
                    "description": raw_data.get("è§†é¢‘ç‰‡æ®µæè¿°", ""),
                    # ä¿ç•™æ£€ç´¢ä¿¡æ¯
                    "position": chunk.get("position"),
                    "score": chunk.get("score")
                }
                content_blocks.append(frame)
            except (ValueError, TypeError) as e:
                print(f"âš ï¸ [Video Frame Parse Error] Skipping frame. Chunk ID: {chunk.get('id')}, Error: {e}")
                continue

        # 3. ç»„è£…æœ€ç»ˆçš„è§†é¢‘æ–‡æ¡£å¯¹è±¡
        doc_meta_dict = meta.get("doc_metadata") or meta.get("metadata") or {}
        raw_doc_name = chunks[0].get("document_name") or video_info.get("videoName") or "Unknown"
        final_doc_name = raw_doc_name
        # è·å–ç›®æ ‡åç¼€ (e.g., "mp4")
        target_ext = doc_meta_dict.get("extension")
        if target_ext and isinstance(raw_doc_name, str):
            # ç§»é™¤æ—§åç¼€å¹¶æ·»åŠ æ–°åç¼€
            if "." in raw_doc_name:
                # åˆ†å‰²æ–‡ä»¶åï¼Œä¿ç•™æœ€åä¸€ä¸ªç‚¹ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ä½œä¸º basename
                # e.g., "my.video.file.xlsx" -> "my.video.file"
                basename = raw_doc_name.rsplit(".", 1)[0]
                final_doc_name = f"{basename}.{target_ext}"
            else:
                # å¦‚æœåŸæ–‡ä»¶åæ²¡æœ‰åç¼€ï¼Œç›´æ¥è¿½åŠ 
                final_doc_name = f"{raw_doc_name}.{target_ext}"

        return {
            "doc_metadata": doc_meta_dict,
            "document_id": chunks[0].get("document_id"),
            "document_name": final_doc_name,
            "source_type": "video",
            "duration": video_info.get("duration"),
            "videoUrl": video_info.get("videoUrl"),
            "content_blocks": content_blocks
        }

    @staticmethod
    def _format_text_document(chunks: List[Dict], meta: Dict, final_source_type: str) -> Dict:
        """
        æ ¼å¼åŒ–æ–‡æœ¬æ–‡æ¡£ã€‚
        final_source_type å¿…é¡»æ˜¯ 'excerpt' æˆ– 'document'ã€‚
        """
        if final_source_type == 'excerpt':
            chunks.sort(key=lambda x: x.get('score', 0), reverse=True)
        else:  # 'document'
            chunks.sort(key=lambda x: x.get('position', 0))
        doc_meta_dict = meta.get("doc_metadata") or meta.get("metadata") or {}

        return {
            "doc_metadata": doc_meta_dict,
            "document_id": chunks[0].get("document_id"),
            "document_name": chunks[0].get("document_name") or "Unknown",
            "source_type": final_source_type,
            "content_blocks": [
                {
                    "content": c["content"],
                    "position": c["position"],
                    "score": c["score"]
                } for c in chunks
            ]
        }

    @classmethod
    def format_document(cls, chunks: List[Dict], meta: Dict, context: str) -> Dict:
        """
        æ€»åˆ†å‘å™¨ï¼šæ ¹æ®å…ƒæ•°æ®å†³å®šä½¿ç”¨å“ªä¸ªæ ¼å¼åŒ–å‡½æ•°ã€‚
        ã€å…³é”®ä¿®å¤ç‚¹ã€‘å¢åŠ å¯¹ source_type ä¸º None çš„å¤„ç†ã€‚
        """
        if not chunks:
            return {}
        # print(chunks)
        # ä¼˜å…ˆä»ç¼“å­˜çš„è¯¦ç»†å…ƒæ•°æ®ä¸­è·å–
        doc_meta = meta.get("doc_metadata", {}) or meta.get("metadata", {})
        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»ç¬¬ä¸€ä¸ª chunk çš„å…ƒæ•°æ®ä¸­é™çº§è·å– (è™½ç„¶ä¸æ¨èï¼Œä½†å¯ä½œä¸ºå¤‡ç”¨)
        # if not doc_meta and chunks[0].get("metadata"):
        #    doc_meta = chunks[0].get("metadata")
        # 2. ã€æ ¸å¿ƒä¿®å¤ã€‘è°ƒç”¨è½¬æ¢å‡½æ•°ï¼Œç¡®ä¿å¾—åˆ°çš„æ˜¯ä¸€ä¸ªå­—å…¸
        doc_meta_dict = cls._transform_metadata(doc_meta)
        # 3. ä»è½¬æ¢åçš„å­—å…¸ä¸­å®‰å…¨åœ°è·å– source_type
        #    è¿™é‡Œçš„'doc_form'æ˜¯æ ¹æ®æ—¥å¿—çŒœæµ‹çš„ï¼Œå¦‚æœä¸è¡Œï¼Œå¯ä»¥å°è¯•'doc_type'
        inherent_type = doc_meta_dict.get("source_type") or doc_meta_dict.get("doc_type")
        # --- DEBUGGING ---
        # # å¢åŠ æ›´è¯¦ç»†çš„æ—¥å¿—ï¼Œæ–¹ä¾¿å®šä½é—®é¢˜
        # if not source_type:
        #     print(f"âš ï¸ [Formatter Warning] No 'source_type' found for doc_id: {chunks[0].get('document_id')}. "
        #           f"Defaulting to 'text' format. Metadata received: {meta}")
        # ---------------
        # æ’åºï¼šæ ¹æ®ç±»å‹å†³å®šæ’åºç­–ç•¥
        # è§†é¢‘æŒ‰åŸæ–‡ä½ç½®ï¼ˆpositionï¼‰æ’åºæ›´åˆç†ï¼Œå› ä¸ºæ—¶é—´æˆ³è§£æåœ¨æ ¼å¼åŒ–å†…éƒ¨

        # if source_type == "video":
        #     chunks.sort(key=lambda x: x.get('position', 0))
        #     return cls._format_video_document(chunks, meta)
        # else:
        #     # å¯¹äºæ‰€æœ‰å…¶ä»–æƒ…å†µ (æ–‡æœ¬, None, æˆ–æœªçŸ¥çš„ source_type)ï¼Œéƒ½æŒ‰åˆ†æ•°æ’åº
        #     chunks.sort(key=lambda x: x.get('score', 0), reverse=True)
        #     return cls._format_text_document(chunks, meta)

        # 2. æ ¹æ®â€œå›ºæœ‰ç±»å‹â€å’Œâ€œè°ƒç”¨ä¸Šä¸‹æ–‡â€å†³å®šæœ€ç»ˆçš„å¹³è¡Œç±»å‹å¹¶åˆ†å‘
        if inherent_type == "video":
            return cls._format_video_document(chunks, meta)
        # elif inherent_type == "image":
        #     return cls._format_image_document(chunks, meta) # æœªæ¥æ‰©å±•
        # elif inherent_type == "audio":
        #     return cls._format_audio_document(chunks, meta) # æœªæ¥æ‰©å±•
        else:
            # é»˜è®¤ä¸ºæ–‡æœ¬å¤„ç†é€»è¾‘
            if context == 'rag':
                final_type = 'excerpt'
            elif context == 'full_doc':
                final_type = 'document'
            else:
                # é»˜è®¤é™çº§ä¸º excerpt
                final_type = 'excerpt'
            return cls._format_text_document(chunks, meta, final_source_type=final_type)


def parse_survey_content(content: str) -> dict:
    """
    è§£æé—®å·/è®¿è°ˆç±»å†…å®¹ï¼ˆæ”¯æŒ Markdownã€åˆ—è¡¨ã€çº¯æ–‡æœ¬æ ¼å¼ï¼‰
    """
    lines = content.strip().split('\n')
    
    # 1. æå–å…ƒæ•°æ® (Metadata) - é€šå¸¸åœ¨æœ€åä¸€è¡Œï¼Œä»¥åˆ†å·åˆ†éš”
    metadata = {}
    if lines and ';' in lines[-1] and ':' in lines[-1]:
        meta_line = lines[-1].strip()
        # ç§»é™¤å¼€å¤´çš„åˆ†å·ï¼ˆå¦‚æœæœ‰ï¼‰
        if meta_line.startswith(';'):
            meta_line = meta_line[1:]
        
        parts = meta_line.split(';')
        for part in parts:
            if ':' in part:
                k, v = part.split(':', 1)
                metadata[k.strip()] = v.strip()
        
        # æå–å®Œ metadata åï¼Œä» lines ä¸­ç§»é™¤æœ€åä¸€è¡Œï¼Œé¿å…å¹²æ‰°åç»­è§£æ
        lines = lines[:-1]

    # 2. æå–åŸºæœ¬ä¿¡æ¯ (Basic Info)
    basic_info = {
        "city": metadata.get("city"),
        "job_role": metadata.get("job_role"),
        "institution_type": metadata.get("institution_type"),
        "institution_host": metadata.get("institution_host"),
        "institution_name": metadata.get("institution_name"),
        "is_inclusive": None, # éœ€ä»æ­£æ–‡æå–
        "education": metadata.get("education"),
        "major": metadata.get("major")
    }

    # å°è¯•ä»ç¬¬ä¸€è¡Œ/æ ‡é¢˜è¡Œæå–ç¼ºå¤±çš„åŸºæœ¬ä¿¡æ¯ (å¦‚æœ metadata ä¸å…¨)
    # æ ¼å¼ç¤ºä¾‹: åŸå¸‚ï¼šåŒ—äº¬-æµ·æ·€åŒº | å²—ä½ï¼šä¿æ•™ä¸»ä»» | æœºæ„ï¼šå¹¼å„¿å›­æ‰˜ç­ | æ€§è´¨ï¼šå…¬åŠ
    if lines:
        header_line = lines[0]
        if '|' in header_line:
            parts = header_line.split('|')
            for part in parts:
                if 'ï¼š' in part or ':' in part:
                    sep = 'ï¼š' if 'ï¼š' in part else ':'
                    k, v = part.split(sep, 1)
                    k = k.strip()
                    v = v.strip()
                    if k == 'åŸå¸‚' and not basic_info['city']: basic_info['city'] = v
                    if k == 'å²—ä½' and not basic_info['job_role']: basic_info['job_role'] = v
                    if k == 'æœºæ„' and not basic_info['institution_type']: basic_info['institution_type'] = v
                    if k == 'æ€§è´¨' and not basic_info['institution_host']: basic_info['institution_host'] = v
    
    # 3. æ‰«ææ­£æ–‡æå–è¡¥å……ä¿¡æ¯ (Contents è§£æç”¨äºè¾…åŠ©æå– Basic Info)
    # è™½ç„¶æœ€ç»ˆè¾“å‡ºå¯èƒ½ä¸éœ€è¦è¯¦ç»†çš„ contents ç»“æ„ï¼Œä½†æˆ‘ä»¬éœ€è¦éå†æ­£æ–‡æ¥è·å–
    # æ˜¯å¦æ™®æƒ ã€å­¦å†ã€ä¸“ä¸š ç­‰å¯èƒ½é—æ¼åœ¨ Basic Info ä¸­çš„å­—æ®µ
    
    for line in lines:
        line = line.strip()
        if not line: continue
        
        # å¿½ç•¥ç¬¬ä¸€è¡Œå¦‚æœæ˜¯ header
        if '|' in line and 'åŸå¸‚' in line and line == lines[0]:
            continue

        # è¯†åˆ«é—®ç­”å¯¹
        if ':' in line or 'ï¼š' in line:
            # ç§»é™¤å¼€å¤´çš„ - 
            clean_line = line.lstrip('-').strip()
                
            # åˆ†å‰² Key-Value
            sep = 'ï¼š' if 'ï¼š' in clean_line else ':'
            parts = clean_line.split(sep, 1)
            if len(parts) == 2:
                q = parts[0].strip()
                a = parts[1].strip()
                
                # ç‰¹æ®Šå¤„ç†ï¼šæ˜¯å¦æ™®æƒ 
                if q == 'æ˜¯å¦æ™®æƒ ' and basic_info['is_inclusive'] is None:
                    if a == 'æ˜¯': basic_info['is_inclusive'] = True
                    elif a == 'å¦': basic_info['is_inclusive'] = False
                
                # ç‰¹æ®Šå¤„ç†ï¼šå­¦å†/ä¸“ä¸š (å¦‚æœ metadata æ²¡æå–åˆ°ï¼Œè¿™é‡Œè¡¥æ•‘)
                if q == 'å­¦å†' and not basic_info['education']: basic_info['education'] = a
                if q == 'ä¸“ä¸š' and not basic_info['major']: basic_info['major'] = a

    return {
        "basic_info": basic_info,
        "raw_text": content
    }

def parse_institution_info(content: str) -> dict:
    """
    è§£ææœºæ„å¤‡æ¡ˆä¿¡æ¯ (Key: Value è¡Œæ ¼å¼)
    """
    lines = content.strip().split('\n')
    data = {}
    for line in lines:
        line = line.strip()
        if not line: continue
        
        sep = 'ï¼š' if 'ï¼š' in line else ':'
        if sep in line:
            k, v = line.split(sep, 1)
            data[k.strip()] = v.strip()
            
    return {
        "institution_info": {
            "name": data.get("æœºæ„åç§°"),
            "alias": data.get("åˆ«å"),
            "credit_code": data.get("ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç "),
            "type": data.get("æœºæ„ç±»å‹"),
            "address": data.get("è¯¦ç»†åœ°å€"),
            "registration_date": data.get("å¤‡æ¡ˆåŠå®Œæˆæ—¶é—´"),
            "region_code": data.get("åŒºåŸŸç¼–å·")
        }
    }

def parse_school_major_info(content: str) -> dict:
    """
    è§£æé«˜æ ¡ä¸“ä¸šä¿¡æ¯ (Key: Value è¡Œæ ¼å¼)
    """
    lines = content.strip().split('\n')
    data = {}
    for line in lines:
        line = line.strip()
        if not line: continue
        
        sep = 'ï¼š' if 'ï¼š' in line else ':'
        if sep in line:
            k, v = line.split(sep, 1)
            data[k.strip()] = v.strip()
            
    # è§£æä¸“ä¸šä»£ç : "ä¸´åºŠåŒ»å­¦ (630101)" -> name="ä¸´åºŠåŒ»å­¦", code="630101"
    raw_major = data.get("å¼€è®¾ä¸“ä¸š", "")
    major_name = raw_major
    major_code = ""
    if "(" in raw_major and ")" in raw_major:
        match = re.match(r"(.*?)\s*\((.*?)\)", raw_major)
        if match:
            major_name = match.group(1).strip()
            major_code = match.group(2).strip()

    return {
        "school_info": {
            "name": data.get("æœºæ„åç§°"),
            "province": data.get("çœä»½"),
            "school_code": data.get("å­¦æ ¡æ ‡è¯†ç ")
        },
        "major_info": {
            "name": major_name,
            "major_code": major_code,
            "duration_years": int(data.get("ä¿®ä¸šå¹´é™")) if data.get("ä¿®ä¸šå¹´é™") and data.get("ä¿®ä¸šå¹´é™").isdigit() else None,
            "year": int(data.get("å¹´ä»½")) if data.get("å¹´ä»½") and data.get("å¹´ä»½").isdigit() else None,
            "note": data.get("å¤‡æ³¨", "")
        }
    }

def auto_parse(content: str) -> dict:
    """
    è‡ªåŠ¨è¯†åˆ«å†…å®¹ç±»å‹å¹¶åˆ†å‘è§£æ
    """
    if "å­¦æ ¡æ ‡è¯†ç " in content and "å¼€è®¾ä¸“ä¸š" in content:
        return parse_school_major_info(content)
    elif "ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç " in content and "å¤‡æ¡ˆåŠå®Œæˆæ—¶é—´" in content:
        return parse_institution_info(content)
    else:
        # é»˜è®¤ä¸ºé—®å·/è®¿è°ˆ
        return parse_survey_content(content)

# --- æ¨¡å—äº”ï¼šTuoyu ä¸“ç”¨å¤„ç†å™¨ ---
class TuoyuContentParser:
    @staticmethod
    def parse_key_value_lines(content: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨å†…éƒ¨å®šä¹‰çš„ auto_parse è¿›è¡Œç»Ÿä¸€è§£æ
        è¿”å›ç»“æ„åŒ–å­—å…¸
        """
        try:
            parsed_data = auto_parse(content)
            
            # ä¸ºäº†å…¼å®¹æ—§é€»è¾‘ (check_rules ä¾èµ–æ‰å¹³å­—å…¸)ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ parsed_data æ‰å¹³åŒ–
            # ä½†åŒæ—¶ä¿ç•™ç»“æ„åŒ–æ•°æ®ä¾›åç»­ä½¿ç”¨
            
            flat_data = {}
            
            # 1. å¤„ç†é—®å·æ•°æ® (Survey)
            if 'basic_info' in parsed_data:
                # æå– basic_info åˆ°é¡¶å±‚
                for k, v in parsed_data['basic_info'].items():
                    if v is not None:
                        flat_data[k] = str(v)
                        # å…¼å®¹æ—§é”®å
                        if k == 'job_role': flat_data['å²—ä½'] = str(v)
                        if k == 'city': flat_data['åŸå¸‚'] = str(v)
                        if k == 'education': flat_data['å­¦å†'] = str(v)
                        if k == 'major': flat_data['ä¸“ä¸š'] = str(v)
                
                # æå– contents é‡Œçš„é—®ç­”å¯¹åˆ°é¡¶å±‚ (å¯é€‰ï¼Œç”¨äºæ›´ç»†ç²’åº¦çš„è§„åˆ™æ£€æŸ¥?)
                # ç›®å‰ check_rules ä¸»è¦æ£€æŸ¥ basic_infoï¼Œæ‰€ä»¥è¿™é‡Œå¯ä»¥ç®€åŒ–
            
            # 2. å¤„ç†æœºæ„æ•°æ® (Institution)
            elif 'institution_info' in parsed_data:
                info = parsed_data['institution_info']
                for k, v in info.items():
                    if v:
                        flat_data[k] = str(v)
                        # å…¼å®¹æ—§é”®å
                        if k == 'name': flat_data['æœºæ„åç§°'] = str(v)
                        if k == 'type': flat_data['æœºæ„ç±»å‹'] = str(v)
                        if k == 'address': flat_data['è¯¦ç»†åœ°å€'] = str(v)
                        if k == 'registration_date': flat_data['å¤‡æ¡ˆåŠå®Œæˆæ—¶é—´'] = str(v)
            
            # 3. å¤„ç†é«˜æ ¡æ•°æ® (School Major)
            elif 'school_info' in parsed_data:
                s_info = parsed_data['school_info']
                m_info = parsed_data['major_info']
                
                for k, v in s_info.items():
                    if v: flat_data[k] = str(v)
                for k, v in m_info.items():
                    if v: flat_data[k] = str(v)
                    
                # å…¼å®¹æ—§é”®å
                if s_info.get('school_code'): flat_data['å­¦æ ¡æ ‡è¯†ç '] = str(s_info['school_code'])
                if m_info.get('name'): flat_data['å¼€è®¾ä¸“ä¸š'] = str(m_info['name'])
            
            # å°†åŸå§‹ç»“æ„åŒ–æ•°æ®æŒ‚è½½åˆ°ç‰¹æ®Šå­—æ®µï¼Œæ–¹ä¾¿åç»­æå–
            flat_data['_structured_data'] = parsed_data
            
            return flat_data
        except Exception as e:
            print(f"âš ï¸ [Parse Error] {e}")
            return {}

class TuoyuProcessor:
    def __init__(self, api_client: DifyApiClient):
        self.api = api_client

    def parse_time_filter(self, time_filter: str) -> Tuple[Optional[datetime], Optional[datetime]]:
        if not time_filter:
            return None, None
        
        now = datetime.now()
        
        if 'è¿‘ä¸‰å¹´' in time_filter:
            # è¿‘ä¸‰å¹´é€šå¸¸æŒ‡å½“å‰å¹´ä»½å¾€å‰æ¨3å¹´ï¼Œæˆ–è€…365*3å¤©
            # è¿™é‡Œå–å½“å‰å¹´ä»½-3çš„1æœˆ1æ—¥å¼€å§‹
            start_date = now.replace(year=now.year - 3, month=1, day=1, hour=0, minute=0, second=0)
            return start_date, now
        
        # Try range "YYYY-MM-DD - YYYY-MM-DD"
        # å…¼å®¹å„ç§åˆ†éš”ç¬¦
        range_match = re.match(r'(\d{4}-\d{2}-\d{2})\s*[-~toè‡³]\s*(\d{4}-\d{2}-\d{2})', time_filter)
        if range_match:
            try:
                start = datetime.strptime(range_match.group(1), '%Y-%m-%d')
                end = datetime.strptime(range_match.group(2), '%Y-%m-%d')
                # End date implies end of that day?
                end = end.replace(hour=23, minute=59, second=59)
                return start, end
            except:
                pass
        
        # Try single date "YYYY-MM-DD" (Start Date)
        single_date_match = re.match(r'^(\d{4}-\d{2}-\d{2})$', time_filter.strip())
        if single_date_match:
             try:
                 start = datetime.strptime(single_date_match.group(1), '%Y-%m-%d')
                 return start, now
             except:
                 pass
                
        # Try single year "2014"
        year_match = re.match(r'^\d{4}$', time_filter.strip())
        if year_match:
             try:
                 year = int(year_match.group(0))
                 start = datetime(year, 1, 1)
                 end = datetime(year, 12, 31, 23, 59, 59)
                 return start, end
             except:
                 pass

        return None, None


    def extract_date_from_content(self, data: Dict[str, str]) -> Optional[datetime]:
        # 1. å¤‡æ¡ˆåŠå®Œæˆæ—¶é—´: 2019-12-31 15:42:13
        if 'å¤‡æ¡ˆåŠå®Œæˆæ—¶é—´' in data:
            try:
                # å¯èƒ½åŒ…å«æ—¶é—´ï¼Œä¹Ÿå¯èƒ½åªæœ‰æ—¥æœŸ
                val = data['å¤‡æ¡ˆåŠå®Œæˆæ—¶é—´']
                if len(val) > 10:
                    return datetime.strptime(val, '%Y-%m-%d %H:%M:%S')
                else:
                    return datetime.strptime(val, '%Y-%m-%d')
            except:
                pass
        
        # 2. å¹´ä»½: 2014
        if 'å¹´ä»½' in data:
            try:
                return datetime(int(data['å¹´ä»½']), 1, 1)
            except:
                pass
        
        # 3. å°è¯•ä» content å­—æ®µæœ¬èº«æ‰¾ï¼ˆå¦‚æœ parser æ²¡æå–å‡ºæ¥ï¼‰
        # æš‚æ—¶ä¾èµ– parser
        return None

    EDUCATION_MAP = {
        "é«˜èŒï¼ˆä¸“ç§‘ï¼‰": "é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰",
        "é«˜èŒä¸“ç§‘": "é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰",
        "ä¸“ç§‘": "é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰",
        "é«˜èŒ": "é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰",
        "é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰": "é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰",
        "å¤§ä¸“": "é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰",
        "vocational_college": "é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰",
        
        "æœ¬ç§‘": "æ™®é€šæœ¬ç§‘",
        "æ™®é€šæœ¬ç§‘": "æ™®é€šæœ¬ç§‘",
        "æœ¬ç§‘åŠä»¥ä¸Š": "æ™®é€šæœ¬ç§‘",
        "undergraduate": "æ™®é€šæœ¬ç§‘",
        
        "ä¸­èŒ": "ä¸­ç­‰èŒä¸šæ•™è‚²",
        "ä¸­ä¸“": "ä¸­ç­‰èŒä¸šæ•™è‚²",
        "é«˜ä¸­/ä¸­èŒ": "ä¸­ç­‰èŒä¸šæ•™è‚²",
        "senior_high_school": "ä¸­ç­‰èŒä¸šæ•™è‚²",
        
        "ç¡•å£«": "ç¡•å£«ç ”ç©¶ç”Ÿ",
        "ç ”ç©¶ç”Ÿ": "ç¡•å£«ç ”ç©¶ç”Ÿ",
        "ç¡•å£«ç ”ç©¶ç”Ÿ": "ç¡•å£«ç ”ç©¶ç”Ÿ",
        "master_degree": "ç¡•å£«ç ”ç©¶ç”Ÿ",
    }

    def normalize_education(self, text: str) -> str:
        if not text: return ""
        text = text.strip()
        # 1. ç›´æ¥æŸ¥è¡¨
        if text in self.EDUCATION_MAP:
            return self.EDUCATION_MAP[text]
        # 2. åŒ…å«åŒ¹é… (ç®€å•çš„åå‘æŸ¥æ‰¾ï¼Œä¼˜å…ˆåŒ¹é…é•¿è¯)
        # Sort keys by length desc to match "é«˜èŒï¼ˆä¸“ç§‘ï¼‰" before "é«˜èŒ"
        sorted_keys = sorted(self.EDUCATION_MAP.keys(), key=len, reverse=True)
        for k in sorted_keys:
            if k in text:
                return self.EDUCATION_MAP[k]
        return text

    def check_rules(self, data: Dict[str, str], regional_rules: Dict, time_range: Tuple) -> bool:
        # 1. Regional Rules Check
        if regional_rules:
            # --- é—®å·æ˜Ÿæ•°æ®è¿‡æ»¤é€»è¾‘ (Questionnaire) ---
            # è¯†åˆ«ç‰¹å¾ï¼šåŒ…å« "å²—ä½" æˆ– "job_role"
            is_questionnaire = 'å²—ä½' in data or 'job_role' in data
            
            if is_questionnaire:
                # è¿‡æ»¤æ¡ä»¶ï¼šmajorï¼ˆä¸“ä¸šï¼‰ã€scopeï¼ˆåŒºåŸŸï¼‰ã€levelï¼ˆå­¦å†ç­‰çº§ï¼‰
                # ã€é‡è¦ã€‘é—®å·æ•°æ®ä¸éœ€è¦æ—¶é—´è¿‡æ»¤
                
                # (1) Major Check
                req_major = regional_rules.get('major')
                if req_major:
                    major = data.get('ä¸“ä¸š') or data.get('major')
                    # æ¨¡ç³ŠåŒ¹é…
                    if not major or req_major not in major:
                        return False
                
                # (2) Scope Check (City/Province)
                req_scope = regional_rules.get('scope')
                if req_scope:
                    loc = data.get('åŸå¸‚') or data.get('çœä»½') or data.get('city') or data.get('province') or ""
                    if req_scope not in loc:
                        return False
                        
                # (3) Level Check (Education)
                req_level = regional_rules.get('level')
                if req_level:
                    edu = data.get('å­¦å†') or data.get('education')
                    
                    # ä½¿ç”¨å½’ä¸€åŒ–é€»è¾‘
                    norm_req = self.normalize_education(req_level)
                    norm_edu = self.normalize_education(edu)
                    
                    # å®½æ¾åŒ¹é…ï¼šå½’ä¸€åŒ–åç›¸ç­‰ï¼Œæˆ–è€…äº’ç›¸åŒ…å«
                    match = False
                    if not edu:
                        match = False
                    elif norm_req == norm_edu:
                        match = True
                    elif norm_req in norm_edu or norm_edu in norm_req:
                        match = True
                    
                    if not match:
                        return False
                
                # é—®å·æ•°æ®ç›´æ¥è¿”å› Trueï¼Œè·³è¿‡åç»­çš„æ—¶é—´æ£€æŸ¥
                return True

            else:
                # --- éé—®å·æ•°æ® (æœºæ„å¤‡æ¡ˆ & MOE) ---
                
                # (3) MOE Special Logic
                # è¯†åˆ«ç‰¹å¾ï¼šåŒ…å« "å­¦æ ¡æ ‡è¯†ç " æˆ– "å¼€è®¾ä¸“ä¸š"
                is_moe = 'å­¦æ ¡æ ‡è¯†ç ' in data or ('å¼€è®¾ä¸“ä¸š' in data and 'å²—ä½' not in data)
                
                if is_moe:
                    # MOE æ•°æ®é¢å¤–æ£€æŸ¥ major å’Œ level
                    req_major = regional_rules.get('major')
                    if req_major:
                        major = data.get('å¼€è®¾ä¸“ä¸š') or data.get('ä¸“ä¸š') or data.get('major')
                        if not major or req_major not in major:
                            return False
                    
                    # Level Check: åªæœ‰ regional_rules é‡Œçš„ level æ˜¯ â€˜é«˜èŒâ€™/'é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰'/'ä¸“ç§‘' æ—¶æ‰ä½¿ç”¨ MOE æ•°æ®
                    req_level = regional_rules.get('level')
                    # è¿™é‡Œçš„ valid_moe_levels ä¹Ÿå¯ä»¥ç”¨ normalize åˆ¤æ–­ï¼Œä½†ä¸ºäº†ä¿é™©å…ˆä¿ç•™ list
                    valid_moe_levels = ['é«˜èŒ', 'é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰', 'ä¸“ç§‘', 'é«˜èŒï¼ˆä¸“ç§‘ï¼‰', 'é«˜èŒä¸“ç§‘']
                    
                    # æ£€æŸ¥ req_level æ˜¯å¦å±äºé«˜èŒç±»
                    is_vocational = False
                    norm_req = self.normalize_education(req_level)
                    if norm_req == "é«˜ç­‰èŒä¸šæ•™è‚²ï¼ˆä¸“ç§‘ï¼‰":
                        is_vocational = True
                    else:
                        for v in valid_moe_levels:
                            if v in req_level:
                                is_vocational = True
                                break
                    
                    if not is_vocational:
                        return False
                        
                    # MOE æ•°æ®ä¹Ÿéœ€è¦æ£€æŸ¥ School
                    req_school = regional_rules.get('school')
                    if req_school:
                        name = data.get('æœºæ„åç§°') or data.get('institution_name') or data.get('åˆ«å') or data.get('institution')
                        if not name or req_school not in name: 
                             return False

                    # MOE æ•°æ®ä¹Ÿéœ€è¦æ£€æŸ¥ Scope
                    req_scope = regional_rules.get('scope')
                    if req_scope:
                        loc = data.get('åŸå¸‚') or data.get('çœä»½') or data.get('city') or data.get('province') or ""
                        if req_scope not in loc:
                            return False

                else:
                    # --- æ‰˜è‚²æœºæ„å¤‡æ¡ˆæ•°æ® (Tuoyu_institution) ---
                    # è¿‡æ»¤æ¡ä»¶ï¼šscopeï¼ˆåŒºåŸŸï¼‰å’Œ time_filterï¼ˆæ—¶é—´èŒƒå›´ï¼‰
                    # ã€é‡è¦ã€‘School ä¸æ˜¯é€šç”¨çš„ï¼æœºæ„å¤‡æ¡ˆæ•°æ®ä¸æ£€æŸ¥ Schoolï¼
                    
                    # (2) Scope Check (é€šç”¨)
                    req_scope = regional_rules.get('scope')
                    if req_scope:
                        # å­—æ®µï¼šåŸå¸‚, çœä»½, åŒºåŸŸç¼–å·(éœ€è¦æ˜ å°„?), city, province
                        # æœºæ„å¤‡æ¡ˆæ•°æ®é€šå¸¸æœ‰ "è¯¦ç»†åœ°å€" æˆ– "åŒºåŸŸç¼–å·"
                        loc = data.get('åŸå¸‚') or data.get('çœä»½') or data.get('city') or data.get('province') or data.get('è¯¦ç»†åœ°å€') or ""
                        # åŒºåŸŸç¼–å·å¤„ç†æ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶åªåŒ¹é…æ–‡æœ¬
                        if req_scope not in loc:
                            return False
                
        # 2. Time Filter Check
        # éœ€æ±‚ï¼šå¯¹äºæ‰˜è‚²æœºæ„å¤‡æ¡ˆæ•°æ®ä½¿ç”¨scopeï¼ˆåŒºåŸŸï¼‰å’Œtime_filterï¼ˆæ—¶é—´èŒƒå›´ï¼‰ä½œä¸ºæ¡ä»¶
        # MOE æ•°æ®å’Œé—®å·æ•°æ®æ˜¯å¦éœ€è¦æ—¶é—´è¿‡æ»¤ï¼Ÿ
        # ç”¨æˆ·æ˜ç¡®æŒ‡å‡ºï¼šé—®å·æ•°æ®ä¸éœ€è¦æ—¶é—´è¿‡æ»¤ã€‚
        # MOE æ•°æ®éœ€è¦æ—¶é—´è¿‡æ»¤å—ï¼Ÿä¹‹å‰çš„éœ€æ±‚é‡Œæåˆ°äº† MOE ä½¿ç”¨ time_filterã€‚
        # æœºæ„å¤‡æ¡ˆæ•°æ®æ˜ç¡®éœ€è¦æ—¶é—´è¿‡æ»¤ã€‚
        
        # å› æ­¤ï¼Œåªæœ‰éé—®å·æ•°æ®æ‰è¿›è¡Œæ—¶é—´æ£€æŸ¥
        # is_questionnaire å·²ç»åœ¨ä¸Šé¢å¤„ç†å¹¶è¿”å›äº†ï¼Œèƒ½èµ°åˆ°è¿™é‡Œçš„éƒ½æ˜¯éé—®å·æ•°æ®
        
        if time_range and time_range[0]:
            date_obj = self.extract_date_from_content(data)
            if date_obj:
                start, end = time_range
                # æ³¨æ„ï¼šend å¯èƒ½æ˜¯ None (å¦‚æœè¾“å…¥åªæ˜¯å¼€å§‹æ—¶é—´)
                # ä½† parse_time_filter å¯¹äºå•ä¸€æ—¥æœŸè¿”å›çš„æ˜¯ (start, now)
                # æ‰€ä»¥è¿™é‡Œå¯ä»¥ç›´æ¥æ¯”è¾ƒèŒƒå›´
                if not (start <= date_obj <= end):
                    return False
            else:
                # å¦‚æœæœ‰æ—¶é—´ç­›é€‰è¦æ±‚ï¼Œä½†æ•°æ®é‡Œæ²¡æœ‰æ—¶é—´ï¼š
                # ä¸¥æ ¼æ¨¡å¼ä¸‹è¿‡æ»¤æ‰ã€‚
                return False

        return True

    async def process(self, tasks: List[Dict], query_groups: List[Dict], regional_rules: Dict, time_filter: str) -> Dict[str, Any]:
        print(f"ğŸš€ [Tuoyu Mode] Rules: {regional_rules}, Time: {time_filter}")
        
        # 1. æ„é€ æŸ¥è¯¢ Query List
        # ä¸ºäº†é¿å… Rules ä¸­çš„ç‰¹å®šå­—æ®µï¼ˆå¦‚ schoolï¼‰æ±¡æŸ“å…¶ä»–ç±»å‹æ•°æ®çš„å¬å›ï¼ˆå¦‚æœºæ„å¤‡æ¡ˆæ•°æ®ï¼‰ï¼Œ
        # æˆ‘ä»¬æ„é€ ä¸¤ç»„ Rule Stringï¼š
        # A. Full Rules: åŒ…å«æ‰€æœ‰å­—æ®µ (é’ˆå¯¹ MOE ç­‰å¼ºåŒ¹é…)
        # B. General Rules: æ’é™¤ school å­—æ®µ (é’ˆå¯¹ æœºæ„å¤‡æ¡ˆ/é—®å· ç­‰é€šç”¨åŒ¹é…)
        
        rule_parts_full = []
        rule_parts_general = []
        
        if regional_rules:
            for k, v in regional_rules.items():
                if not v: continue
                v_str = str(v)
                
                # Full åŒ…å«æ‰€æœ‰
                rule_parts_full.append(v_str)
                
                # General æ’é™¤ school
                if k != 'school':
                    rule_parts_general.append(v_str)
                    
        rule_str_full = " ".join(rule_parts_full)
        rule_str_general = " ".join(rule_parts_general)
        
        queries_to_run = set()
        
        # åŸºç¡€ç­–ç•¥ï¼šå¦‚æœæ²¡æœ‰ query_groupsï¼Œç›´æ¥ä½¿ç”¨ Rule Strings
        if not query_groups:
            if rule_str_full: queries_to_run.add(rule_str_full)
            if rule_str_general: queries_to_run.add(rule_str_general)
            if not queries_to_run: queries_to_run.add("å…¨éƒ¨")
        else:
            # ç»„åˆç­–ç•¥ï¼šLocal Query + Rule String
            for group in query_groups:
                for q in group.get('local_queries', []):
                    # ç»„åˆ Full
                    if rule_str_full:
                        queries_to_run.add(f"{q} {rule_str_full}".strip())
                    # ç»„åˆ General (å¦‚æœä¸ Full ä¸åŒ)
                    if rule_str_general and rule_str_general != rule_str_full:
                        queries_to_run.add(f"{q} {rule_str_general}".strip())
                    # å¦‚æœæ²¡æœ‰ Rulesï¼Œå°±åªç”¨ q
                    if not rule_str_full and not rule_str_general:
                        queries_to_run.add(q)
                        
        queries_to_run = list(queries_to_run)
        print(f"ğŸ“‹ Generated {len(queries_to_run)} queries: {queries_to_run}")
        
        time_range = self.parse_time_filter(time_filter)
        
        # Store results as (db_id, doc_info)
        final_results_list = []
        
        for task in tasks:
            db_id = task.get('database_id')
            if not db_id: continue
            
            print(f"ğŸ” [Tuoyu] Searching DB: {db_id}")
            
            # Step 1: å¹¶å‘å¬å›æ‰€æœ‰ Query çš„ç»“æœ (Retrieve Chunks)
            # è¿™é‡Œçš„ payload å¯ä»¥å¤ç”¨
            payload = {
                "database_id_for_url": db_id,
                # "top_k": 100 
            }
            
            # Create retrieve tasks
            retrieve_coros = [self.api.retrieve(q, payload.copy()) for q in queries_to_run]
            results_list = await asyncio.gather(*retrieve_coros)
            
            # Flatten chunks
            all_chunks = []
            seen_chunk_ids = set()
            for res in results_list:
                for chunk in res:
                    # ç®€å•å»é‡ï¼Œé¿å…é‡å¤å¤„ç†
                    cid = chunk.get('id')
                    if cid not in seen_chunk_ids:
                        all_chunks.append(chunk)
                        seen_chunk_ids.add(cid)
            
            print(f"   -> Retrieved {len(all_chunks)} unique chunks from raw search")
            
            # Step 2: ç­›é€‰ç›¸å…³æ–‡æ¡£ ID (å»é‡ + è§„åˆ™è¿‡æ»¤)
            relevant_doc_ids = set()
            for chunk in all_chunks:
                # è§£æå†…å®¹
                content_data = TuoyuContentParser.parse_key_value_lines(chunk['content'])
                # è§„åˆ™æ£€æŸ¥
                if self.check_rules(content_data, regional_rules, time_range):
                    relevant_doc_ids.add(chunk['document_id'])
            
            print(f"   -> Found {len(relevant_doc_ids)} relevant unique documents")
            
            # Step 3: è·å–å®Œæ•´æ–‡æ¡£å¹¶å†æ¬¡è¿‡æ»¤ (Fetch Full Doc & Filter Segments)
            async def process_doc(d_id):
                # è·å–è¯¦æƒ…
                d_detail = await self.api.fetch_document_detail(db_id, d_id)
                if not d_detail: return None
                
                # è·å–åˆ†æ®µ
                segs = await self.api.fetch_all_segments(db_id, d_id)
                
                # è¿‡æ»¤åˆ†æ®µ
                valid_segs = []
                for seg in segs:
                    s_content = seg.get('content', '')
                    s_data = TuoyuContentParser.parse_key_value_lines(s_content)
                    if self.check_rules(s_data, regional_rules, time_range):
                        valid_segs.append(seg)
                
                if not valid_segs: return None
                
                # æ„é€ ç»“æœ
                pseudo_chunks = []
                for s in valid_segs:
                    # è§£æå†…å®¹ä»¥è·å–ç»“æ„åŒ–æ•°æ®
                    s_content = s.get("content", "")
                    s_parsed = TuoyuContentParser.parse_key_value_lines(s_content)
                    structured_data = s_parsed.get('_structured_data', {})
                    
                    pseudo_chunks.append({
                        "content": s_content,
                        "position": s.get("position"),
                        "score": 1.0, 
                        "document_id": d_id,
                        "database_id": db_id,
                        "document_name": d_detail.get('name'),
                        # å°†ç»“æ„åŒ–æ•°æ®æ³¨å…¥åˆ° chunk çš„ metadata ä¸­
                        "doc_metadata": structured_data 
                    })
                
                # æ³¨æ„ï¼šè¿™é‡Œä¼ é€’ç»™ format_document çš„ meta æ˜¯ d_detail (Dify API è¿”å›çš„æ–‡æ¡£è¯¦æƒ…)
                # ä½†æˆ‘ä»¬éœ€è¦æŠŠ structured_data ä¼ é€’å‡ºå»ã€‚
                # ContentFormatter.format_document ä¼šä¼˜å…ˆä½¿ç”¨ meta å‚æ•°é‡Œçš„ doc_metadata
                
                # ç­–ç•¥ï¼šä¿®æ”¹ d_detail çš„ doc_metadataï¼Œç”¨æˆ‘ä»¬çš„ç»“æ„åŒ–æ•°æ®è¦†ç›–/åˆå¹¶å®ƒ
                if pseudo_chunks:
                     # å–ç¬¬ä¸€ä¸ª chunk çš„ç»“æ„åŒ–æ•°æ®ä½œä¸ºæ•´ä¸ªæ–‡æ¡£çš„ metadata (é€šå¸¸ä¸€ä¸ªæ–‡æ¡£çš„å†…å®¹ç»“æ„æ˜¯ä¸€è‡´çš„)
                    doc_struct = pseudo_chunks[0]["doc_metadata"]
                    
                    # è¿™æ˜¯ä¸€ä¸ª Hack: æˆ‘ä»¬æŠŠç»“æ„åŒ–æ•°æ®å¡è¿› doc_metadata
                    # è¿™æ · ContentFormatter åœ¨å¤„ç†æ—¶ï¼Œå¦‚æœèƒ½è¯†åˆ«ï¼Œå°±å¯ä»¥ç›´æ¥è¾“å‡º
                    if "doc_metadata" not in d_detail:
                        d_detail["doc_metadata"] = {}
                    
                    # è¿™é‡Œçš„ doc_metadata å¯èƒ½æ˜¯ list (Dify é£æ ¼) ä¹Ÿå¯èƒ½æ˜¯ dict
                    # ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬æŠŠç»“æ„åŒ–æ•°æ®æ”¾åœ¨ä¸€ä¸ªç‰¹æ®Šå­—æ®µé‡Œï¼Œæˆ–è€…ç›´æ¥æ›¿æ¢
                    # ç”¨æˆ·å¸Œæœ› "æœ€ç»ˆè¾“å‡ºçš„æ—¶å€™å°†åŸæœ¬çš„metadataå­—æ®µä½¿ç”¨è¿™ç§jsonç»“æ„æ›¿æ¢"
                    
                    # è®©æˆ‘ä»¬ä¿®æ”¹ ContentFormatter.clean_metadata æˆ– format_document é€»è¾‘ï¼Ÿ
                    # ä¸ï¼Œç›´æ¥åœ¨è¿™é‡Œæ›¿æ¢æœ€ç®€å•ã€‚
                    # ä½†æ˜¯ d_detail["doc_metadata"] åŸæœ¬å¯èƒ½åŒ…å« filename ç­‰ä¿¡æ¯ï¼Œæœ€å¥½ä¿ç•™
                    
                    # æ–¹æ¡ˆï¼šå°† structured_data ä½œä¸ºä¸€ä¸ªå­—æ®µ 'structured_content' åŠ å…¥
                    # æˆ–è€…ï¼Œå¦‚æœç”¨æˆ·å¸Œæœ›å®Œå…¨æ›¿æ¢ doc_metadata ä¸ºè¿™ä¸ªç»“æ„åŒ– JSON...
                    
                    # æ ¹æ®ç”¨æˆ·éœ€æ±‚ 2: "æˆ‘å¸Œæœ›æœ€ç»ˆè¾“å‡ºçš„æ—¶å€™å°†åŸæœ¬çš„metadataå­—æ®µä½¿ç”¨è¿™ç§jsonç»“æ„æ›¿æ¢"
                    # è¿™æ„å‘³ç€æœ€ç»ˆ JSON é‡Œçš„ doc_metadata åº”è¯¥å°±æ˜¯ structured_data
                    d_detail["doc_metadata"] = doc_struct

                fmt_doc = ContentFormatter.format_document(pseudo_chunks, d_detail, context='full_doc')
                
                # è®¾ç½® Source Type
                sample_content = valid_segs[0].get('content', '')
                sample_data = TuoyuContentParser.parse_key_value_lines(sample_content)
                
                if 'å²—ä½' in sample_data or 'job_role' in sample_data:
                    fmt_doc['source_type'] = 'Tuoyu_Questionnaire'
                else:
                    fmt_doc['source_type'] = 'Tuoyu_institution'
                    
                return fmt_doc

            # Execute doc processing
            doc_tasks = [process_doc(did) for did in relevant_doc_ids]
            doc_results = await asyncio.gather(*doc_tasks, return_exceptions=True)
            
            for res in doc_results:
                if isinstance(res, Exception):
                    print(f"âš ï¸ [Doc Process Error] {repr(res)}")
                    continue
                if res:
                    final_results_list.append((db_id, res))

        return {"result": [{"retrieve_data": self._package_results(final_results_list)}]}


    def _package_results(self, results_list: List[Tuple[str, Dict]]) -> List[Dict]:
        grouped = defaultdict(list)
        for db_id, doc in results_list:
            grouped[db_id].append(doc)
            
        output = []
        for db_id, docs in grouped.items():
            output.append({
                "database_id": db_id,
                "document_infos": docs
            })
        return output

# --- ä¸»ç¨‹åºå…¥å£ ---
async def async_main(tasks: List[Dict], query_groups: List[Dict] = None, 
                     regional_rules: Any = None, time_filter: Any = None, run_mode: str = "X-Pilot") -> Dict[str, Any]:
    if not tasks: return {"result": []}
    
    client = DifyApiClient()
    
    try:
        # --- Tuoyu Mode Branch ---
        if run_mode == "Tuoyu":
            processor = TuoyuProcessor(client)
            # Re-implement packaging logic inside process or here
            # Let's verify process implementation
            
            # We need to pass DB ID out.
            # Let's modify TuoyuProcessor.process slightly to return structured data directly
            # Or handle it here.
            
            # Better to fix TuoyuProcessor.process to return the correct structure.
            return await processor.process(tasks, query_groups, regional_rules, time_filter)

        # --- Standard X-Pilot Mode ---
        if not query_groups: query_groups = [{"slide_id": "default", "local_queries": []}]
        
        orchestrator = RetrievalOrchestrator(client)
        # ... existing logic ...

        # --- Stage 0: ä»»åŠ¡å½’ç±» ---
        rag_tasks = [t for t in tasks if t.get("retrieval_mode") != "full_document_retrieval"]
        full_doc_tasks = [t for t in tasks if t.get("retrieval_mode") == "full_document_retrieval"]

        # Step 1: é¢„çƒ­ (Metadata Prefetch)
        await orchestrator.prefetch_metadata(tasks)

        # # Step 2: è®¡åˆ’ (Plan Construction)
        # plan = orchestrator.build_execution_plan(tasks)
        # debug_print(plan, "Execution Plan")
        #
        # # Step 3: æ‰§è¡Œ (Concurrent Slide Processing)
        # slide_tasks = [
        #     orchestrator.process_slide(group, plan, tasks)
        #     for group in query_groups
        # ]
        # slide_results = await asyncio.gather(*slide_tasks)
        #
        # return {"result": slide_results}

        # --- Stage 2: å¹¶å‘è®¡åˆ’ ---
        # æˆ‘ä»¬éœ€è¦åŒæ—¶åšä¸¤ä»¶äº‹ï¼š
        # A. è·‘æ‰€æœ‰çš„ Slide RAG
        # B. è·‘ä¸€æ¬¡ Full Document ä¸‹è½½

        # 2.1 å‡†å¤‡ RAG ä»»åŠ¡
        plan = orchestrator.build_execution_plan(rag_tasks)

        # 2.2 å®šä¹‰ A ç»„åç¨‹ (RAG)
        rag_coros = [
            orchestrator.process_slide(group, plan, rag_tasks)
            for group in query_groups
        ]

        # 2.3 å®šä¹‰ B ç»„åç¨‹ (Full Doc)
        full_doc_coros = [
            orchestrator.process_full_document_task(t)
            for t in full_doc_tasks
        ]

        print(f"ğŸš€ [Execute] Running {len(rag_coros)} slides RAG & {len(full_doc_coros)} doc fetches...")

        # --- Stage 3: å¹¶å‘æ‰§è¡Œ A å’Œ B ---
        # all_results ç»“æ„: [Slide1_Res, Slide2_Res, ..., Doc1_Res, Doc2_Res]
        all_results = await asyncio.gather(*(rag_coros + full_doc_coros))

        # --- Stage 4: ç»“æœåˆ†ç¦»ä¸æ³¨å…¥ (The Injection) ---

        # åˆ‡åˆ†ç»“æœåˆ—è¡¨
        split_idx = len(rag_coros)
        slide_results = list(all_results[:split_idx])  # åªæœ‰ Slide ç»“æœ
        doc_resources = list(all_results[split_idx:])  # åªæœ‰ Full Doc ç»“æœ (æ ‡å‡†åŒ–çš„ Dict)
        # ã€æ ¸å¿ƒé€»è¾‘ã€‘ï¼šå°† doc_resources æ³¨å…¥åˆ°æ¯ä¸€ä¸ª Slide çš„ retrieve_data ä¸­
        # è¿™æ ·ä¿è¯äº†â€œä¸€æ¬¡è·å–ï¼Œå¤„å¤„å¯ç”¨â€ï¼Œä¸”ç»´æŒäº† retrieve_data çš„ç»“æ„ç»Ÿä¸€æ€§
        if doc_resources:
            for slide in slide_results:
                # å»ºç«‹å½“å‰ slide å·²æœ‰çš„ DB æ˜ å°„è¡¨ï¼Œæ–¹ä¾¿åˆå¹¶
                existing_dbs = {item["database_id"]: item for item in slide["retrieve_data"]}

                for res in doc_resources:
                    db_id = res["database_id"]

                    # å¦‚æœè¯¥ Database å·²å­˜åœ¨äº RAG ç»“æœä¸­ï¼Œåˆ™å°† Full Doc çš„ document_infos åˆå¹¶è¿›å»
                    if db_id in existing_dbs:
                        # res["document_infos"] é‡Œçš„å…ƒç´ å·²ç»å¸¦æœ‰äº† source_type="document"
                        existing_dbs[db_id]["document_infos"].extend(res["document_infos"])
                    else:
                        # å¦‚æœæ˜¯æ–°çš„ Databaseï¼Œç›´æ¥æ·»åŠ 
                        slide["retrieve_data"].append(res)
        return {"result": slide_results}

    finally:
        await client.close()


def main(tasks: List[Dict], query_groups: List[Dict] = None, 
         regional_rules: Any = None, time_filter: Any = None, run_mode: str = "X-Pilot") -> Dict[str, Any]:
    """Dify èŠ‚ç‚¹çš„ä¸»å…¥å£ç‚¹"""
    try:
        return debug_print(asyncio.run(async_main(tasks, query_groups, regional_rules, time_filter, run_mode)))
    except Exception as e:
        import traceback
        return {
            "result": [
                {
                    "error": f"Workflow Failed: {str(e)}",
                    "traceback": traceback.format_exc()
                }
            ]
        }

# main([
#     {
#         "database_id": "5bf50c7a-3ba4-46c7-bbdc-71d68f641e0a",
#         "document_id": "6cc5b1e2-3bf3-47a8-b370-0eb0c4516c08",
#         "retrieval_mode": "segment_retrieval"
#     },
#     # {
#     #     "database_id": "5bf50c7a-3ba4-46c7-bbdc-71d68f641e0a",
#     #     "retrieval_mode": "full_database_retrieval"
#     # }
# ],
#     [
#         {
#             "local_queries": [
#                 "è§†é¢‘",
#                 "ç”µåŠ¨æ±½è½¦ è§†é¢‘",
#                 "æ¯”äºšè¿ªæ±‰EVè§†é¢‘"
#             ],
#             "slide_id": "chapter_1_slide_1"
#         }
#     ])
