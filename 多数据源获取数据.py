# Dify ä¾èµ–ç®¡ç†: è¯·ç¡®ä¿å·²æ·»åŠ  httpx, json-repair, trafilatura, pypdf2, beautifulsoup4, lxml
import asyncio
import httpx
import re
import os
import json
import time
import traceback
from typing import Any, Dict, List, Literal, Optional
from abc import ABC, abstractmethod
from io import BytesIO
from urllib.parse import urljoin
import tempfile
try:
    from markitdown import MarkItDown
except ImportError as e:
    print(f"â€¼ï¸ MarkItDown Import Error: {e}")
    MarkItDown = None
except Exception as e:
    print(f"â€¼ï¸ MarkItDown Unexpected Error: {e}")
    MarkItDown = None


# --- æ ¸å¿ƒä¾èµ– ---
# trafilatura ç”¨äºä»HTMLæå–ä¸»è¦å†…å®¹
import trafilatura
from trafilatura.settings import use_config

# PyPDF2 ç”¨äºè§£æPDF
import pdfplumber
import fitz
# BeautifulSoup ç”¨äºè¾…åŠ©è§£æHTMLï¼ˆä¾‹å¦‚æå–è§†é¢‘ï¼‰
from bs4 import BeautifulSoup

# ==============================================================================
# ====================== DIFY æœ¬åœ°è°ƒè¯•è¾…åŠ©æ¨¡å— =========================
# ==============================================================================
import pprint

# --- æœ¬åœ°è°ƒè¯•å¼€å…³ ---
# åœ¨ä½ çš„ IDE ä¸­è¿›è¡Œæµ‹è¯•æ—¶ï¼Œå°†æ­¤å€¼è®¾ä¸º Trueã€‚
# å½“ä½ å‡†å¤‡å°†ä»£ç å¤åˆ¶åˆ° Dify å¹³å°æ—¶ï¼Œè¯·å°†å…¶æ”¹å› Falseï¼Œæˆ–ç›´æ¥åˆ é™¤æ­¤è°ƒè¯•æ¨¡å—ã€‚
IS_LOCAL_DEBUG = True


def _dify_debug_return(data: Dict[str, Any], label: str = "Final Return") -> Dict[str, Any]:
    """
    ä¸€ä¸ªç”¨äºåœ¨ Dify ä»£ç èŠ‚ç‚¹ä¸­è¿›è¡Œæœ¬åœ°è°ƒè¯•çš„åŒ…è£…å‡½æ•°ã€‚

    å½“ IS_LOCAL_DEBUG ä¸º True æ—¶ï¼Œå®ƒä¼šæ¼‚äº®åœ°æ‰“å°å‡ºæœ€ç»ˆè¦è¿”å›çš„æ•°æ®ï¼Œ
    ç„¶ååŸå°ä¸åŠ¨åœ°è¿”å›è¯¥æ•°æ®ï¼Œä»¥ä¾¿ Dify å¹³å°èƒ½æ­£ç¡®æ¥æ”¶ã€‚

    Args:
        data (Dict[str, Any]): å‡†å¤‡ä» Dify èŠ‚ç‚¹è¿”å›çš„æ•°æ®ã€‚
        label (str, optional): ä¸€ä¸ªæ ‡ç­¾ï¼Œç”¨äºåœ¨æ§åˆ¶å°è¾“å‡ºä¸­æ ‡è¯†æ¥æºã€‚é»˜è®¤ä¸º "Final Return"ã€‚

    Returns:
        Dict[str, Any]: ä¼ å…¥çš„åŸå§‹æ•°æ®ã€‚
    """
    if IS_LOCAL_DEBUG:
        # æ‰“å°ä¸€ä¸ªæ¸…æ™°çš„åˆ†éš”ç¬¦å’Œæ ‡ç­¾ï¼Œæ–¹ä¾¿åœ¨ç»ˆç«¯ä¸­è¯†åˆ«
        print("\n" + "=" * 40 + f" DIFY DEBUG OUTPUT [{label}] " + "=" * 40)

        # ä½¿ç”¨ pprint æ¨¡å—è¿›è¡Œç¾åŒ–è¾“å‡ºï¼Œå¯¹å¤æ‚çš„åµŒå¥—å­—å…¸ç‰¹åˆ«å‹å¥½
        pprint.pprint(data, indent=2, width=120)

        # æ‰“å°ç»“æŸåˆ†éš”ç¬¦
        print("=" * 105 + "\n")

    # æ— è®ºæ˜¯å¦æ‰“å°ï¼Œéƒ½å¿…é¡»åŸå°ä¸åŠ¨åœ°è¿”å›åŸå§‹æ•°æ®
    return data


def _parse_input_data(raw_input: Any) -> Dict[str, Any]:
    """
    å¥å£®åœ°è§£æä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºï¼Œèƒ½åŒæ—¶å¤„ç†å¸¦ "datas" åŒ…è£…å’Œä¸å¸¦åŒ…è£…çš„ä¸¤ç§ç»“æ„ã€‚
    å¹¶åˆ†ç¦»å‡ºä¸åŒæ¥æºçš„ç½‘é¡µæœç´¢URLã€è§†é¢‘URLã€æ‹›è˜æŸ¥è¯¢å‚æ•°å’Œä¼ä¸šåç§°ï¼ŒåŒæ—¶ä¿ç•™å…ƒæ•°æ®ã€‚
    """
    print(
        f"============== æ­¥éª¤ 1: æ¥æ”¶åˆ°åŸå§‹è¾“å…¥ ==============\nTYPE: {type(raw_input)}\nVALUE: {raw_input}\n=======================================================")
    if isinstance(raw_input, str):
        if not raw_input.strip(): return {"web_url_info_list": [], "video_url_info_list": [], "career_payload": {},
                                          "enterprise_names": []}
        try:
            data = json.loads(raw_input)
        except json.JSONDecodeError as e:
            raise ValueError(f"æ— æ³•å°†è¾“å…¥å­—ç¬¦ä¸²è§£æä¸ºJSON: {e}")
    elif isinstance(raw_input, dict):
        data = raw_input
    else:
        raise TypeError(f"æœŸæœ›çš„è¾“å…¥ç±»å‹æ˜¯ str æˆ– dict, ä½†æ”¶åˆ°äº† {type(raw_input).__name__}")

    if "datas" in data and isinstance(data["datas"], dict):
        print("  [è§£æå™¨] æ£€æµ‹åˆ° 'datas' åŒ…è£…å±‚ï¼Œå°†ä½¿ç”¨å…¶å†…éƒ¨æ•°æ®ã€‚")
        datas_obj = data["datas"]
    else:
        print("  [è§£æå™¨] æœªæ£€æµ‹åˆ° 'datas' åŒ…è£…å±‚ï¼Œå°†ç›´æ¥ä½¿ç”¨é¡¶å±‚æ•°æ®ã€‚")
        datas_obj = data

    if not isinstance(datas_obj, dict): datas_obj = {}

    # Extract run_mode from input data if available
    extracted_run_mode = datas_obj.get("run_mode")
    print(f"  [è§£æå™¨] ä»è¾“å…¥ä¸­æå–åˆ°çš„ run_mode: {extracted_run_mode}")

    # --- 1. æå– Legacy Mode æ•°æ® ---
    comprehensive_data = datas_obj.get("comprehensive_data", [])
    tianyan_data = datas_obj.get("tianyan_check_data", [])
    
    # --- 2. æå– External Mode æ•°æ® ---
    general_web_data = datas_obj.get("general_web_data", [])
    institution_source_data = datas_obj.get("institution_source_data", [])
    # å…¼å®¹ Tuoyu æ¨¡å¼ä¸‹ web_query å¯èƒ½è¢«åˆ†é…åˆ°çš„ä½ç½®ï¼Œæˆ–è€…æœªæ¥ç›´æ¥ä¼ é€’ tuoyu_web_data
    # ç›®å‰ä¸Šæ¸¸é€»è¾‘æ˜¯å°† tuoyu_web_queries åˆ†é…ç»™äº† general_web_data æˆ– institution_source_data
    # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦é¢å¤–æå– tuoyu_web_dataï¼Œé™¤éä¸Šæ¸¸ç»“æ„æ”¹å˜


    # --- 3. æå– Shared æ•°æ® ---
    career_data = datas_obj.get("career_data", {})

    web_url_info_list = []
    video_url_info_list = []

    def _extract_urls(source_data, origin_key):
        """Helper to extract URLs from a list of query results."""
        if not isinstance(source_data, list): return
        for query_result in source_data:
            if not isinstance(query_result, dict): continue
            query_text = query_result.get("query", "")
            
            # Iterate through all keys to find result lists (e.g., 'web_results', 'video_results', 'policy_regional_results')
            for key, result_list in query_result.items():
                if not key.endswith("_results") or not isinstance(result_list, list):
                    continue
                
                for res in result_list:
                    if not isinstance(res, dict): continue

                    # Identify provider
                    provider = next(
                        (k[:-4] for k in res if k.endswith('_url') and '_embed_' not in k and '_thumbnail_' not in k),
                        None)
                    if not provider:
                        provider = next((k.split('_')[0] for k in res if '_url' in k), None)
                        if not provider: continue

                    result_type = res.get(f"{provider}_type")
                    
                    # Common info
                    info = {
                        "url": res.get(f"{provider}_url"), 
                        "title": res.get(f"{provider}_title", "Untitled"),
                        "source": res.get(f"{provider}_source"), 
                        "snippet": res.get(f"{provider}_snippet"),
                        "provider": provider, 
                        "query": query_text,
                        "origin_key": origin_key # Tag the source origin
                    }
                    
                    if not info["url"]: continue

                    if result_type == 'video':
                        info.update({
                            "video_id": res.get(f"{provider}_video_id"), 
                            "embed_url": res.get(f"{provider}_embed_url"),
                            "thumbnail_url": res.get(f"{provider}_thumbnail_url"),
                        })
                        video_url_info_list.append(info)
                    else:
                        web_url_info_list.append(info)

    # Process all sources
    _extract_urls(comprehensive_data, "comprehensive_data")
    _extract_urls(general_web_data, "general_web_data")
    _extract_urls(institution_source_data, "institution_source_data")

    career_payload = career_data if isinstance(career_data, dict) else {}
    enterprise_names: List[str] = []
    if isinstance(tianyan_data, str) and tianyan_data.strip():
        print("  [è§£æå™¨] æ£€æµ‹åˆ° tianyan_check_data ä¸ºå­—ç¬¦ä¸²ï¼Œå°†å¤„ç†å•ä¸ªä¼ä¸šã€‚")
        enterprise_names.append(tianyan_data.strip())
    elif isinstance(tianyan_data, list):
        print(f"  [è§£æå™¨] æ£€æµ‹åˆ° tianyan_check_data ä¸ºåˆ—è¡¨ï¼Œå°†å¤„ç† {len(tianyan_data)} ä¸ªä¼ä¸šã€‚")
        enterprise_names = [str(name).strip() for name in tianyan_data if isinstance(name, str) and str(name).strip()]
    
    # Detect Mode
    mode = "legacy"
    if extracted_run_mode and extracted_run_mode.lower() == "tuoyu":
        mode = "external"
    elif extracted_run_mode and extracted_run_mode.lower() == "x-pilot":
        mode = "legacy"
    else:
        if general_web_data or institution_source_data:
            mode = "external"

    parsed_result = {
        "mode": mode,
        "web_url_info_list": web_url_info_list,
        "video_url_info_list": video_url_info_list,
        "career_payload": career_payload,
        "enterprise_names": enterprise_names
    }

    print(
        f"============== æ­¥éª¤ 2: è¾“å…¥è§£æå®Œæ¯• ==============\næ¨¡å¼: {mode}\nç½‘é¡µURLæ•°é‡: {len(web_url_info_list)}\nè§†é¢‘URLæ•°é‡: {len(video_url_info_list)}\næ‹›è˜è´Ÿè½½: {career_payload}\nä¼ä¸šåç§°åˆ—è¡¨: {enterprise_names} (å…± {len(enterprise_names)} ä¸ª)\n=======================================================")

    return parsed_result


# --- 1. è¾“å…¥è§£ææ¨¡å— ---
# ã€å·²ä¿®å¤ã€‘æ›¿æ¢è¿™ä¸ªå‡½æ•°
# def _parse_input_data(raw_input: Any) -> Dict[str, Any]:
#     """
#     å¥å£®åœ°è§£æä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºï¼Œèƒ½åŒæ—¶å¤„ç†å¸¦ "datas" åŒ…è£…å’Œä¸å¸¦åŒ…è£…çš„ä¸¤ç§ç»“æ„ã€‚
#     """
#     print(
#         f"============== æ­¥éª¤ 1: æ¥æ”¶åˆ°åŸå§‹è¾“å…¥ ==============\nTYPE: {type(raw_input)}\nVALUE: {raw_input}\n=======================================================")
#     if isinstance(raw_input, str):
#         if not raw_input.strip(): return {"url_list": [], "career_payload": {}, "enterprise_name": ""}
#         try:
#             data = json.loads(raw_input)
#         except json.JSONDecodeError as e:
#             raise ValueError(f"æ— æ³•å°†è¾“å…¥å­—ç¬¦ä¸²è§£æä¸ºJSON: {e}")
#     elif isinstance(raw_input, dict):
#         data = raw_input
#     else:
#         raise TypeError(f"æœŸæœ›çš„è¾“å…¥ç±»å‹æ˜¯ str æˆ– dict, ä½†æ”¶åˆ°äº† {type(raw_input).__name__}")
#
#     # --- æ ¸å¿ƒä¿®å¤é€»è¾‘ ---
#     # æ£€æŸ¥é¡¶å±‚æ˜¯å¦æœ‰ "datas" é”®ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°±è®¤ä¸ºå½“å‰æ•´ä¸ªå¯¹è±¡å°±æ˜¯æˆ‘ä»¬è¦çš„æ•°æ®ä½“ã€‚
#     if "datas" in data and isinstance(data["datas"], dict):
#         print("  [è§£æå™¨] æ£€æµ‹åˆ° 'datas' åŒ…è£…å±‚ï¼Œå°†ä½¿ç”¨å…¶å†…éƒ¨æ•°æ®ã€‚")
#         datas_obj = data["datas"]
#     else:
#         print("  [è§£æå™¨] æœªæ£€æµ‹åˆ° 'datas' åŒ…è£…å±‚ï¼Œå°†ç›´æ¥ä½¿ç”¨é¡¶å±‚æ•°æ®ã€‚")
#         datas_obj = data
#     # --- ä¿®å¤ç»“æŸ ---
#
#     if not isinstance(datas_obj, dict): datas_obj = {}
#
#     comprehensive_data = datas_obj.get("comprehensive_data", [])
#     career_data = datas_obj.get("career_data", {})
#     tianyan_data = datas_obj.get("tianyan_check_data", "")
#
#     url_list = []
#     if isinstance(comprehensive_data, list):
#         for query_result in comprehensive_data:
#             if not isinstance(query_result, dict): continue
#             for res_list_key in ["web_results", "video_results"]:
#                 for res in query_result.get(res_list_key, []):
#                     if not isinstance(res, dict): continue
#                     url, title, provider = None, None, None
#                     for key, value in res.items():
#                         if key.endswith("_url"):
#                             url, provider = value, key.split('_url')[0]
#                             title = res.get(f"{provider}_title", "Untitled")
#                             break
#                     if url and provider:
#                         url_list.append({"url": url, "title": title, "provider": provider})
#     career_payload = career_data if isinstance(career_data, dict) else {}
#     enterprise_name = tianyan_data if isinstance(tianyan_data, str) else ""
#     parsed_result = {"url_list": url_list, "career_payload": career_payload, "enterprise_name": enterprise_name.strip()}
#
#     print(
#         f"============== æ­¥éª¤ 2: è¾“å…¥è§£æå®Œæ¯• ==============\nURL æ•°é‡: {len(url_list)}\næ‹›è˜è´Ÿè½½: {career_payload}\nä¼ä¸šåç§°: '{enterprise_name.strip()}'\n=======================================================")
#
#     return parsed_result


# def _parse_input_data(raw_input: Any) -> Dict[str, Any]:
#     """
#     å¥å£®åœ°è§£æä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºï¼Œåˆ†ç¦»å‡ºwebæœç´¢URLå’Œæ‹›è˜æŸ¥è¯¢å‚æ•°ã€‚
#     """
#     if isinstance(raw_input, str):
#         if not raw_input.strip(): return {"url_list": [], "career_payload": {}}
#         try:
#             data = json.loads(raw_input)
#         except json.JSONDecodeError as e:
#             raise ValueError(f"æ— æ³•å°†è¾“å…¥å­—ç¬¦ä¸²è§£æä¸ºJSON: {e}")
#     elif isinstance(raw_input, dict):
#         data = raw_input
#     else:
#         raise TypeError(f"æœŸæœ›çš„è¾“å…¥ç±»å‹æ˜¯ str æˆ– dict, ä½†æ”¶åˆ°äº† {type(raw_input).__name__}")
#     # å®‰å…¨åœ°æ·±å…¥åˆ° 'datas' ç»“æ„
#     datas_obj = data.get("datas", {})
#     if not isinstance(datas_obj, dict): datas_obj = {}
#     comprehensive_data = datas_obj.get("comprehensive_data", [])
#     career_data = datas_obj.get("career_data", {})
#     tianyan_data = datas_obj.get("tianyan_check_data", "")
#     # 1. æå–URLåˆ—è¡¨
#     url_list = []
#     if isinstance(comprehensive_data, list):
#         for query_result in comprehensive_data:
#             if not isinstance(query_result, dict): continue
#             for res_list_key in ["web_results", "video_results"]:
#                 for res in query_result.get(res_list_key, []):
#                     if not isinstance(res, dict): continue
#                     url, title, provider = None, None, None
#                     for key, value in res.items():
#                         if key.endswith("_url"):
#                             url = value
#                             provider = key.split('_url')[0]
#                             title = res.get(f"{provider}_title", "Untitled")
#                             break
#                     if url and provider:
#                         url_list.append({"url": url, "title": title, "provider": provider})
#     # 2. æå–èŒä¸šæŸ¥è¯¢è´Ÿè½½
#     career_payload = career_data if isinstance(career_data, dict) else {}

#     # 3. æå–ä¼ä¸šåç§°
#     enterprise_name = tianyan_data if isinstance(tianyan_data, str) else ""
#     return {"url_list": url_list, "career_payload": career_payload, "enterprise_name": enterprise_name.strip()}

# --- 2. æŠ½è±¡ä¸å®ç°åˆ†ç¦»ï¼šå†…å®¹æŠ“å–å™¨ ---

class ResourceParser:
    """
    Unified resource parser using MarkItDown to handle various document formats
    (PDF, DOCX, PPTX, XLSX, CSV, etc.) and convert them to Markdown.
    """
    def __init__(self):
        if MarkItDown:
            try:
                self.md = MarkItDown()
            except Exception as e:
                print(f"âš ï¸ Failed to initialize MarkItDown instance: {e}")
                print(traceback.format_exc())
                self.md = None
        else:
            print("âš ï¸ MarkItDown package not found.")
            self.md = None

    def parse(self, binary_content: bytes, file_extension: str) -> str:
        """
        Parse binary content based on file extension.
        """
        if not self.md:
            return "ResourceParser is not available (MarkItDown not initialized)."

        # MarkItDown typically requires a file path to auto-detect specific formats correctly
        # We will create a temporary file with the correct extension
        suffix = file_extension if file_extension.startswith(".") else f".{file_extension}"
        
        tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(binary_content)
                tmp_path = tmp_file.name
            
            # Perform conversion
            result = self.md.convert(tmp_path)
            if result and result.text_content:
                return result.text_content
            return ""
        except Exception as e:
            print(f"âš ï¸ MarkItDown parsing failed for {suffix} file: {e}")
            return f"Error parsing document: {e}"
        finally:
            # Clean up the temporary file
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                except OSError:
                    pass

class ContentScraper(ABC):
    """å†…å®¹æŠ“å–å™¨çš„æŠ½è±¡åŸºç±»ã€‚"""

    # ã€ä¿®æ”¹ã€‘æ–¹æ³•ç­¾åï¼Œæ¥æ”¶ä¸€ä¸ªåŒ…å«æ‰€æœ‰å…ƒæ•°æ®çš„å­—å…¸
    @abstractmethod
    async def scrape(self, item_info: Dict[str, Any], client: httpx.AsyncClient) -> Dict[str, Any]:
        """
        æŠ“å–å•ä¸ªURLçš„å†…å®¹ï¼Œå¹¶è¿”å›åˆå¹¶äº†åŸå§‹ä¿¡æ¯çš„ç»“æœã€‚
        """
        pass


# --- 2.1 SearchAPI.io çš„æ‰‹åŠ¨æŠ“å–ä¸æ¸…æ´—å®ç° ---
class SearchApiScraper(ContentScraper):
    def __init__(self):
        self.trafilatura_config = use_config()
        self.trafilatura_config.set("DEFAULT", "EXTRACTION_TIMEOUT", "10")
        self.resource_parser = ResourceParser()

        # ç¼–è¯‘å¸¸ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ€§èƒ½
        self.NOISY_PATTERNS = [re.compile(p, re.IGNORECASE) for p in [
            r'^\s*$', r'^[\-=*#_]{3,}$', r'.*\.(html|shtml|htm|php)\s*$',
            r'.{0,50}(æœç‹|ç½‘æ˜“|è…¾è®¯|æ–°æµª|ç™»å½•|æ³¨å†Œ|ç‰ˆæƒæ‰€æœ‰|ç‰ˆæƒå£°æ˜).{0,50}$',
            r'\[\d+\]|\[ä¸‹ä¸€é¡µ\]|\[ä¸Šä¸€é¡µ\]', r'\[(ç¼–è¾‘|æŸ¥çœ‹å†å²|è®¨è®º|é˜…è¯»|æ¥æº|åŸæ ‡é¢˜)\]',
            r'^\*+\s*\[.*?\]\(.*?\)',
            r'^\s*(åˆ†äº«åˆ°|æ‰«æäºŒç»´ç |è¿”å›æœç‹|æŸ¥çœ‹æ›´å¤š|è´£ä»»ç¼–è¾‘|è®°è€…|é€šè®¯å‘˜)',
            r'^\s*([äº¬å…¬ç½‘å®‰å¤‡äº¬ç½‘æ–‡äº¬ICPå¤‡]|äº’è”ç½‘æ–°é—»ä¿¡æ¯æœåŠ¡è®¸å¯è¯|ä¿¡æ¯ç½‘ç»œä¼ æ’­è§†å¬èŠ‚ç›®è®¸å¯è¯)',
        ]]
        self.IMG_PATTERN = re.compile(r'(!\[(.*?)\]\((.*?)\))')
        self.LINK_PATTERN = re.compile(r'\[.*?\]\(.*?\)')
        self.EDITOR_PATTERN = re.compile(r'(\(|\[)\s*è´£ä»»ç¼–è¾‘ï¼š.*?\s*(\)|\])')

    # --- 2.1.1 å†…å®¹æå–å·¥å…· (æ¥è‡ªæ‚¨çš„ä»£ç ) ---
    def _extract_pdf_text(self, binary_content: bytes) -> str:
        """
        ä½¿ç”¨ PyMuPDF (fitz) ä» PDF çš„äºŒè¿›åˆ¶å†…å®¹ä¸­å¿«é€Ÿæå–æ–‡æœ¬ã€‚
        """
        text_parts = []
        try:
            # ç›´æ¥ä»å†…å­˜ä¸­çš„å­—èŠ‚æµæ‰“å¼€ PDF
            with fitz.open(stream=binary_content, filetype="pdf") as doc:
                # é™åˆ¶å¤„ç†çš„é¡µæ•°ï¼Œé¿å…å¤„ç†è¶…å¤§æ–‡ä»¶
                num_pages_to_process = min(len(doc), self.PDF_MAX_PAGES_TO_PROCESS)
                if len(doc) > self.PDF_MAX_PAGES_TO_PROCESS:
                    print(f"  ğŸ“„ PDF é¡µæ•°è¿‡å¤š ({len(doc)} pages), åªå¤„ç†å‰ {self.PDF_MAX_PAGES_TO_PROCESS} é¡µã€‚")
                for i in range(num_pages_to_process):
                    page = doc.load_page(i)
                    page_text = page.get_text("text", sort=True)  # sort=True å°è¯•ä¿æŒé˜…è¯»é¡ºåº
                    if page_text:
                        text_parts.append(page_text)

            return "\n\n".join(text_parts).strip()
        except Exception as e:
            print(f"âš ï¸ PyMuPDF (fitz) è§£æå¤±è´¥: {e}")
            return ""

    def _parse_videos_from_html(self, html: str, base_url: str) -> List[str]:
        try:
            soup = BeautifulSoup(html, "lxml")
            videos = []
            for video in soup.find_all("video"):
                src = video.get("src")
                if src: videos.append(urljoin(base_url, src))
                for source in video.find_all("source"):
                    src = source.get("src")
                    if src: videos.append(urljoin(base_url, src))
            for iframe in soup.find_all("iframe"):
                src = iframe.get("src")
                if src and any(k in src for k in ["youtube", "vimeo", "embed", ".mp4"]):
                    videos.append(urljoin(base_url, src))
            return list(dict.fromkeys(videos))  # å»é‡å¹¶ä¿æŒé¡ºåº
        except Exception as e:
            print(f"âš ï¸ è§†é¢‘è§£æå¤±è´¥: {e}")
            return []

    # --- 2.1.2 å†…å®¹æ¸…æ´—å·¥å…· (æ¥è‡ªæ‚¨çš„ä»£ç ï¼Œå·²ä¼˜åŒ–å’Œå¼‚æ­¥åŒ–) ---
    async def _is_valid_image_url_async(self, url: str, client: httpx.AsyncClient) -> bool:
        if not url or not url.startswith(('http://', 'https://')): return False
        try:
            resp = await client.head(url, timeout=5, follow_redirects=True)
            content_type = resp.headers.get('content-type', '').lower()
            return resp.is_success and 'image' in content_type
        except httpx.RequestError:
            return False

    async def _remove_invalid_images_async(self, md: str, client: httpx.AsyncClient) -> str:
        MAX_IMAGES_TO_VALIDATE = 25
        matches = list(self.IMG_PATTERN.finditer(md))
        urls_to_check_all = {m.group(3).strip() for m in matches}

        urls_to_check = set(list(urls_to_check_all)[:MAX_IMAGES_TO_VALIDATE])
        if len(urls_to_check_all) > MAX_IMAGES_TO_VALIDATE:
            print(f"âš ï¸ å›¾ç‰‡æ•°é‡è¿‡å¤š ({len(urls_to_check_all)}), åªéªŒè¯å‰ {MAX_IMAGES_TO_VALIDATE} å¼ ã€‚")

        tasks = {url: self._is_valid_image_url_async(url, client) for url in urls_to_check}
        results = await asyncio.gather(*tasks.values(), return_exceptions=True)

        url_status = dict(zip(tasks.keys(), results))
        valid_urls = {url for url, res in url_status.items() if isinstance(res, bool) and res}
        valid_urls.update(urls_to_check_all - urls_to_check)  # æœªæ£€æŸ¥çš„é»˜è®¤æœ‰æ•ˆ

        def replacer(match: re.Match):
            return match.group(0) if match.group(3).strip() in valid_urls else ""

        return self.IMG_PATTERN.sub(replacer, md)

    def _is_noisy_line(self, line: str) -> bool:
        stripped = line.strip()
        for pat in self.NOISY_PATTERNS:
            if pat.search(stripped): return True
        links = self.LINK_PATTERN.findall(stripped)
        if len(links) > 2 and len(stripped) / (len(links) + 1) < 30: return True
        return False

    async def _clean_content_async(self, text: str, client: httpx.AsyncClient) -> str:
        if not text: return ""
        text = await self._remove_invalid_images_async(text, client)

        lines = text.splitlines()
        cleaned_lines = []
        for line in lines:
            if not self._is_noisy_line(line):
                line = self.EDITOR_PATTERN.sub('', line).strip()
                if line: cleaned_lines.append(line)

        # å»é™¤è¿ç»­ç©ºè¡Œ
        out = []
        for i, line in enumerate(cleaned_lines):
            if i > 0 and not line.strip() and not cleaned_lines[i - 1].strip():
                continue
            out.append(line)

        return "\n".join(out).strip()

    # --- 2.1.3 ä¸»æŠ“å–å‡½æ•° (æ¥è‡ªæ‚¨çš„ä»£ç ï¼Œå°è£…ä¸ºscrapeæ–¹æ³•) ---
    async def scrape(self, item_info: Dict[str, Any], client: httpx.AsyncClient) -> dict:
        url = item_info.get("url")
        print(f"ğŸ•¸ï¸ [SearchAPI Scraper] å¼€å§‹å¤„ç†: {url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

        try:
            # 1. æ£€æµ‹æ˜¯å¦ä¸ºæ”¯æŒçš„æ–‡æ¡£ç±»å‹
            supported_extensions = {".pdf", ".docx", ".doc", ".pptx", ".ppt", ".xlsx", ".xls", ".csv"}
            url_lower = url.lower()
            ext = os.path.splitext(url_lower)[1]
            
            is_document = ext in supported_extensions
            content_type = ""

            # å¦‚æœè¿™ç±»å·²çŸ¥æ‰©å±•åï¼Œå¯ä»¥ç›´æ¥è®¤å®šä¸ºæ–‡æ¡£ï¼Œå‡å°‘ HEAD è¯·æ±‚ï¼ˆæˆ–ä¿ç•™ HEAD ä»¥æ£€æµ‹å¤§å°ï¼‰
            # è¿™é‡Œä¿ç•™ HEAD è¯·æ±‚ä»¥è·å– Content-Type å’Œ Content-Length
            async with client.stream("HEAD", url, headers=headers, follow_redirects=True) as head_response:
                content_type = head_response.headers.get('content-type', '').lower()
                content_length = int(head_response.headers.get('content-length', 0))
                
                # Check for document mime types if extension wasn't obvious
                if not is_document:
                    if 'pdf' in content_type: is_document = True; ext = ".pdf"
                    elif 'word' in content_type or 'officedocument' in content_type: is_document = True; ext = ".docx" # Simplification
                    elif 'excel' in content_type or 'spreadsheet' in content_type: is_document = True; ext = ".xlsx"
                    elif 'powerpoint' in content_type or 'presentation' in content_type: is_document = True; ext = ".pptx"
                    elif 'csv' in content_type: is_document = True; ext = ".csv"

                # ã€ä¼˜åŒ–ã€‘æ£€æŸ¥æ–‡ä»¶å¤§å° (é™åˆ¶ä¸º 20MB)
                MAX_SIZE = 20 * 1024 * 1024
                if is_document and content_length > MAX_SIZE:
                    raise ValueError(f"æ–‡æ¡£æ–‡ä»¶è¿‡å¤§ ({content_length / 1024 / 1024:.2f}MB > 20MB)ï¼Œè·³è¿‡å¤„ç†ã€‚")

            # æ ¹æ®ç±»å‹æ‰§è¡Œä¸åŒé€»è¾‘
            raw_content, final_url = "", url

            if is_document:
                print(f"  ğŸ“„ [SearchAPI Scraper] æ£€æµ‹åˆ°æ–‡æ¡£ ({ext}): {url}")
                try:
                    async def _download_doc():
                        resp = await client.get(url, timeout=None, headers=headers, follow_redirects=True)
                        resp.raise_for_status()
                        return str(resp.url), await resp.aread()
                    
                    final_url, file_bytes = await asyncio.wait_for(_download_doc(), timeout=60)
                except asyncio.TimeoutError:
                    raise TimeoutError(f"ä¸‹è½½è¶…æ—¶ (60s): {url}")

                print(f"   rocket [SearchAPI Scraper] æ­£åœ¨ä½¿ç”¨ ResourceParser (MarkItDown) è§£æ...")
                # Run synchronous parsing in a thread
                raw_content = await asyncio.to_thread(self.resource_parser.parse, file_bytes, ext or ".bin")
            else:
                print(f"  ğŸ“‘ [SearchAPI Scraper] æ£€æµ‹åˆ° HTML: {url}")
                response = await client.get(url, timeout=20, headers=headers, follow_redirects=True)
                response.raise_for_status()
                final_url = str(response.url)
                html_content = response.text
                raw_content = await asyncio.to_thread(
                    trafilatura.extract, html_content, config=self.trafilatura_config,
                    output_format='markdown', include_images=True, favor_recall=True)

                # HTMLçš„è§†é¢‘è§£æå’Œæ¸…æ´—
                if raw_content:
                    print(f"  ğŸ§¹ [SearchAPI Scraper] æ­£åœ¨æ¸…æ´—HTMLå†…å®¹: {final_url}")
                    cleaned_content = await self._clean_content_async(raw_content, client)
                    videos = self._parse_videos_from_html(html_content, final_url)
                    if videos:
                        video_section = "\n\n## å‚è€ƒè§†é¢‘:\n" + "\n".join(f"- {vid}" for vid in videos)
                        cleaned_content += video_section
                    raw_content = cleaned_content

            if not raw_content: raise ValueError("å†…å®¹æå–è¿”å›ä¸ºç©ºã€‚")
            print(f"âœ… [SearchAPI Scraper] æˆåŠŸ: {url}")
            return {**item_info, "url": final_url, "content": raw_content, "status": "success"}
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥ {url}: {type(e).__name__} - {e}"
            print(f"âš ï¸ [SearchAPI Scraper] {error_msg}")
            return {**item_info, "content": "", "status": "failed", "error_message": str(e)}


# --- 2.2 FirecrawlScraper ---
class FirecrawlScraper(ContentScraper):
    def __init__(self):
        self.api_key = os.environ.get("FIRECRAWL_API_KEY", "fc-a36b7d2fb273485680d0fe6abd686935")
        if not self.api_key: raise ValueError("æœªæä¾› Firecrawl API Keyã€‚")
        self.base_url = "https://api.firecrawl.dev/v2/scrape"
        self.headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}

    async def scrape(self, item_info: Dict[str, Any], client: httpx.AsyncClient) -> dict:
        url = item_info.get("url")
        print(f"ğŸ”¥ [Firecrawl Scraper] å¼€å§‹å¤„ç†: {url}")
        try:
            # ã€ä¿®æ”¹ã€‘æ ¹æ®æ–‡æ¡£ï¼Œç§»é™¤ pageOptionsï¼Œå°†é€‰é¡¹ç½®äºé¡¶å±‚
            payload = {
                "url": url,
                "onlyMainContent": True,
                "removeBase64Images": True,
                "blockAds": True
            }
            resp = await client.post(self.base_url, headers=self.headers, json=payload, timeout=45)

            if not resp.is_success:
                try:
                    error_details = resp.json()
                    raise httpx.HTTPStatusError(f"APIè¿”å›é”™è¯¯: {error_details.get('error', str(error_details))}",
                                                request=resp.request, response=resp)
                except json.JSONDecodeError:
                    resp.raise_for_status()

            data_wrapper = resp.json()

            # ã€ä¿®æ”¹ã€‘æ ¹æ®æ–‡æ¡£ï¼Œæ£€æŸ¥é¡¶å±‚ success é”®å’Œ data å­—æ®µ
            if not data_wrapper.get("success"):
                raise ValueError(f"APIè¿”å›å¤±è´¥çŠ¶æ€: {data_wrapper.get('error', 'æœªçŸ¥é”™è¯¯')}")

            data = data_wrapper.get("data")
            if not data:
                raise ValueError("APIè¿”å›çš„ 'data' å­—æ®µä¸ºç©ºã€‚")

            content = data.get("markdown")
            if content is None:
                raise ValueError("APIæœªè¿”å› 'markdown' å­—æ®µã€‚")

            final_url = data.get("metadata", {}).get("sourceURL", url)

            print(f"âœ… [Firecrawl Scraper] æˆåŠŸ: {url}")
            return {**item_info, "url": final_url, "content": content, "status": "success"}

        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥ {url}: {type(e).__name__} - {e}"
            print(f"âš ï¸ [Firecrawl Scraper] {error_msg}")
            return {**item_info, "content": "", "status": "failed", "error_message": str(e)}


# --- 2.3 JinaScraper ---
class JinaScraper(ContentScraper):
    def __init__(self):
        self.api_key = os.environ.get("JINA_API_KEY",
                                      "jina_b4348ffc39ca47bfbe753b95f59428c7i6ifkOFXRPdF3dRa5Rwb6T8FvrLH")
        if not self.api_key: raise ValueError("æœªæä¾› Jina API Keyã€‚")
        self.base_url = "https://r.jina.ai/"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "X-Return-Format": "markdown"  # å…³é”®ï¼šç›´æ¥è·å–Markdown
        }

    async def scrape(self, item_info: Dict[str, Any], client: httpx.AsyncClient) -> dict:
        url = item_info.get("url")
        print(f"ğŸŒ€ [Jina Scraper] å¼€å§‹å¤„ç†: {url}")
        try:
            # Jinaçš„Reader API å¯¹GETè¯·æ±‚æ›´å‹å¥½ï¼Œç›´æ¥æ‹¼æ¥URL
            target_url = f"{self.base_url}{url}"
            resp = await client.get(target_url, headers=self.headers, timeout=45)
            resp.raise_for_status()

            # ã€ä¿®æ”¹ã€‘æ ¹æ®æ–‡æ¡£ï¼ŒJina å¯èƒ½ç›´æ¥è¿”å› Markdown æ–‡æœ¬ï¼Œä¹Ÿå¯èƒ½è¿”å› JSON
            content_type = resp.headers.get("content-type", "").lower()
            if "application/json" in content_type:
                data_wrapper = resp.json()
                if data_wrapper.get("code") == 200 and "data" in data_wrapper:
                    data = data_wrapper["data"]
                    content = data.get("content")
                    final_url = data.get("url", url)
                    if content is None: raise ValueError("API JSONå“åº”ä¸­ç¼ºå°‘ 'content' å­—æ®µã€‚")
                else:
                    raise ValueError(f"API JSONå“åº”é”™è¯¯: {data_wrapper}")
            else:
                # å‡è®¾ç›´æ¥è¿”å›Markdownæ–‡æœ¬
                content = resp.text
                final_url = url

            if not content.strip(): raise ValueError("API è¿”å›å†…å®¹ä¸ºç©ºã€‚")

            print(f"âœ… [Jina Scraper] æˆåŠŸ: {url}")
            return {**item_info, "url": final_url, "content": content, "status": "success"}

        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥ {url}: {type(e).__name__} - {e}"
            print(f"âš ï¸ [Jina Scraper] {error_msg}")
            return {**item_info, "content": "", "status": "failed", "error_message": str(e)}


# --- 2.4 TavilyScraper ---
class TavilyScraper(ContentScraper):
    def __init__(self):
        self.api_key = os.environ.get("TAVILY_API_KEY", "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")
        if not self.api_key: raise ValueError("æœªæä¾› Tavily API Keyã€‚")
        self.base_url = "https://api.tavily.com/extract"
        self.headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}

    async def scrape(self, item_info: Dict[str, Any], client: httpx.AsyncClient) -> dict:
        url = item_info.get("url")
        print(f"ğŸ¤– [Tavily Scraper] å¼€å§‹å¤„ç†: {url}")
        try:
            # ã€ä¿®æ”¹ã€‘æ ¹æ®æ–‡æ¡£ï¼Œurlså­—æ®µåº”è¯¥æ˜¯åˆ—è¡¨ï¼Œä¸”ä½¿ç”¨ format: markdown
            payload = {"urls": [url], "format": "markdown"}
            resp = await client.post(self.base_url, json=payload, headers=self.headers, timeout=45)
            resp.raise_for_status()
            data = resp.json()

            if not data.get("results") or not isinstance(data["results"], list):
                failed_info = data.get("failed_results", [])
                raise ValueError(f"APIè°ƒç”¨å¤±è´¥: {failed_info}")

            result = data["results"][0]
            # ã€ä¿®æ”¹ã€‘æ ¹æ®æ–‡æ¡£ï¼Œå†…å®¹å­—æ®µæ˜¯ raw_content
            content = result.get("raw_content")
            if content is None: raise ValueError("APIæœªè¿”å›raw_contentå†…å®¹ã€‚")

            final_url = result.get("url", url)

            print(f"âœ… [Tavily Scraper] æˆåŠŸ: {url}")
            return {**item_info, "url": final_url, "content": content, "status": "success"}

        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥ {url}: {type(e).__name__} - {e}"
            print(f"âš ï¸ [Tavily Scraper] {error_msg}")
            return {**item_info, "content": "", "status": "failed", "error_message": str(e)}


# --- 2.5 ZhiLianJobScraper ---
class ZhiLianJobScraper:
    def __init__(self):
        self.api_url = "http://119.45.167.133:12906/api/scrape/zhilian"
        self.headers = {'accept': 'application/json', 'Content-Type': 'application/json'}

    async def scrape_jobs(self, payload: Dict[str, Any], client: httpx.AsyncClient) -> Dict[str, Any]:
        print(f"ğŸ’¼ [ZhiLian Scraper] å¼€å§‹ä½¿ç”¨è´Ÿè½½è°ƒç”¨API: {json.dumps(payload, ensure_ascii=False)}")
        if not payload or not payload.get("keywords") or not payload.get("provinces"):
            msg = "è´Ÿè½½æ— æ•ˆï¼Œç¼ºå°‘ 'keywords' æˆ– 'provinces'ã€‚"
            print(f"âš ï¸ [ZhiLian Scraper] {msg}")
            return {"status": "skipped", "data": [], "message": msg}
        try:
            # ç¡®ä¿ page_size æ˜¯æ•´æ•°
            if 'page_size' in payload: payload['page_size'] = int(payload['page_size'])

            resp = await client.post(self.api_url, headers=self.headers, json=payload, timeout=60)
            resp.raise_for_status()
            response_data = resp.json()
            if response_data.get("code") == 200:
                print(f"âœ… [ZhiLian Scraper] æˆåŠŸ: {response_data.get('message')}")
                return {"status": "success", "data": response_data.get("data", []),
                        "message": response_data.get("message")}
            else:
                msg = f"APIè¿”å›é”™è¯¯ç  {response_data.get('code')}: {response_data.get('message')}"
                print(f"API returned non-200 code: {msg}")
                return {"status": "failed", "data": [], "message": msg}
        except Exception as e:
            error_msg = f"APIè¯·æ±‚å¤±è´¥: {type(e).__name__} - {e}"
            print(f"âš ï¸ [ZhiLian Scraper] {error_msg}")
            return {"status": "failed", "data": [], "message": error_msg}


class TianyanEnterpriseScraper:
    def __init__(self):
        self.api_url = "http://open.api.tianyancha.com/services/open/ic/baseinfo/normal"
        # ä»ç¯å¢ƒå˜é‡æˆ–ç›´æ¥ç¡¬ç¼–ç è·å–Token
        self.token = os.environ.get("TIANYANCHA_TOKEN", "4d882100-ed23-4c22-a83b-c77af2e4be42")
        self.headers = {'Authorization': self.token}

    async def scrape_enterprise(self, name: str, client: httpx.AsyncClient) -> Dict[str, Any]:
        print(f"ğŸ¢ [Tianyan Scraper] å¼€å§‹æŸ¥è¯¢ä¼ä¸š: {name}")
        base_return = {"query_name": name}
        if not name:
            msg = "ä¼ä¸šåç§°ä¸ºç©ºï¼Œè·³è¿‡æŸ¥è¯¢ã€‚"
            print(f"ğŸŸ¡ [Tianyan Scraper] {msg}")
            return {**base_return, "status": "skipped", "data": None, "message": msg}
        try:
            params = {"keyword": name}
            resp = await client.get(self.api_url, headers=self.headers, params=params, timeout=30)
            resp.raise_for_status()
            response_data = resp.json()
            if response_data.get("error_code") == 0:
                print(f"âœ… [Tianyan Scraper] æˆåŠŸæŸ¥è¯¢åˆ°: {name}")
                return {**base_return, "status": "success", "data": response_data.get("result"),
                        "message": response_data.get("reason")}
            else:
                msg = f"APIè¿”å›é”™è¯¯ç  {response_data.get('error_code')}: {response_data.get('reason')}"
                print(f"âš ï¸ [Tianyan Scraper] {msg}")
                return {**base_return, "status": "failed", "data": None, "message": msg}
        except Exception as e:
            error_msg = f"APIè¯·æ±‚å¤±è´¥: {type(e).__name__} - {e}"
            print(f"âš ï¸ [Tianyan Scraper] {error_msg}")
            return {**base_return, "status": "failed", "data": None, "message": error_msg}


class DataOrchestrator:
    def __init__(self):
        self.content_scrapers: Dict[str, ContentScraper] = {
            "searchapi": SearchApiScraper(), "firecrawl": FirecrawlScraper(),
            "jina": JinaScraper(), "tavily": TavilyScraper(),
        }
        self.job_scraper = ZhiLianJobScraper()
        self.enterprise_scraper = TianyanEnterpriseScraper()

    # ã€è°ƒæ•´ã€‘æ•´ä¸ª process_all æ–¹æ³•è¢«é‡æ„ï¼Œä»¥å®ç°æ¡ä»¶åŒ–ä»»åŠ¡è°ƒåº¦ã€‚
    async def process_all(
            self,
            web_url_info_list: List[Dict[str, Any]],
            career_payload: Dict,
            enterprise_names: List[str]  # æ¥æ”¶åˆ—è¡¨
    ) -> Dict[str, Any]:
        """
        æ ¹æ®æœ‰æ•ˆçš„è¾“å…¥ï¼Œæ¡ä»¶åŒ–åœ°åˆ›å»ºå¹¶å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŠ“å–ä»»åŠ¡ã€‚
        """
        final_results = {
            "content_results": [],
            "job_result": None,
            "enterprise_results": [],  # é»˜è®¤è¿”å›ç©ºåˆ—è¡¨
        }
        ssl_context = httpx.create_ssl_context(verify=False)
        async with httpx.AsyncClient(http2=True, verify=ssl_context, timeout=30, follow_redirects=True,
                                     limits=httpx.Limits(max_connections=50)) as client:

            content_tasks = []
            if web_url_info_list:
                print(f"  [Orchestrator] å‡†å¤‡ {len(web_url_info_list)}ä¸ªç½‘é¡µæŠ“å–ä»»åŠ¡ã€‚")
                for item in web_url_info_list:
                    scraper = self.content_scrapers.get(item.get("provider")) or self.content_scrapers["searchapi"]
                    content_tasks.append(scraper.scrape(item, client))
            job_tasks = []
            if career_payload and career_payload.get("keywords") and career_payload.get("provinces"):
                print("  [Orchestrator] å‡†å¤‡æ‹›è˜ä¿¡æ¯æŠ“å–ä»»åŠ¡ã€‚")
                job_tasks.append(self.job_scraper.scrape_jobs(career_payload, client))
            else:
                print("  [Orchestrator] æ‹›è˜ä¿¡æ¯è´Ÿè½½æ— æ•ˆï¼Œè·³è¿‡ä»»åŠ¡ã€‚")

            # ã€è°ƒæ•´ã€‘ä¸ºåˆ—è¡¨ä¸­çš„æ¯ä¸ªä¼ä¸šåç§°åˆ›å»ºæŸ¥è¯¢ä»»åŠ¡
            enterprise_tasks = []
            if enterprise_names:
                print(f"  [Orchestrator] å‡†å¤‡ {len(enterprise_names)}ä¸ªä¼ä¸šä¿¡æ¯æŸ¥è¯¢ä»»åŠ¡ã€‚")
                for name in enterprise_names:
                    enterprise_tasks.append(self.enterprise_scraper.scrape_enterprise(name, client))
            else:
                print("  [Orchestrator] ä¼ä¸šåç§°åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡ä»»åŠ¡ã€‚")

            tasks_to_run = content_tasks + job_tasks + enterprise_tasks
            if not tasks_to_run:
                print("  [Orchestrator] æ²¡æœ‰å¯æ‰§è¡Œçš„ä»»åŠ¡ã€‚")
                return final_results
            all_results = await asyncio.gather(*tasks_to_run, return_exceptions=True)
            # ã€è°ƒæ•´ã€‘å®‰å…¨åœ°è§£æå’Œåˆ†ç¦»ä¸‰ç»„ä»»åŠ¡çš„ç»“æœ
            content_end_idx = len(content_tasks)
            job_end_idx = content_end_idx + len(job_tasks)
            final_results["content_results"] = all_results[:content_end_idx]

            job_task_results = all_results[content_end_idx:job_end_idx]
            if job_task_results:
                final_results["job_result"] = job_task_results[0]

            final_results["enterprise_results"] = all_results[job_end_idx:]
            return final_results


# --- 5. Dify èŠ‚ç‚¹ä¸»å…¥å£ ---
async def main_async(raw_input: Any) -> Dict[str, Any]:
    # 1. è§£æè¾“å…¥
    parsed_data = _parse_input_data(raw_input)
    web_url_info_list = parsed_data["web_url_info_list"]
    video_url_info_list = parsed_data["video_url_info_list"]
    career_payload = parsed_data["career_payload"]
    enterprise_names = parsed_data["enterprise_names"]
    mode = parsed_data.get("mode", "legacy")

    if not web_url_info_list and not video_url_info_list and (
            not career_payload or not career_payload.get("keywords")) and not enterprise_names:
        print("ğŸŸ¡ æ‰€æœ‰è¾“å…¥å‡ä¸ºç©ºï¼Œæå‰è¿”å›ã€‚")
        return {"scraped_datas": {}, "scraped_datas_str": "{}"}
    # 2. è¿è¡Œè°ƒåº¦å™¨
    orchestrator = DataOrchestrator()
    results = await orchestrator.process_all(web_url_info_list, career_payload, enterprise_names)
    
    # 3. æ ¼å¼åŒ–ç½‘é¡µå†…å®¹è¾“å‡º (æŒ‰ origin_key åˆ†ç»„)
    content_results_by_origin = {
        "comprehensive_data": [],
        "general_web_data": [],
        "institution_source_data": []
    }
    
    for i, result in enumerate(results["content_results"]):
        if isinstance(result, Exception): continue
        if result.get("status") == "success":
            sanitized_url = re.sub(r'[^a-zA-Z0-9]', '-',
                                   result.get("url", "").replace("https://", "").replace("http://", ""))
            
            # Retrieve the origin_key from the input list corresponding to this result
            # Note: results["content_results"] corresponds exactly to web_url_info_list order
            origin_key = web_url_info_list[i].get("origin_key", "comprehensive_data")
            
            item_data = {
                "type": "web", "source_id": f"web-{sanitized_url[:100]}", "url": result.get("url"),
                "title": result.get("title"), "source": result.get("source"), "snippet": result.get("snippet"),
                "query": result.get("query"), "content": result.get("content", "")
            }
            
            if origin_key in content_results_by_origin:
                content_results_by_origin[origin_key].append(item_data)
            else:
                # Fallback
                content_results_by_origin["comprehensive_data"].append(item_data)

    # 4. æ ¼å¼åŒ–è§†é¢‘å†…å®¹è¾“å‡º (æš‚ä¸åˆ†ç»„ï¼Œè§†é¢‘é€šå¸¸åªå‡ºç°åœ¨ comprehensive æˆ– general ä¸­ï¼Œè¿™é‡Œç®€å•å¤„ç†)
    # å¦‚æœéœ€è¦ä¸¥æ ¼åˆ†ç»„ï¼Œä¹Ÿéœ€è¦åœ¨ _extract_urls ä¸­å¯¹è§†é¢‘åŠ  origin_key
    all_video_list = []
    for video_item in video_url_info_list:
        all_video_list.append({
            "type": "video", "url": video_item.get("url"), "title": video_item.get("title"),
            "source": video_item.get("source"), "snippet": video_item.get("snippet"),
            "video_id": video_item.get("video_id"), "embed_url": video_item.get("embed_url"),
            "thumbnail_url": video_item.get("thumbnail_url"), "query": video_item.get("query")
        })

    # ã€è°ƒæ•´ã€‘å¤„ç†æ‹›è˜å’Œä¼ä¸šä¿¡æ¯ç»“æœæ—¶ï¼Œæ£€æŸ¥å®ƒä»¬æ˜¯å¦å­˜åœ¨ï¼ˆæ˜¯å¦ä¸ºNoneï¼‰
    career_postings = results.get("job_result")
    if career_postings is None:
        career_postings = {}  # å¦‚æœæœªæ‰§è¡Œï¼Œåˆ™è¿”å›ç©ºå¯¹è±¡
    elif isinstance(career_postings, Exception):
        career_postings = {"status": "failed", "data": [], "message": f"ä»»åŠ¡å¼‚å¸¸: {career_postings}"}

    enterprise_infos_raw = results.get("enterprise_results", [])
    enterprise_infos_output = {}
    if enterprise_names:
        successful_data = []
        failed_queries = []

        for item in enterprise_infos_raw:
            res = item
            if isinstance(item, Exception):
                res = {"status": "failed", "message": f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(item)}", "query_name": "Unknown"}

            if res.get("status") == "success" and res.get("data"):
                successful_data.append(res["data"])
            elif res.get("status") in ["failed", "skipped"]:
                failed_queries.append({
                    "query_name": res.get("query_name", "N/A"),
                    "error_message": res.get("message", "æœªçŸ¥é”™è¯¯")
                })
        final_status = "skipped"
        if successful_data or failed_queries:
            if not failed_queries:
                final_status = "success"
            elif not successful_data:
                final_status = "failed"
            else:
                final_status = "partial_success"

        enterprise_infos_output = {
            "status": final_status,
            "data": successful_data,
            "failed_queries": failed_queries,
            "summary": f"å…±æŸ¥è¯¢ {len(enterprise_names)} ä¸ªä¼ä¸šï¼ŒæˆåŠŸ {len(successful_data)} ä¸ªï¼Œå¤±è´¥ {len(failed_queries)} ä¸ªã€‚"
        }

    # 6. ç»„è£…æœ€ç»ˆè¾“å‡º
    final_output = {}
    
    if mode == "legacy":
        # Legacy Mode Output Structure
        comprehensive_data_output = {
            "all_source_list": content_results_by_origin["comprehensive_data"], 
            "all_video_list": all_video_list
        }
        final_output = {
            "scraped_datas": {
                "comprehensive_data": comprehensive_data_output,
                "career_postings": career_postings,
                "enterprise_infos": enterprise_infos_output
            }
        }
    else:
        # External Mode Output Structure
        # Note: External mode currently doesn't focus on video list separately in the top structure, 
        # but we can include it if needed. For now, we follow the plan to separate general and institution.
        final_output = {
            "scraped_datas": {
                "general_web_data": content_results_by_origin["general_web_data"],
                "institution_source_data": content_results_by_origin["institution_source_data"],
                "career_postings": career_postings,
                # Video list can be appended to general_web_data or kept separate if required by downstream.
                # For now, let's keep it simple.
            }
        }
        
    return {
        "scraped_datas": final_output["scraped_datas"],
        "scraped_datas_str": json.dumps(final_output, ensure_ascii=False, indent=2)
    }


def main(datas_input: Any) -> Dict[str, Any]:
    try:
        return _dify_debug_return(asyncio.run(main_async(raw_input=datas_input)))
    except Exception as e:
        print(f"â€¼ï¸ èŠ‚ç‚¹æ‰§è¡Œæ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}")
        error_payload = {
            "comprehensive_data": {
                "all_source_list": [
                    {"type": "web", "source_id": "NODE_EXECUTION_ERROR", "title": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "url": "",
                     "content": f"An error occurred: {str(e)}\n\n{traceback.format_exc()}"}],
                "all_video_list": []
            },
            "career_postings": {},
            "enterprise_infos": {
                "status": "failed",
                "data": [],
                "failed_queries": [{"query_name": "Node Execution", "error_message": "èŠ‚ç‚¹é¡¶å±‚å¼‚å¸¸"}],
                "summary": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥"
            }
        }
        return _dify_debug_return({
            "scraped_datas": error_payload,
            "scraped_datas_str": json.dumps({"scraped_datas": error_payload}, ensure_ascii=False, indent=2)
        })


# main({ 'career_data': {},
#              'general_web_data': [ { 'errors': [],
#                                      'query': 'ä¿è‚²å‘˜',
#                                      'web_results': [ { 'searchapi_snippet': 'è“ç›ˆè¹åœ¨çŸ­å‰§ã€Šé©¬èƒŒæ‘‡ç¯®ã€‹ä¸­é¥°æ¼”ä¿è‚²å‘˜æ–‡çº«ç§‹ï¼Œè™½æ— å®é™…è‚²å„¿ç»éªŒï¼Œå´é€šè¿‡ä¸136åå°æ¼”å‘˜çš„çœŸå®äº’åŠ¨ã€æ²‰æµ¸å¼é‡èµ°å†å²è·¯çº¿åŠç»†è…»çš„è¡¨æ¼”è®¾è®¡ï¼Œå°†æˆ˜æ—¶â€œæ–‡å¦ˆå¦ˆâ€çš„æŸ”éŸ§ä¸åšæ¯…æ¼”ç» '
#                                                                              '...',
#                                                         'searchapi_source': 'æ–°æµªæ–°é—»_æ‰‹æœºæ–°æµªç½‘',
#                                                         'searchapi_title': "æ¼”å‘˜è“ç›ˆè¹æ— å­å¥³å´æ¼”æ´»'æ–‡å¦ˆå¦ˆ'ï¼ŒçœŸå®å„¿ç«¥äº’åŠ¨å¦‚ä½•æˆå°±çŸ­ ...",
#                                                         'searchapi_type': 'web',
#                                                         'searchapi_url': 'https://news.sina.cn/bignews/insight/2026-01-24/detail-inhikete1442916.d.html?oid=%E9%AB%98%E4%BB%BFmiumiu%E5%A5%B3%E5%8C%85%E7%B2%89%E7%BA%A2%E8%89%B2%EF%BC%88%E5%BE%AE%E4%BF%A1198099199%EF%BC%89lvN7&vt=4'},
#                                                       { 'searchapi_snippet': 'ä¿è‚²å‘˜æ˜¯å¹¼å„¿å›­é‡è¦å·¥ç§ä¹‹ä¸€ï¼Œæ˜¯ä¿è‚²å·¥ä½œçš„å…·ä½“å®æ–½è€…ã€‚è™½ç„¶æ¯ä¸ªå¹¼å„¿å›­çš„å·¥ç§æœ‰æ‰€å·®å¼‚ä½†æ˜¯å…¶åŸºæœ¬èŒè´£ä¸è¦æ±‚éƒ½æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯å…¶ç›®çš„æ˜¯ä¿ƒè¿›å¹¼å„¿çš„å…¨é¢å‘å±•ã€‚',
#                                                         'searchapi_source': 'orginview.com',
#                                                         'searchapi_title': 'å¹¼å„¿å›­çš„ä¿è‚²å‘˜éœ€è¦ä»€ä¹ˆè¯ï¼ˆéœ€è¦ä»€ä¹ˆæ¡ä»¶å¥½ä¸å¥½åšï¼‰ -',
#                                                         'searchapi_type': 'web',
#                                                         'searchapi_url': 'http://www.orginview.com/plugin.php?id=tom_tctoutiao&site=1&mod=info&aid=398'}]}],
#              'institution_source_data': [],
#              'run_mode': 'Tuoyu'})

# # --- 4. ç»Ÿä¸€è°ƒåº¦ä¸­å¿ƒ (å·²é‡å‘½åå’Œæ‰©å±•) ---
# class DataOrchestrator:
#     def __init__(self):
#         self.content_scrapers: Dict[str, ContentScraper] = {
#             "searchapi": SearchApiScraper(), "firecrawl": FirecrawlScraper(),
#             "jina": JinaScraper(), "tavily": TavilyScraper(),
#         }
#         self.job_scraper = ZhiLianJobScraper()
#         self.enterprise_scraper = TianyanEnterpriseScraper()
#
#     # async def process_all(self, url_list: List[Dict[str, str]], career_payload: Dict) -> Dict[str, Any]:
#     # ssl_context = httpx.create_ssl_context(verify=False)
#     # async with httpx.AsyncClient(http2=True, verify=ssl_context, timeout=30, follow_redirects=True,
#     #                              limits=httpx.Limits(max_connections=50)) as client:
#     #     # åˆ›å»ºä¸¤ç»„ä»»åŠ¡
#     #     content_tasks = []
#     #     for item in url_list:
#     #         scraper = self.content_scrapers.get(item.get("provider"))
#     #         if scraper: content_tasks.append(scraper.scrape(item["url"], item["title"], client))
#
#     #     job_task = self.job_scraper.scrape_jobs(career_payload, client)
#
#     #     # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
#     #     results = await asyncio.gather(*content_tasks, job_task, return_exceptions=True)
#
#     #     # åˆ†ç¦»ç»“æœ
#     #     content_results = results[:-1]
#     #     job_result = results[-1]
#
#     #     return {"content_results": content_results, "job_result": job_result}
#     async def process_all(self, web_url_info_list: List[Dict[str, Any]], career_payload: Dict, enterprise_name: str) -> \
#             Dict[str, Any]:
#         ssl_context = httpx.create_ssl_context(verify=False)
#         async with httpx.AsyncClient(http2=True, verify=ssl_context, timeout=30, follow_redirects=True,
#                                      limits=httpx.Limits(max_connections=50)) as client:
#             content_tasks = []
#
#             # ã€ä¿®æ”¹ã€‘ä» web_url_info_list åˆ›å»ºæŠ“å–ä»»åŠ¡
#             for item in web_url_info_list:
#                 scraper = self.content_scrapers.get(item.get("provider"))
#                 # é»˜è®¤ä½¿ç”¨ SearchApiScraper ä½œä¸ºå¤‡é€‰
#                 if not scraper: scraper = self.content_scrapers["searchapi"]
#                 content_tasks.append(scraper.scrape(item, client))
#
#             job_task = self.job_scraper.scrape_jobs(career_payload, client)
#             enterprise_task = self.enterprise_scraper.scrape_enterprise(enterprise_name, client)
#
#             all_tasks = content_tasks + [job_task, enterprise_task]
#             results = await asyncio.gather(*all_tasks, return_exceptions=True)
#
#             content_results = results[:len(content_tasks)]
#             job_result = results[len(content_tasks)]
#             enterprise_result = results[len(content_tasks) + 1]
#             return {"content_results": content_results, "job_result": job_result,
#                     "enterprise_result": enterprise_result}
#
#
# # --- 5. Dify èŠ‚ç‚¹ä¸»å…¥å£ ---
# async def main_async(raw_input: Any) -> Dict[str, Any]:
#     # 1. è§£æè¾“å…¥
#     parsed_data = _parse_input_data(raw_input)
#     # ã€ä¿®æ”¹ã€‘è·å–åˆ†ç¦»åçš„ç½‘é¡µå’Œè§†é¢‘åˆ—è¡¨
#     web_url_info_list = parsed_data["web_url_info_list"]
#     video_url_info_list = parsed_data["video_url_info_list"]
#     career_payload = parsed_data["career_payload"]
#     enterprise_name = parsed_data["enterprise_name"]
#
#     if not web_url_info_list and not video_url_info_list and not career_payload.get("keywords") and not enterprise_name:
#         print("ğŸŸ¡ æ‰€æœ‰è¾“å…¥å‡ä¸ºç©ºï¼Œæå‰è¿”å›ã€‚")
#         return {"scraped_datas": {}, "scraped_datas_str": "{}"}
#
#     # 2. è¿è¡Œè°ƒåº¦å™¨ (åªæŠ“å–ç½‘é¡µå†…å®¹)
#     orchestrator = DataOrchestrator()
#     results = await orchestrator.process_all(web_url_info_list, career_payload, enterprise_name)
#
#     # 3. ã€ä¿®æ”¹ã€‘æ ¼å¼åŒ–ç½‘é¡µå†…å®¹è¾“å‡ºï¼Œæ„å»º all_source_list
#     all_source_list = []
#     for result in results["content_results"]:
#         if isinstance(result, Exception): continue
#         if result.get("status") == "success":  # å³ä½¿ content ä¸ºç©ºä¹Ÿä¿ç•™ï¼Œä»¥ä¾¿ä¸‹æ¸¸åˆ¤æ–­
#             sanitized_url = re.sub(r'[^a-zA-Z0-9]', '-',
#                                    result.get("url", "").replace("https://", "").replace("http://", ""))
#             all_source_list.append({
#                 "type": "web",
#                 "source_id": f"web-{sanitized_url[:100]}",
#                 "url": result.get("url"),
#                 "title": result.get("title"),
#                 "source": result.get("source"),
#                 "snippet": result.get("snippet"),
#                 "query": result.get("query"),
#                 "content": result.get("content", "")  # ç¡®ä¿æœ‰ content å­—æ®µ
#             })
#
#     # ã€ä¿®æ”¹ã€‘æ ¼å¼åŒ–è§†é¢‘å†…å®¹è¾“å‡ºï¼Œæ„å»º all_video_list
#     all_video_list = []
#     for video_item in video_url_info_list:
#         all_video_list.append({
#             "type": "video",
#             "url": video_item.get("url"),
#             "title": video_item.get("title"),
#             "source": video_item.get("source"),
#             "snippet": video_item.get("snippet"),
#             "video_id": video_item.get("video_id"),
#             "embed_url": video_item.get("embed_url"),
#             "thumbnail_url": video_item.get("thumbnail_url"),
#             "query": video_item.get("query")
#         })
#
#     # 4. æ ¼å¼åŒ–æ‹›è˜ä¿¡æ¯è¾“å‡º
#     career_postings = results["job_result"]
#     if isinstance(career_postings, Exception): career_postings = {"status": "failed", "data": [],
#                                                                   "message": f"ä»»åŠ¡å¼‚å¸¸: {career_postings}"}
#
#     # 5. æ ¼å¼åŒ–ä¼ä¸šä¿¡æ¯è¾“å‡º
#     enterprise_info = results["enterprise_result"]
#     if isinstance(enterprise_info, Exception): enterprise_info = {"status": "failed", "data": None,
#                                                                   "message": f"ä»»åŠ¡å¼‚å¸¸: {enterprise_info}"}
#
#     # 6. ã€ä¿®æ”¹ã€‘ç»„è£…æœ€ç»ˆè¾“å‡ºä»¥ç¬¦åˆæ–°çš„æ•°æ®ç»“æ„
#     comprehensive_data_output = {
#         "all_source_list": all_source_list,
#         "all_video_list": all_video_list
#     }
#
#     final_output = {
#         "scraped_datas": {
#             "comprehensive_data": comprehensive_data_output,  # ä¿®æ”¹æ­¤å¤„çš„é”®å’Œå€¼
#             "career_postings": career_postings,
#             "enterprise_info": enterprise_info
#         }
#     }
#
#     return {
#         "scraped_datas": final_output["scraped_datas"],
#         "scraped_datas_str": json.dumps(final_output, ensure_ascii=False, indent=2)
#     }
#
#
# def main(datas_input: Any) -> Dict[str, Any]:
#     try:
#         return asyncio.run(main_async(raw_input=datas_input))
#     except Exception as e:
#         print(f"â€¼ï¸ èŠ‚ç‚¹æ‰§è¡Œæ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}")
#         # ã€ä¿®æ”¹ã€‘é”™è¯¯è´Ÿè½½ä»¥åŒ¹é…æ–°çš„ comprehensive_data ç»“æ„
#         error_payload = {
#             "comprehensive_data": {
#                 "all_source_list": [
#                     {"type": "web", "source_id": "NODE_EXECUTION_ERROR", "title": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "url": "",
#                      "content": f"An error occurred: {str(e)}\n\n{traceback.format_exc()}"}],
#                 "all_video_list": []
#             },
#             "career_postings": {"status": "failed", "message": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "data": []},
#             "enterprise_info": {"status": "failed", "message": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "data": None}
#         }
#         return {
#             "scraped_datas": error_payload,
#             "scraped_datas_str": json.dumps({"scraped_datas": error_payload}, ensure_ascii=False, indent=2)
#         }

# async def main_async(raw_input: Any) -> Dict[str, Any]:
#     # 1. è§£æè¾“å…¥
#     parsed_data = _parse_input_data(raw_input)
#     url_list = parsed_data["url_list"]
#     career_payload = parsed_data["career_payload"]
#     if not url_list and not career_payload.get("keywords"):
#         print("ğŸŸ¡ è¾“å…¥ä¸­æ²¡æœ‰æœ‰æ•ˆçš„URLæˆ–æ‹›è˜æŸ¥è¯¢ï¼Œæå‰è¿”å›ã€‚")
#         return {"scraped_datas": {}, "scraped_datas_str": "{}"}
#     enterprise_name = parsed_data["enterprise_name"]
#     # 2. è¿è¡Œè°ƒåº¦å™¨
#     orchestrator = DataOrchestrator()
#     results = await orchestrator.process_all(url_list, career_payload, enterprise_name)

#     # 3. æ ¼å¼åŒ–ç½‘é¡µå†…å®¹è¾“å‡º
#     comprehensive_content = []
#     for result in results["content_results"]:
#         if isinstance(result, Exception): continue
#         if result.get("status") == "success" and result.get("content"):
#             sanitized_url = re.sub(r'[^a-zA-Z0-9]', '-', result["url"].replace("https://", "").replace("http://", ""))
#             comprehensive_content.append({
#                 "source_id": f"web-{sanitized_url[:100]}", "source_name": result["title"],
#                 "url": result["url"], "content": result["content"]
#             })
#     # 4. æ ¼å¼åŒ–æ‹›è˜ä¿¡æ¯è¾“å‡º
#     career_postings = results["job_result"]
#     if isinstance(career_postings, Exception):
#         career_postings = {"status": "failed", "data": [], "message": f"ä»»åŠ¡å¼‚å¸¸: {career_postings}"}
#     # 5. ç»„è£…æœ€ç»ˆè¾“å‡º
#     final_output = {
#         "scraped_datas": {
#             "comprehensive_content": comprehensive_content,
#             "career_postings": career_postings
#         }
#     }
#     return {
#         "scraped_datas": final_output["scraped_datas"],
#         "scraped_datas_str": json.dumps(final_output, ensure_ascii=False, indent=2)
#     }

# def main(datas_input: Any) -> Dict[str, Any]:
#     try:
#         return asyncio.run(main_async(raw_input=datas_input))
#     except Exception as e:
#         print(f"â€¼ï¸ èŠ‚ç‚¹æ‰§è¡Œæ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}")
#         error_payload = {
#             "comprehensive_content": [{
#                 "source_id": "NODE_EXECUTION_ERROR", "source_name": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "url": "",
#                 "content": f"An error occurred: {str(e)}\n\n{traceback.format_exc()}"
#             }],
#             "career_postings": {"status": "failed", "message": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "data": []}
#         }
#         return {
#             "scraped_datas": error_payload,
#             "scraped_datas_str": json.dumps({"scraped_datas": error_payload}, ensure_ascii=False, indent=2)

# # Dify ä¾èµ–ç®¡ç†: è¯·ç¡®ä¿å·²æ·»åŠ  httpx, json-repair, trafilatura, pypdf2, beautifulsoup4, lxml
# import asyncio
# import httpx
# import re
# import os
# import json
# import time
# import traceback
# from typing import Any, Dict, List, Literal, Optional
# from abc import ABC, abstractmethod
# from io import BytesIO
# from urllib.parse import urljoin

# # --- æ ¸å¿ƒä¾èµ– ---
# # trafilatura ç”¨äºä»HTMLæå–ä¸»è¦å†…å®¹
# import trafilatura
# from trafilatura.settings import use_config

# # PyPDF2 ç”¨äºè§£æPDF
# from PyPDF2 import PdfReader

# # BeautifulSoup ç”¨äºè¾…åŠ©è§£æHTMLï¼ˆä¾‹å¦‚æå–è§†é¢‘ï¼‰
# from bs4 import BeautifulSoup

# # --- 1. è¾“å…¥è§£ææ¨¡å— ---
# # ã€å·²ä¿®å¤ã€‘æ›¿æ¢è¿™ä¸ªå‡½æ•°
# def _parse_input_data(raw_input: Any) -> Dict[str, Any]:
#     """
#     å¥å£®åœ°è§£æä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºï¼Œèƒ½åŒæ—¶å¤„ç†å¸¦ "datas" åŒ…è£…å’Œä¸å¸¦åŒ…è£…çš„ä¸¤ç§ç»“æ„ã€‚
#     """
#     print(f"============== æ­¥éª¤ 1: æ¥æ”¶åˆ°åŸå§‹è¾“å…¥ ==============\nTYPE: {type(raw_input)}\nVALUE: {raw_input}\n=======================================================")
#     if isinstance(raw_input, str):
#         if not raw_input.strip(): return {"url_list": [], "career_payload": {}, "enterprise_name": ""}
#         try:
#             data = json.loads(raw_input)
#         except json.JSONDecodeError as e:
#             raise ValueError(f"æ— æ³•å°†è¾“å…¥å­—ç¬¦ä¸²è§£æä¸ºJSON: {e}")
#     elif isinstance(raw_input, dict):
#         data = raw_input
#     else:
#         raise TypeError(f"æœŸæœ›çš„è¾“å…¥ç±»å‹æ˜¯ str æˆ– dict, ä½†æ”¶åˆ°äº† {type(raw_input).__name__}")

#     # --- æ ¸å¿ƒä¿®å¤é€»è¾‘ ---
#     # æ£€æŸ¥é¡¶å±‚æ˜¯å¦æœ‰ "datas" é”®ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°±è®¤ä¸ºå½“å‰æ•´ä¸ªå¯¹è±¡å°±æ˜¯æˆ‘ä»¬è¦çš„æ•°æ®ä½“ã€‚
#     if "datas" in data and isinstance(data["datas"], dict):
#         print("  [è§£æå™¨] æ£€æµ‹åˆ° 'datas' åŒ…è£…å±‚ï¼Œå°†ä½¿ç”¨å…¶å†…éƒ¨æ•°æ®ã€‚")
#         datas_obj = data["datas"]
#     else:
#         print("  [è§£æå™¨] æœªæ£€æµ‹åˆ° 'datas' åŒ…è£…å±‚ï¼Œå°†ç›´æ¥ä½¿ç”¨é¡¶å±‚æ•°æ®ã€‚")
#         datas_obj = data
#     # --- ä¿®å¤ç»“æŸ ---

#     if not isinstance(datas_obj, dict): datas_obj = {}

#     comprehensive_data = datas_obj.get("comprehensive_data", [])
#     career_data = datas_obj.get("career_data", {})
#     tianyan_data = datas_obj.get("tianyan_check_data", "")

#     url_list = []
#     if isinstance(comprehensive_data, list):
#         for query_result in comprehensive_data:
#             if not isinstance(query_result, dict): continue
#             for res_list_key in ["web_results", "video_results"]:
#                 for res in query_result.get(res_list_key, []):
#                     if not isinstance(res, dict): continue
#                     url, title, provider = None, None, None
#                     for key, value in res.items():
#                         if key.endswith("_url"):
#                             url, provider = value, key.split('_url')[0]
#                             title = res.get(f"{provider}_title", "Untitled")
#                             break
#                     if url and provider:
#                         url_list.append({"url": url, "title": title, "provider": provider})
#     career_payload = career_data if isinstance(career_data, dict) else {}
#     enterprise_name = tianyan_data if isinstance(tianyan_data, str) else ""
#     parsed_result = {"url_list": url_list, "career_payload": career_payload, "enterprise_name": enterprise_name.strip()}

#     print(f"============== æ­¥éª¤ 2: è¾“å…¥è§£æå®Œæ¯• ==============\nURL æ•°é‡: {len(url_list)}\næ‹›è˜è´Ÿè½½: {career_payload}\nä¼ä¸šåç§°: '{enterprise_name.strip()}'\n=======================================================")

#     return parsed_result

# # def _parse_input_data(raw_input: Any) -> Dict[str, Any]:
# #     """
# #     å¥å£®åœ°è§£æä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºï¼Œåˆ†ç¦»å‡ºwebæœç´¢URLå’Œæ‹›è˜æŸ¥è¯¢å‚æ•°ã€‚
# #     """
# #     if isinstance(raw_input, str):
# #         if not raw_input.strip(): return {"url_list": [], "career_payload": {}}
# #         try:
# #             data = json.loads(raw_input)
# #         except json.JSONDecodeError as e:
# #             raise ValueError(f"æ— æ³•å°†è¾“å…¥å­—ç¬¦ä¸²è§£æä¸ºJSON: {e}")
# #     elif isinstance(raw_input, dict):
# #         data = raw_input
# #     else:
# #         raise TypeError(f"æœŸæœ›çš„è¾“å…¥ç±»å‹æ˜¯ str æˆ– dict, ä½†æ”¶åˆ°äº† {type(raw_input).__name__}")
# #     # å®‰å…¨åœ°æ·±å…¥åˆ° 'datas' ç»“æ„
# #     datas_obj = data.get("datas", {})
# #     if not isinstance(datas_obj, dict): datas_obj = {}
# #     comprehensive_data = datas_obj.get("comprehensive_data", [])
# #     career_data = datas_obj.get("career_data", {})
# #     tianyan_data = datas_obj.get("tianyan_check_data", "")
# #     # 1. æå–URLåˆ—è¡¨
# #     url_list = []
# #     if isinstance(comprehensive_data, list):
# #         for query_result in comprehensive_data:
# #             if not isinstance(query_result, dict): continue
# #             for res_list_key in ["web_results", "video_results"]:
# #                 for res in query_result.get(res_list_key, []):
# #                     if not isinstance(res, dict): continue
# #                     url, title, provider = None, None, None
# #                     for key, value in res.items():
# #                         if key.endswith("_url"):
# #                             url = value
# #                             provider = key.split('_url')[0]
# #                             title = res.get(f"{provider}_title", "Untitled")
# #                             break
# #                     if url and provider:
# #                         url_list.append({"url": url, "title": title, "provider": provider})
# #     # 2. æå–èŒä¸šæŸ¥è¯¢è´Ÿè½½
# #     career_payload = career_data if isinstance(career_data, dict) else {}

# #     # 3. æå–ä¼ä¸šåç§°
# #     enterprise_name = tianyan_data if isinstance(tianyan_data, str) else ""
# #     return {"url_list": url_list, "career_payload": career_payload, "enterprise_name": enterprise_name.strip()}

# # --- 2. æŠ½è±¡ä¸å®ç°åˆ†ç¦»ï¼šå†…å®¹æŠ“å–å™¨ ---
# class ContentScraper(ABC):
#     """å†…å®¹æŠ“å–å™¨çš„æŠ½è±¡åŸºç±»ã€‚"""

#     @abstractmethod
#     async def scrape(self, url: str, title: str, client: httpx.AsyncClient) -> Dict[str, Any]:
#         """
#         æŠ“å–å•ä¸ªURLçš„å†…å®¹ã€‚
#         æˆåŠŸæ—¶è¿”å›: {"url": str, "title": str, "content": str, "status": "success"}
#         å¤±è´¥æ—¶è¿”å›: {"url": str, "title": str, "content": "", "status": "failed", "error_message": str}
#         """
#         pass

# # --- 2.1 SearchAPI.io çš„æ‰‹åŠ¨æŠ“å–ä¸æ¸…æ´—å®ç° ---
# class SearchApiScraper(ContentScraper):
#     def __init__(self):
#         self.trafilatura_config = use_config()
#         self.trafilatura_config.set("DEFAULT", "EXTRACTION_TIMEOUT", "10")

#         # ç¼–è¯‘å¸¸ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ€§èƒ½
#         self.NOISY_PATTERNS = [re.compile(p, re.IGNORECASE) for p in [
#             r'^\s*$', r'^[\-=*#_]{3,}$', r'.*\.(html|shtml|htm|php)\s*$',
#             r'.{0,50}(æœç‹|ç½‘æ˜“|è…¾è®¯|æ–°æµª|ç™»å½•|æ³¨å†Œ|ç‰ˆæƒæ‰€æœ‰|ç‰ˆæƒå£°æ˜).{0,50}$',
#             r'\[\d+\]|\[ä¸‹ä¸€é¡µ\]|\[ä¸Šä¸€é¡µ\]', r'\[(ç¼–è¾‘|æŸ¥çœ‹å†å²|è®¨è®º|é˜…è¯»|æ¥æº|åŸæ ‡é¢˜)\]',
#             r'^\*+\s*\[.*?\]\(.*?\)',
#             r'^\s*(åˆ†äº«åˆ°|æ‰«æäºŒç»´ç |è¿”å›æœç‹|æŸ¥çœ‹æ›´å¤š|è´£ä»»ç¼–è¾‘|è®°è€…|é€šè®¯å‘˜)',
#             r'^\s*([äº¬å…¬ç½‘å®‰å¤‡äº¬ç½‘æ–‡äº¬ICPå¤‡]|äº’è”ç½‘æ–°é—»ä¿¡æ¯æœåŠ¡è®¸å¯è¯|ä¿¡æ¯ç½‘ç»œä¼ æ’­è§†å¬èŠ‚ç›®è®¸å¯è¯)',
#         ]]
#         self.IMG_PATTERN = re.compile(r'(!\[(.*?)\]\((.*?)\))')
#         self.LINK_PATTERN = re.compile(r'\[.*?\]\(.*?\)')
#         self.EDITOR_PATTERN = re.compile(r'(\(|\[)\s*è´£ä»»ç¼–è¾‘ï¼š.*?\s*(\)|\])')

#     # --- 2.1.1 å†…å®¹æå–å·¥å…· (æ¥è‡ªæ‚¨çš„ä»£ç ) ---
#     def _extract_pdf_text(self, binary_content: bytes) -> str:
#         try:
#             reader = PdfReader(BytesIO(binary_content))
#             return "\n".join(page.extract_text() or "" for page in reader.pages)
#         except Exception as e:
#             print(f"âš ï¸ PDF è§£æå¤±è´¥: {e}")
#             return ""

#     def _parse_videos_from_html(self, html: str, base_url: str) -> List[str]:
#         try:
#             soup = BeautifulSoup(html, "lxml")
#             videos = []
#             for video in soup.find_all("video"):
#                 src = video.get("src")
#                 if src: videos.append(urljoin(base_url, src))
#                 for source in video.find_all("source"):
#                     src = source.get("src")
#                     if src: videos.append(urljoin(base_url, src))
#             for iframe in soup.find_all("iframe"):
#                 src = iframe.get("src")
#                 if src and any(k in src for k in ["youtube", "vimeo", "embed", ".mp4"]):
#                     videos.append(urljoin(base_url, src))
#             return list(dict.fromkeys(videos))  # å»é‡å¹¶ä¿æŒé¡ºåº
#         except Exception as e:
#             print(f"âš ï¸ è§†é¢‘è§£æå¤±è´¥: {e}")
#             return []

#     # --- 2.1.2 å†…å®¹æ¸…æ´—å·¥å…· (æ¥è‡ªæ‚¨çš„ä»£ç ï¼Œå·²ä¼˜åŒ–å’Œå¼‚æ­¥åŒ–) ---
#     async def _is_valid_image_url_async(self, url: str, client: httpx.AsyncClient) -> bool:
#         if not url or not url.startswith(('http://', 'https://')): return False
#         try:
#             resp = await client.head(url, timeout=5, follow_redirects=True)
#             content_type = resp.headers.get('content-type', '').lower()
#             return resp.is_success and 'image' in content_type
#         except httpx.RequestError:
#             return False

#     async def _remove_invalid_images_async(self, md: str, client: httpx.AsyncClient) -> str:
#         MAX_IMAGES_TO_VALIDATE = 25
#         matches = list(self.IMG_PATTERN.finditer(md))
#         urls_to_check_all = {m.group(3).strip() for m in matches}

#         urls_to_check = set(list(urls_to_check_all)[:MAX_IMAGES_TO_VALIDATE])
#         if len(urls_to_check_all) > MAX_IMAGES_TO_VALIDATE:
#             print(f"âš ï¸ å›¾ç‰‡æ•°é‡è¿‡å¤š ({len(urls_to_check_all)}), åªéªŒè¯å‰ {MAX_IMAGES_TO_VALIDATE} å¼ ã€‚")

#         tasks = {url: self._is_valid_image_url_async(url, client) for url in urls_to_check}
#         results = await asyncio.gather(*tasks.values(), return_exceptions=True)

#         url_status = dict(zip(tasks.keys(), results))
#         valid_urls = {url for url, res in url_status.items() if isinstance(res, bool) and res}
#         valid_urls.update(urls_to_check_all - urls_to_check)  # æœªæ£€æŸ¥çš„é»˜è®¤æœ‰æ•ˆ

#         def replacer(match: re.Match):
#             return match.group(0) if match.group(3).strip() in valid_urls else ""

#         return self.IMG_PATTERN.sub(replacer, md)

#     def _is_noisy_line(self, line: str) -> bool:
#         stripped = line.strip()
#         for pat in self.NOISY_PATTERNS:
#             if pat.search(stripped): return True
#         links = self.LINK_PATTERN.findall(stripped)
#         if len(links) > 2 and len(stripped) / (len(links) + 1) < 30: return True
#         return False

#     async def _clean_content_async(self, text: str, client: httpx.AsyncClient) -> str:
#         if not text: return ""
#         text = await self._remove_invalid_images_async(text, client)

#         lines = text.splitlines()
#         cleaned_lines = []
#         for line in lines:
#             if not self._is_noisy_line(line):
#                 line = self.EDITOR_PATTERN.sub('', line).strip()
#                 if line: cleaned_lines.append(line)

#         # å»é™¤è¿ç»­ç©ºè¡Œ
#         out = []
#         for i, line in enumerate(cleaned_lines):
#             if i > 0 and not line.strip() and not cleaned_lines[i - 1].strip():
#                 continue
#             out.append(line)

#         return "\n".join(out).strip()

#     # --- 2.1.3 ä¸»æŠ“å–å‡½æ•° (æ¥è‡ªæ‚¨çš„ä»£ç ï¼Œå°è£…ä¸ºscrapeæ–¹æ³•) ---
#     async def scrape(self, url: str, title: str, client: httpx.AsyncClient) -> dict:
#         print(f"ğŸ•¸ï¸ [SearchAPI Scraper] å¼€å§‹å¤„ç†: {url}")
#         try:
#             headers = {
#                 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
#             response = await client.get(url, timeout=20, headers=headers, follow_redirects=True)
#             response.raise_for_status()

#             final_url = str(response.url)
#             content_type = response.headers.get('content-type', '').lower()

#             raw_content, html_for_video_parsing = "", ""

#             if 'pdf' in content_type or final_url.lower().endswith(".pdf"):
#                 print(f"  ğŸ“„ [SearchAPI Scraper] æ£€æµ‹åˆ° PDF: {final_url}")
#                 pdf_bytes = await response.aread()
#                 raw_content = await asyncio.to_thread(self._extract_pdf_text, pdf_bytes)
#             else:
#                 print(f"  ğŸ“‘ [SearchAPI Scraper] æ£€æµ‹åˆ° HTML: {final_url}")
#                 html_content = response.text
#                 html_for_video_parsing = html_content
#                 raw_content = await asyncio.to_thread(
#                     trafilatura.extract, html_content, config=self.trafilatura_config,
#                     output_format='markdown', include_images=True, favor_recall=True)

#             if not raw_content: raise ValueError("trafilatura å†…å®¹æå–è¿”å›ä¸ºç©ºã€‚")

#             print(f"  ğŸ§¹ [SearchAPI Scraper] æ­£åœ¨æ¸…æ´—å†…å®¹: {final_url}")
#             cleaned_content = await self._clean_content_async(raw_content, client)

#             if html_for_video_parsing:
#                 videos = self._parse_videos_from_html(html_for_video_parsing, final_url)
#                 if videos:
#                     video_section = "\n\n## å‚è€ƒè§†é¢‘:\n" + "\n".join(f"- {vid}" for vid in videos)
#                     cleaned_content += video_section

#             print(f"âœ… [SearchAPI Scraper] æˆåŠŸ: {url}")
#             return {"url": final_url, "title": title, "content": cleaned_content, "status": "success"}

#         except Exception as e:
#             error_msg = f"å¤„ç†å¤±è´¥ {url}: {type(e).__name__} - {e}"
#             print(f"âš ï¸ [SearchAPI Scraper] {error_msg}")
#             return {"url": url, "title": title, "content": "", "status": "failed", "error_message": str(e)}

# # --- 2.2 FirecrawlScraper ---
# class FirecrawlScraper(ContentScraper):
#     def __init__(self):
#         self.api_key = os.environ.get("FIRECRAWL_API_KEY", "fc-a36b7d2fb273485680d0fe6abd686935")
#         if not self.api_key: raise ValueError("æœªæä¾› Firecrawl API Keyã€‚")
#         self.base_url = "https://api.firecrawl.dev/v2/scrape"
#         self.headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}

#     async def scrape(self, url: str, title: str, client: httpx.AsyncClient) -> dict:
#         print(f"ğŸ”¥ [Firecrawl Scraper] å¼€å§‹å¤„ç†: {url}")
#         try:
#             # **å…³é”®ä¿®æ­£ï¼šå°† pageOptions å±•å¹³ä¸ºé¡¶çº§å­—æ®µ**
#             payload = {
#                 "url": url,
#                 "onlyMainContent": True,
#                 # ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–é¡¶çº§é€‰é¡¹ï¼Œä¾‹å¦‚ï¼š
#                 "removeBase64Images": True,
#                 "blockAds": True
#             }
#             resp = await client.post(self.base_url, headers=self.headers, json=payload, timeout=45)

#             # å¢åŠ å¯¹4xx/5xxé”™è¯¯çš„è¯¦ç»†æ—¥å¿—è®°å½•
#             if not resp.is_success:
#                 try:
#                     error_details = resp.json()
#                     raise httpx.HTTPStatusError(f"APIè¿”å›é”™è¯¯: {error_details}", request=resp.request, response=resp)
#                 except json.JSONDecodeError:
#                     resp.raise_for_status()  # å¦‚æœæ— æ³•è§£æjsonï¼Œåˆ™æŠ›å‡ºåŸå§‹é”™è¯¯

#             data = resp.json()

#             # Firecrawl v2 çš„æˆåŠŸå“åº”ä¸­æ²¡æœ‰ "success" é”®ï¼Œç›´æ¥æ£€æŸ¥ data å­—æ®µ
#             content_data = data.get("data", {})
#             if content_data is None:  # å¯èƒ½æ˜¯ null
#                 raise ValueError("APIè¿”å›çš„ 'data' å­—æ®µä¸º nullã€‚")

#             content = content_data.get("markdown")  # markdown å¯èƒ½ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¿™æ˜¯æ­£å¸¸çš„
#             if content is None:
#                 raise ValueError("APIæœªè¿”å› 'markdown' å­—æ®µã€‚")

#             print(f"âœ… [Firecrawl Scraper] æˆåŠŸ: {url}")
#             return {"url": url, "title": title, "content": content, "status": "success"}
#         except Exception as e:
#             error_msg = f"å¤„ç†å¤±è´¥ {url}: {type(e).__name__} - {e}"
#             print(f"âš ï¸ [Firecrawl Scraper] {error_msg}")
#             return {"url": url, "title": title, "content": "", "status": "failed", "error_message": str(e)}

# # --- 2.3 JinaScraper ---
# class JinaScraper(ContentScraper):
#     def __init__(self):
#         self.api_key = os.environ.get("JINA_API_KEY",
#                                       "jina_b4348ffc39ca47bfbe753b95f59428c7i6ifkOFXRPdF3dRa5Rwb6T8FvrLH")
#         if not self.api_key: raise ValueError("æœªæä¾› Jina API Keyã€‚")
#         self.base_url = "https://r.jina.ai/"
#         self.headers = {
#             "Authorization": f"Bearer {self.api_key}",
#             "Content-Type": "application/json",
#             "Accept": "application/json",
#             "X-Return-Format": "markdown"  # å…³é”®ï¼šç›´æ¥è·å–Markdown
#         }

#     async def scrape(self, url: str, title: str, client: httpx.AsyncClient) -> dict:
#         print(f"ğŸŒ€ [Jina Scraper] å¼€å§‹å¤„ç†: {url}")
#         try:
#             # Jinaçš„Reader APIæœ‰æ—¶å¯¹æ™®é€šçš„GETè¯·æ±‚æ›´å‹å¥½
#             target_url = f"{self.base_url}{url}"
#             resp = await client.get(target_url, headers=self.headers, timeout=45)
#             resp.raise_for_status()
#             content = resp.text
#             if not content: raise ValueError("API è¿”å›å†…å®¹ä¸ºç©ºã€‚")

#             print(f"âœ… [Jina Scraper] æˆåŠŸ: {url}")
#             return {"url": url, "title": title, "content": content, "status": "success"}

#         except Exception as e:
#             error_msg = f"å¤„ç†å¤±è´¥ {url}: {type(e).__name__} - {e}"
#             print(f"âš ï¸ [Jina Scraper] {error_msg}")
#             return {"url": url, "title": title, "content": "", "status": "failed", "error_message": str(e)}

# # --- 2.4 TavilyScraper ---
# class TavilyScraper(ContentScraper):
#     def __init__(self):
#         self.api_key = os.environ.get("TAVILY_API_KEY", "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")
#         if not self.api_key: raise ValueError("æœªæä¾› Tavily API Keyã€‚")
#         self.base_url = "https://api.tavily.com/extract"
#         self.headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}

#     async def scrape(self, url: str, title: str, client: httpx.AsyncClient) -> dict:
#         print(f"ğŸ¤– [Tavily Scraper] å¼€å§‹å¤„ç†: {url}")
#         try:
#             # æ³¨æ„ï¼šTavily çš„Pythonåº“æ²¡æœ‰extractæ–¹æ³•ï¼Œå¿…é¡»ç›´æ¥è°ƒç”¨API
#             payload = {"urls": [url], "format": "markdown"}
#             resp = await client.post(self.base_url, json=payload, headers=self.headers, timeout=45)
#             resp.raise_for_status()
#             data = resp.json()

#             if not data.get("results") or not isinstance(data["results"], list):
#                 failed_info = data.get("failed_results", [])
#                 raise ValueError(f"APIè°ƒç”¨å¤±è´¥: {failed_info}")

#             result = data["results"][0]
#             content = result.get("raw_content")  # æ–‡æ¡£æ˜¾ç¤ºraw_contentï¼Œå¹¶ä¸”format=markdown
#             if not content: raise ValueError("APIæœªè¿”å›raw_contentå†…å®¹ã€‚")

#             print(f"âœ… [Tavily Scraper] æˆåŠŸ: {url}")
#             return {"url": url, "title": title, "content": content, "status": "success"}

#         except Exception as e:
#             error_msg = f"å¤„ç†å¤±è´¥ {url}: {type(e).__name__} - {e}"
#             print(f"âš ï¸ [Tavily Scraper] {error_msg}")
#             return {"url": url, "title": title, "content": "", "status": "failed", "error_message": str(e)}

# # --- 2.5 ZhiLianJobScraper ---
# class ZhiLianJobScraper:
#     def __init__(self):
#         self.api_url = "http://119.45.167.133:12906/api/scrape/zhilian"
#         self.headers = {'accept': 'application/json', 'Content-Type': 'application/json'}

#     async def scrape_jobs(self, payload: Dict[str, Any], client: httpx.AsyncClient) -> Dict[str, Any]:
#         print(f"ğŸ’¼ [ZhiLian Scraper] å¼€å§‹ä½¿ç”¨è´Ÿè½½è°ƒç”¨API: {json.dumps(payload, ensure_ascii=False)}")
#         if not payload or not payload.get("keywords") or not payload.get("provinces"):
#             msg = "è´Ÿè½½æ— æ•ˆï¼Œç¼ºå°‘ 'keywords' æˆ– 'provinces'ã€‚"
#             print(f"âš ï¸ [ZhiLian Scraper] {msg}")
#             return {"status": "skipped", "data": [], "message": msg}
#         try:
#             # ç¡®ä¿ page_size æ˜¯æ•´æ•°
#             if 'page_size' in payload: payload['page_size'] = int(payload['page_size'])

#             resp = await client.post(self.api_url, headers=self.headers, json=payload, timeout=60)
#             resp.raise_for_status()
#             response_data = resp.json()
#             if response_data.get("code") == 200:
#                 print(f"âœ… [ZhiLian Scraper] æˆåŠŸ: {response_data.get('message')}")
#                 return {"status": "success", "data": response_data.get("data", []),
#                         "message": response_data.get("message")}
#             else:
#                 msg = f"APIè¿”å›é”™è¯¯ç  {response_data.get('code')}: {response_data.get('message')}"
#                 print(f"API returned non-200 code: {msg}")
#                 return {"status": "failed", "data": [], "message": msg}
#         except Exception as e:
#             error_msg = f"APIè¯·æ±‚å¤±è´¥: {type(e).__name__} - {e}"
#             print(f"âš ï¸ [ZhiLian Scraper] {error_msg}")
#             return {"status": "failed", "data": [], "message": error_msg}

# class TianyanEnterpriseScraper:
#     def __init__(self):
#         self.api_url = "http://open.api.tianyancha.com/services/open/ic/baseinfo/normal"
#         # ä»ç¯å¢ƒå˜é‡æˆ–ç›´æ¥ç¡¬ç¼–ç è·å–Token
#         self.token = os.environ.get("TIANYANCHA_TOKEN", "4d882100-ed23-4c22-a83b-c77af2e4be42")
#         self.headers = {'Authorization': self.token}
#     async def scrape_enterprise(self, name: str, client: httpx.AsyncClient) -> Dict[str, Any]:
#         """æ ¹æ®ä¼ä¸šåç§°æŸ¥è¯¢åŸºæœ¬ä¿¡æ¯ã€‚"""
#         print(f"ğŸ¢ [Tianyan Scraper] å¼€å§‹æŸ¥è¯¢ä¼ä¸š: {name}")
#         if not name:
#             msg = "ä¼ä¸šåç§°ä¸ºç©ºï¼Œè·³è¿‡æŸ¥è¯¢ã€‚"
#             print(f"ğŸŸ¡ [Tianyan Scraper] {msg}")
#             return {"status": "skipped", "data": None, "message": msg}
#         try:
#             params = {"keyword": name}
#             resp = await client.get(self.api_url, headers=self.headers, params=params, timeout=30)
#             resp.raise_for_status()
#             response_data = resp.json()
#             if response_data.get("error_code") == 0:
#                 print(f"âœ… [Tianyan Scraper] æˆåŠŸæŸ¥è¯¢åˆ°: {name}")
#                 return {"status": "success", "data": response_data.get("result"), "message": response_data.get("reason")}
#             else:
#                 msg = f"APIè¿”å›é”™è¯¯ç  {response_data.get('error_code')}: {response_data.get('reason')}"
#                 print(f"âš ï¸ [Tianyan Scraper] {msg}")
#                 return {"status": "failed", "data": None, "message": msg}
#         except Exception as e:
#             error_msg = f"APIè¯·æ±‚å¤±è´¥: {type(e).__name__} - {e}"
#             print(f"âš ï¸ [Tianyan Scraper] {error_msg}")
#             return {"status": "failed", "data": None, "message": error_msg}

# # --- 4. ç»Ÿä¸€è°ƒåº¦ä¸­å¿ƒ (å·²é‡å‘½åå’Œæ‰©å±•) ---
# class DataOrchestrator:
#     def __init__(self):
#         self.content_scrapers: Dict[str, ContentScraper] = {
#             "searchapi": SearchApiScraper(), "firecrawl": FirecrawlScraper(),
#             "jina": JinaScraper(), "tavily": TavilyScraper(),
#         }
#         self.job_scraper = ZhiLianJobScraper()
#         self.enterprise_scraper = TianyanEnterpriseScraper()
#     # async def process_all(self, url_list: List[Dict[str, str]], career_payload: Dict) -> Dict[str, Any]:
#         # ssl_context = httpx.create_ssl_context(verify=False)
#         # async with httpx.AsyncClient(http2=True, verify=ssl_context, timeout=30, follow_redirects=True,
#         #                              limits=httpx.Limits(max_connections=50)) as client:
#         #     # åˆ›å»ºä¸¤ç»„ä»»åŠ¡
#         #     content_tasks = []
#         #     for item in url_list:
#         #         scraper = self.content_scrapers.get(item.get("provider"))
#         #         if scraper: content_tasks.append(scraper.scrape(item["url"], item["title"], client))

#         #     job_task = self.job_scraper.scrape_jobs(career_payload, client)

#         #     # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
#         #     results = await asyncio.gather(*content_tasks, job_task, return_exceptions=True)

#         #     # åˆ†ç¦»ç»“æœ
#         #     content_results = results[:-1]
#         #     job_result = results[-1]

#         #     return {"content_results": content_results, "job_result": job_result}
#     async def process_all(self, url_list: List[Dict[str, str]], career_payload: Dict, enterprise_name: str) -> Dict[str, Any]:
#         ssl_context = httpx.create_ssl_context(verify=False)
#         async with httpx.AsyncClient(http2=True, verify=ssl_context, timeout=30, follow_redirects=True, limits=httpx.Limits(max_connections=50)) as client:
#             # åˆ›å»ºä¸‰ç»„ä»»åŠ¡
#             content_tasks, job_task, enterprise_task = [], None, None

#             for item in url_list:
#                 scraper = self.content_scrapers.get(item.get("provider"))
#                 if scraper: content_tasks.append(scraper.scrape(item["url"], item["title"], client))

#             job_task = self.job_scraper.scrape_jobs(career_payload, client)
#             enterprise_task = self.enterprise_scraper.scrape_enterprise(enterprise_name, client) # ã€æ–°å¢ã€‘

#             # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
#             all_tasks = content_tasks + [job_task, enterprise_task]
#             results = await asyncio.gather(*all_tasks, return_exceptions=True)

#             # åˆ†ç¦»ç»“æœ
#             content_results = results[:len(content_tasks)]
#             job_result = results[len(content_tasks)]
#             enterprise_result = results[len(content_tasks) + 1] # ã€æ–°å¢ã€‘
#             return {"content_results": content_results, "job_result": job_result, "enterprise_result": enterprise_result}

# # --- 5. Dify èŠ‚ç‚¹ä¸»å…¥å£ ---
# async def main_async(raw_input: Any) -> Dict[str, Any]:
#     # 1. è§£æè¾“å…¥
#     parsed_data = _parse_input_data(raw_input)
#     url_list = parsed_data["url_list"]
#     career_payload = parsed_data["career_payload"]
#     enterprise_name = parsed_data["enterprise_name"] # ã€æ–°å¢ã€‘

#     if not url_list and not career_payload.get("keywords") and not enterprise_name:
#         print("ğŸŸ¡ æ‰€æœ‰è¾“å…¥å‡ä¸ºç©ºï¼Œæå‰è¿”å›ã€‚")
#         return {"scraped_datas": {}, "scraped_datas_str": "{}"}
#     # 2. è¿è¡Œè°ƒåº¦å™¨
#     orchestrator = DataOrchestrator()
#     results = await orchestrator.process_all(url_list, career_payload, enterprise_name)
#     # 3. æ ¼å¼åŒ–ç½‘é¡µå†…å®¹è¾“å‡º
#     comprehensive_content = []
#     for result in results["content_results"]:
#         if isinstance(result, Exception): continue
#         if result.get("status") == "success" and result.get("content"):
#             sanitized_url = re.sub(r'[^a-zA-Z0-9]', '-', result["url"].replace("https://", "").replace("http://", ""))
#             comprehensive_content.append({"source_id": f"web-{sanitized_url[:100]}", "source_name": result["title"], "url": result["url"], "content": result["content"]})
#     # 4. æ ¼å¼åŒ–æ‹›è˜ä¿¡æ¯è¾“å‡º
#     career_postings = results["job_result"]
#     if isinstance(career_postings, Exception): career_postings = {"status": "failed", "data": [], "message": f"ä»»åŠ¡å¼‚å¸¸: {career_postings}"}
#     # 5. ã€æ–°å¢ã€‘æ ¼å¼åŒ–ä¼ä¸šä¿¡æ¯è¾“å‡º
#     enterprise_info = results["enterprise_result"]
#     if isinstance(enterprise_info, Exception): enterprise_info = {"status": "failed", "data": None, "message": f"ä»»åŠ¡å¼‚å¸¸: {enterprise_info}"}
#     # 6. ã€è°ƒæ•´ã€‘ç»„è£…æœ€ç»ˆè¾“å‡º
#     final_output = {
#         "scraped_datas": {
#             "comprehensive_content": comprehensive_content,
#             "career_postings": career_postings,
#             "enterprise_info": enterprise_info
#         }
#     }
#     return {
#         "scraped_datas": final_output["scraped_datas"],
#         "scraped_datas_str": json.dumps(final_output, ensure_ascii=False, indent=2)
#     }
# # ã€è°ƒæ•´ã€‘main å‡½æ•°
# def main(datas_input: Any) -> Dict[str, Any]:
#     try:
#         return asyncio.run(main_async(raw_input=datas_input))
#     except Exception as e:
#         print(f"â€¼ï¸ èŠ‚ç‚¹æ‰§è¡Œæ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}")
#         error_payload = {
#             "comprehensive_content": [{"source_id": "NODE_EXECUTION_ERROR", "source_name": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "url": "", "content": f"An error occurred: {str(e)}\n\n{traceback.format_exc()}"}],
#             "career_postings": {"status": "failed", "message": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "data": []},
#             "enterprise_info": {"status": "failed", "message": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "data": None}
#         }
#         return {
#             "scraped_datas": error_payload,
#             "scraped_datas_str": json.dumps({"scraped_datas": error_payload}, ensure_ascii=False, indent=2)
#         }

# # async def main_async(raw_input: Any) -> Dict[str, Any]:
# #     # 1. è§£æè¾“å…¥
# #     parsed_data = _parse_input_data(raw_input)
# #     url_list = parsed_data["url_list"]
# #     career_payload = parsed_data["career_payload"]
# #     if not url_list and not career_payload.get("keywords"):
# #         print("ğŸŸ¡ è¾“å…¥ä¸­æ²¡æœ‰æœ‰æ•ˆçš„URLæˆ–æ‹›è˜æŸ¥è¯¢ï¼Œæå‰è¿”å›ã€‚")
# #         return {"scraped_datas": {}, "scraped_datas_str": "{}"}
# #     enterprise_name = parsed_data["enterprise_name"]
# #     # 2. è¿è¡Œè°ƒåº¦å™¨
# #     orchestrator = DataOrchestrator()
# #     results = await orchestrator.process_all(url_list, career_payload, enterprise_name)

# #     # 3. æ ¼å¼åŒ–ç½‘é¡µå†…å®¹è¾“å‡º
# #     comprehensive_content = []
# #     for result in results["content_results"]:
# #         if isinstance(result, Exception): continue
# #         if result.get("status") == "success" and result.get("content"):
# #             sanitized_url = re.sub(r'[^a-zA-Z0-9]', '-', result["url"].replace("https://", "").replace("http://", ""))
# #             comprehensive_content.append({
# #                 "source_id": f"web-{sanitized_url[:100]}", "source_name": result["title"],
# #                 "url": result["url"], "content": result["content"]
# #             })
# #     # 4. æ ¼å¼åŒ–æ‹›è˜ä¿¡æ¯è¾“å‡º
# #     career_postings = results["job_result"]
# #     if isinstance(career_postings, Exception):
# #         career_postings = {"status": "failed", "data": [], "message": f"ä»»åŠ¡å¼‚å¸¸: {career_postings}"}
# #     # 5. ç»„è£…æœ€ç»ˆè¾“å‡º
# #     final_output = {
# #         "scraped_datas": {
# #             "comprehensive_content": comprehensive_content,
# #             "career_postings": career_postings
# #         }
# #     }
# #     return {
# #         "scraped_datas": final_output["scraped_datas"],
# #         "scraped_datas_str": json.dumps(final_output, ensure_ascii=False, indent=2)
# #     }

# # def main(datas_input: Any) -> Dict[str, Any]:
# #     try:
# #         return asyncio.run(main_async(raw_input=datas_input))
# #     except Exception as e:
# #         print(f"â€¼ï¸ èŠ‚ç‚¹æ‰§è¡Œæ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}")
# #         error_payload = {
# #             "comprehensive_content": [{
# #                 "source_id": "NODE_EXECUTION_ERROR", "source_name": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "url": "",
# #                 "content": f"An error occurred: {str(e)}\n\n{traceback.format_exc()}"
# #             }],
# #             "career_postings": {"status": "failed", "message": "èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥", "data": []}
# #         }
# #         return {
# #             "scraped_datas": error_payload,
# #             "scraped_datas_str": json.dumps({"scraped_datas": error_payload}, ensure_ascii=False, indent=2)
